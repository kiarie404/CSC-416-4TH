<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Developer Documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="setting_things_up.html"><strong aria-hidden="true">1.</strong> Setting Things Up</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="setting_up_the_compiler.html"><strong aria-hidden="true">1.1.</strong> Setting up the compiler</a></li><li class="chapter-item expanded "><a href="setting_up_LLD_linker.html"><strong aria-hidden="true">1.2.</strong> Setting up the linker</a></li><li class="chapter-item expanded "><a href="setting_up_qemu.html"><strong aria-hidden="true">1.3.</strong> Setting up the Riscv Virtual environment</a></li><li class="chapter-item expanded "><a href="setting_up_build_automation.html"><strong aria-hidden="true">1.4.</strong> Setting up the Build automation tool</a></li></ol></li><li class="chapter-item expanded "><a href="writing_a_bare_metal_rust_executable.html"><strong aria-hidden="true">2.</strong> writing a bare metal Rust executable</a></li><li class="chapter-item expanded "><a href="the_bootloader.html"><strong aria-hidden="true">3.</strong> The Bootloader</a></li><li class="chapter-item expanded "><a href="the_bootloader_2.html"><strong aria-hidden="true">4.</strong> The Bootloader_2</a></li><li class="chapter-item expanded "><a href="setting_up_comunications.html"><strong aria-hidden="true">5.</strong> Setting Up Communications</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="communications_theory.html"><strong aria-hidden="true">5.1.</strong> communications_theory</a></li></ol></li><li class="chapter-item expanded "><a href="theory_on_paging.html"><strong aria-hidden="true">6.</strong> Theory on Paging</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="segmentation.html"><strong aria-hidden="true">6.1.</strong> segmentation</a></li><li class="chapter-item expanded "><a href="paging.html"><strong aria-hidden="true">6.2.</strong> paging</a></li></ol></li><li class="chapter-item expanded "><a href="setting_up_memory_management.html"><strong aria-hidden="true">7.</strong> Setting Up Memory Management</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="RAM_management.html"><strong aria-hidden="true">7.1.</strong> The RAM Management</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="abstracting_the_RAM.html"><strong aria-hidden="true">7.1.1.</strong> Abstracting the RAM</a></li><li class="chapter-item expanded "><a href="allocation_and_deallocation_RAM.html"><strong aria-hidden="true">7.1.2.</strong> Allocating RAM Memory</a></li><li class="chapter-item expanded "><a href="fine_grained_alloation.html"><strong aria-hidden="true">7.1.3.</strong> Byte-grained allocation</a></li><li class="chapter-item expanded "><a href="setting_up_memory_virtualization_and_access_management.html"><strong aria-hidden="true">7.1.4.</strong> Setting Up RAM Memory Virtualization and access_management</a></li><li class="chapter-item expanded "><a href="handling_the_Physical_MMU.html"><strong aria-hidden="true">7.1.5.</strong> Using the Physical MMU instead of Virtual MMU</a></li><li class="chapter-item expanded "><a href="actual_implementation.html"><strong aria-hidden="true">7.1.6.</strong> Actual_implementation</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="handling_interrupts_and_traps.html"><strong aria-hidden="true">8.</strong> Handling interrupts and Traps</a></li><li class="chapter-item expanded "><a href="handling_interrupts_and_traps_2.html"><strong aria-hidden="true">9.</strong> Handling interrupts and Traps 2</a></li><li class="chapter-item expanded "><a href="handling_external_interrupts.html"><strong aria-hidden="true">10.</strong> Handling External Interrupts</a></li><li class="chapter-item expanded "><a href="setting_up_processes.html"><strong aria-hidden="true">11.</strong> Setting up Processes</a></li><li class="chapter-item expanded "><a href="the_block_driver.html"><strong aria-hidden="true">12.</strong> The Block Driver</a></li><li class="chapter-item expanded "><a href="system_calls.html"><strong aria-hidden="true">13.</strong> system_calls</a></li><li class="chapter-item expanded "><a href="Filesystem.html"><strong aria-hidden="true">14.</strong> Filesystem</a></li><li class="chapter-item expanded "><a href="user_processes.html"><strong aria-hidden="true">15.</strong> User Processes</a></li><li class="chapter-item expanded "><a href="overall_design.html"><strong aria-hidden="true">16.</strong> Overall Design</a></li><li class="chapter-item expanded "><a href="definitions_and_theories.html"><strong aria-hidden="true">17.</strong> Definitions and Theories</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="theory_on_the_linker.html"><strong aria-hidden="true">17.1.</strong> The linker</a></li><li class="chapter-item expanded "><a href="theory_on_Qemu.html"><strong aria-hidden="true">17.2.</strong> Qemu</a></li><li class="chapter-item expanded "><a href="fragmentation_issues.html"><strong aria-hidden="true">17.3.</strong> fragmentation_issues</a></li><li class="chapter-item expanded "><a href="memory_tracking_mechanisms.html"><strong aria-hidden="true">17.4.</strong> Memory Tracking Mechanisms</a></li><li class="chapter-item expanded "><a href="theory_on_MMU_implementation_in_riscv.html"><strong aria-hidden="true">17.5.</strong> Theory on MMU implementation in Riscv</a></li><li class="chapter-item expanded "><a href="VirtIO.html"><strong aria-hidden="true">17.6.</strong> VirtIO</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">18.</strong> Miscellenious</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="errors.html"><strong aria-hidden="true">18.1.</strong> Error Numbers</a></li><li class="chapter-item expanded "><a href="measuring_software_performance.html"><strong aria-hidden="true">18.2.</strong> Measuring Performance of software</a></li><li class="chapter-item expanded "><a href="importing_variables_from_the_linker_script.html"><strong aria-hidden="true">18.3.</strong> Importing variables from the Linker script</a></li><li class="chapter-item expanded "><a href="GNU_assembly_macros.html"><strong aria-hidden="true">18.4.</strong> GNU assembly macros</a></li><li class="chapter-item expanded "><a href="the_singleton_design.html"><strong aria-hidden="true">18.5.</strong> The singleton Structure</a></li><li class="chapter-item expanded "><a href="multitasking.html"><strong aria-hidden="true">18.6.</strong> Multitasking</a></li><li class="chapter-item expanded "><a href="Bitmasking_and_bit_operations.html"><strong aria-hidden="true">18.7.</strong> Bitmasking_and_bit_operations</a></li><li class="chapter-item expanded "><a href="compressed_instructions.html"><strong aria-hidden="true">18.8.</strong> Compressed Instructions</a></li><li class="chapter-item expanded "><a href="the_ABI.html"><strong aria-hidden="true">18.9.</strong> The ABI</a></li><li class="chapter-item expanded "><a href="ELF_files.html"><strong aria-hidden="true">18.10.</strong> Elf Files</a></li><li class="chapter-item expanded "><a href="riscv_registers.html"><strong aria-hidden="true">18.11.</strong> Riscv_registers</a></li><li class="chapter-item expanded "><a href="virt.html"><strong aria-hidden="true">18.12.</strong> Virtual representation of riscv in Qemu</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="pcie_express_devices.html"><strong aria-hidden="true">18.12.1.</strong> PCIe express devices</a></li><li class="chapter-item expanded "><a href="virtio_devices.html"><strong aria-hidden="true">18.12.2.</strong> VIRTIO devices</a></li></ol></li><li class="chapter-item expanded "><a href="global_allocator.html"><strong aria-hidden="true">18.13.</strong> Global Allocator</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">19.</strong> AfterMath</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="buffer_overflow_attacks.html"><strong aria-hidden="true">19.1.</strong> buffer_overflow_attacks</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">19.2.</strong> fork_bomb</div></li></ol></li><li class="chapter-item expanded "><a href="web_assembly.html"><strong aria-hidden="true">20.</strong> Web Assembly</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="literature_review_papers.html"><strong aria-hidden="true">20.1.</strong> Literature review papers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="software_deployment.html"><strong aria-hidden="true">20.1.1.</strong> Software_deployment</a></li></ol></li><li class="chapter-item expanded "><a href="setting_up_wasm_runtime.html"><strong aria-hidden="true">20.2.</strong> Setting Up Wasm Runtime</a></li><li class="chapter-item expanded "><a href="webassembly_challenges.html"><strong aria-hidden="true">20.3.</strong> webassembly_challenges</a></li><li class="chapter-item expanded "><a href="the_wasm_book.html"><strong aria-hidden="true">20.4.</strong> The Book</a></li></ol></li><li class="chapter-item expanded "><a href="RISCV_RUN.html"><strong aria-hidden="true">21.</strong> RISCV_RUN</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="reasons_for_RISCV.html"><strong aria-hidden="true">21.1.</strong> reasons_for_RISCV</a></li><li class="chapter-item expanded "><a href="priviledged_architecture.html"><strong aria-hidden="true">21.2.</strong> priviledged_architecture</a></li><li class="chapter-item expanded "><a href="error_handling_in_machine_mode.html"><strong aria-hidden="true">21.3.</strong> error_handling_in_machine_mode</a></li><li class="chapter-item expanded "><a href="seperating_user_mode_from_machine_mode.html"><strong aria-hidden="true">21.4.</strong> seperating_user_mode_from_machine_mode</a></li><li class="chapter-item expanded "><a href="Supervisor_mode_to_the_rescue.html"><strong aria-hidden="true">21.5.</strong> Supervisor_mode_to_the_rescue</a></li></ol></li><li class="chapter-item expanded "><a href="references.html"><strong aria-hidden="true">22.</strong> References</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Developer Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="setting-things-up"><a class="header" href="#setting-things-up">Setting Things Up</a></h2>
<p>Under this chapter, we intend to answer the following 3 questions :</p>
<ol>
<li>What are we setting up?</li>
<li>Why are we setting up those things?</li>
<li>How are we seting up those things?</li>
</ol>
<h3 id="what-are-we-setting-up"><a class="header" href="#what-are-we-setting-up">What are we setting up?</a></h3>
<p>We are setting up a <strong>development toolchain</strong>, <strong>RISCV virtual environment</strong> and a <strong>no-std Rust file</strong>.</p>
<h4 id="the-development-toolchain"><a class="header" href="#the-development-toolchain">The development Toolchain</a></h4>
<p>A toolchain is a group of software tools that typically get used together...a chain of tools...<br />
In OS Development, the name toolchain usually refers to the combination of the compiler, linker, debugger and a bunch of programs that help in inspecting files. This toolchain gets used to convert source code into a format that can run on an execution <em>environment</em>.</p>
<p>An execution environment is a place where a software program can run. It provides the necessary resources, like the operating system and libraries, that the program needs to function. Examples of execution enviroments include: Bare metal, Browsers, Virtual Machines, Operating systems and Containers.</p>
<p>The toolchain in our case will consist of the following tools :</p>
<ol>
<li>The Rust Nightly Compiler with a riscv64gc-unknown-none-elf backend</li>
<li>linker : Rust-lld</li>
<li>Binutils </li>
<li>Make</li>
</ol>
<p>To our luck, we do not have to install all these elements seperately. There exists compact toolchains :</p>
<ol>
<li>LLVM Riscv toolchain</li>
<li>The GNU Riscv Toolchain</li>
<li>The Rust Toolchain</li>
</ol>
<h4 id="why-we-need-the-toolchain"><a class="header" href="#why-we-need-the-toolchain">Why we need the toolchain</a></h4>
<p>We will have two kinds of source code files in our project : Rust source files and RISCV-Assembly files. Both of these types of files need to be turned into object files. Afterwards, those object files need to get linked together into a single executable file.</p>
<p>We can go about this process of creating a single executable file in two ways:</p>
<h5 id="method-1"><a class="header" href="#method-1">Method 1</a></h5>
<p>We can compile the Rust files seperately from the Assembly files.<br />
Meaning that we will use a stand-alone assembler to assemble the RISCV assembly files and turn them into object code.<br />
Later, we will then compile the RUST files into object code using a RUST_compiler.<br />
Afterwords we can combine the resultant object files using a linker to form a single executable.</p>
<h5 id="method-2"><a class="header" href="#method-2">Method 2</a></h5>
<p>We can embed the assembly code into the Rust source code.<br />
That way, we only need one compilation, we will only need to compile the asm_embedded Rust files. This method seems more of 'plug and play'.<br />
The disadvantage of this method is that we will always have to re-compile every file each time we change anything in any source file. But this is not really a problem. Modern compilers are Fast. Using method one will save up a few nano_seconds. A few nanoseconds is  cheap price to pay.<br />
Method 2  is a more user friendly method. Trading off negligible compile time over a user-friendliness in building and tweaking configurations is by far a very good choice.</p>
<p>Moreover, the rust compiler comes with its own inbuilt LLVM linker, rust-lld. That means that once we hit compile, we get the executable file output. One click, and all the build process runs inbuilt; from compiling rust files, to compiling assembly files, to creating a riscv-compliant executable file.</p>
<p>No more Makefiles nightmares, no more. This is very big news.<br />
For this reason, we will use Method 2.</p>
<h4 id="the-rust-llvm-compiler-and-targets"><a class="header" href="#the-rust-llvm-compiler-and-targets">The Rust LLVM compiler and Targets</a></h4>
<h4 id="the-linker"><a class="header" href="#the-linker">The Linker</a></h4>
<p><em><strong>references</strong></em></p>
<ul>
<li><a href="https://lld.llvm.org/">The LLD official Page</a></li>
<li><a href="https://nnethercote.github.io/perf-book/compile-times.html">Linking in Rust</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-the-compiler"><a class="header" href="#setting-up-the-compiler">Setting Up the Compiler</a></h1>
<p>The compiler is the tool that will convert our source code to machine code that suits a specific machine target.<br />
In our case, that specific machine target is &quot;The RISCV CPU, bare metal&quot;.</p>
<p>The rust compiler gets installed as a toolchain, so it comes with a linker attached. For this reason, our compile button will do the following : </p>
<ol>
<li>Compile rust files</li>
<li>Compile the embedded assembly files.</li>
<li>Link all the object files and produce an executable ELF file. (linker part)</li>
</ol>
<p>Ofcourse you an use a 3rd party linker that you prefer, you are not forced to use the attached linker. But using another linker looks like a lot of unnecessary hard work.</p>
<p>In the compiler world, people identify compilation targets using a standard naming convention called &quot;Target Triple&quot;.<br />
Initially the Target triple specified three characteristics of the machine target : 
- CPU Architecture                         : eg x86, RISCV, ARM
- Vendor who manufactures the target       : eg Apple, IBM
- The operating system running on the CPU  : eg Linux, Windows, FreeBSD, None</p>
<p>For example you would define a target as : ARM-Apple-IoS</p>
<p>But the world got more complex, now we have people naming things like... i don't know... it is not 3 characteristics anymore.   Sometimes you have 2 sometimes 5, 4 or 3.</p>
<p>So here is a 4 identifier description :
- CPU architecture
- Vendor
- Operating System
- ABI</p>
<p>Really, there is confusion, but hopefully you can tell what stands for what when you see a triple target with a weird number of identifiers.</p>
<h3 id="commands"><a class="header" href="#commands">Commands</a></h3>
<p>To install the Stable Rust compiler, enter the following comand :</p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh  
</code></pre>
<p>Alternatively, you can visit this page : <a href="https://www.rust-lang.org/tools/install">Rust Compiler installation Main Page</a></p>
<p>But in our project, we will use Nightly features. So you will need to install Rust Nightly :</p>
<pre><code class="language-bash">rustup toolchain install nightly  //install nightly Compiler
rustup default nightly            // set nightly Compiler as the default toolchain
</code></pre>
<p>The Machine Target we are compiling for is &quot;riscv64gc-unknown-none-elf&quot; which means we are compiling for </p>
<ul>
<li>&quot;<strong>riscv646gc</strong> -  64-bit-Riscv CPU that  : supports all general instructions 'g' and supports <a href="./compressed_instructions.html">compressed instructions</a> 'c'</li>
<li><strong>unknown</strong> - means that the manufaturer of the CPU is unknown or that info is irrelevant</li>
<li><strong>none</strong> - means that the CPU has no operating system running on top of it</li>
<li><strong>elf</strong> - This component identifies the format of the output binary file that will be generated by the compiler. In this case, it specifies that the binary file will be in the ELF (Executable and Linkable Format) format, which is a common format used for executables and shared libraries on Unix-based systems.</li>
</ul>
<p>To check out all the targets that the compiler can support by default, type the following comand :</p>
<pre><code class="language-bash">rustup target list               // list all supported targets
rustup target list --installed   // list all installed supported targets
</code></pre>
<p>To install our riscv64gc-unknown-none-elf target, enter th following command ;</p>
<pre><code class="language-bash">rustup target add riscv64gc-unknown-none-elf  // install a supported target
</code></pre>
<p>If you come up with your own custom target, you can tweak the toolchain to support your target. Like for our case, we are going to come write an operating system. The toolchain does not know our OS, So people are not able to compile specifically for our OS. That topic is discussed <a href="./Custom_targets.html">here</a></p>
<p>We are done setting up the compiler!!!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-the-linker"><a class="header" href="#setting-up-the-linker">Setting up the linker</a></h1>
<p>References : </p>
<ul>
<li><a href="http://bravegnu.org/gnu-eprog/lds.html">Guide to Linker Scripting</a></li>
</ul>
<p>As earlier mentioned, the Rust compiler comes with an inbuilt linker. 
Each target comes with its own configured linker</p>
<p>So by default we do not need a linker script. But as for our case, we need a linker script.</p>
<p><strong>Why?</strong>	</p>
<p>Reason 1:	
The default compiler does not know the name of your entry_point function. Normally, the linker deals with rust crates that depend on the std libraries, and the entry_point of these crates is &quot;_start&quot;--&gt;&quot;start&quot;--&gt;&quot;main&quot; by default. In our case the linker has no clue. We need to tell it using a linker script.	</p>
<p>Reason 2:
Here is the thing, the elf file has many sections. the global_data section, the heap, the stack, the bss, the text section...	</p>
<p>To write kernel code, you need to know the memory addresses of different elf sections.	
For example....<br />
When programming how the kernel heap will get allocated and deallocated, you will need to know the exact memory address of the heap and you will reference it in your allocating function using a pointer. Or when you want the bootloader to point to the text section of the kernel, you will need to give the bootloader the exact memory address of the start of the text section.<br />
Point is, to write kernel code, you need to know the memory addresses of the different sections.</p>
<p>The linker script lets you tell the linker the exact memory addresses where you want it to place the different elf sections. This way, you can make the linker point to the same memory addresses used in your code. You can be assured that the pointers in your code are pointing to some place that you KNOW.<br />
And the good thing is that the linker lets you label this known memory points using variables.	</p>
<p>If you let the linker decide the memory addresses,you would have to constantly change your code to point to the addresses the linker chose for you. And the linker is not that deterministic. Today it places the heap here, tomorrow there.</p>
<p>There is a problem with this explanation, the linker only deals with virtual memory. Not Physical memory. It is the job of the BIOS to load the kernel in physical addresses that mao to the virtual addresses.</p>
<p>Reason 3:	
You may want to make sure the different elf sections are aligned to a certain multiple. For example, if you plan to divide the heap into 4KB blocks, you may prefer to make the heap_start memory address a multiple of 4096</p>
<p>End of reasons...	</p>
<p>So how do we write a Linker Script? And to which linker are we scripting for?</p>
<h4 id="which-linker-are-we-scripting-for"><a class="header" href="#which-linker-are-we-scripting-for">Which linker are we scripting for?</a></h4>
<p>The rust gives you an option to choose whichever linker you want to use.<br />
Rust uses the LLVM Linker by default. So we are currently scripting for the LLVM Linker.<br />
You may want to use other linkers based on your usecase. For example the LLVM linker is known for its advanced optimizations. The gold linker is optimized for elf files only, so it is lightweight and faster than the GNU linker. Meaning that you will not prefer the gold linker when creating non_elf files.</p>
<p>To know which linker you are currently using, you can enter the command below :</p>
<pre><code class="language-bash">rustc --version --verbose
</code></pre>
<p>You get a result like this :
rustc 1.70.0-nightly (f63ccaf25 2023-03-06)
binary: rustc
commit-hash: f63ccaf25f74151a5d8ce057904cd944074b01d2
commit-date: 2023-03-06
host: x86_64-unknown-linux-gnu
release: 1.70.0-nightly
LLVM version: 15.0.7</p>
<p>From the above result, you can see That <strong>LLVM linker is used</strong> and specifically version 15.0.7</p>
<p>But each target uses a particular linker flavour, what if you want more information about your current host target? What if you want information about another non_host target ? Use the following command :</p>
<pre><code class="language-bash">rustc +nightly -Z unstable-options --target=wasm32-unknown-unknown --print target-spec-json   // for the nightly compiler
OR
rustc  -Z unstable-options --target=riscv64gc-unknown-none-elf --print target-spec-json     //for the stable compiler

</code></pre>
<p>You can optionaly specify your linker choice in the build manifest file (configuration file) - cargo.toml as follows :</p>
<pre><code class="language-toml">[target.'cfg(target_os = &quot;linux&quot;)'.llvm]
linker = &quot;/usr/bin/ld.gold&quot;                   //this specifies the path to the gold linker
</code></pre>
<p>But this is hard work, we are not taking that path. The less configurations we do, the more portable our code, the less headaches we get.</p>
<h4 id="how-do-we-write-a-linker-script"><a class="header" href="#how-do-we-write-a-linker-script">How do we write a Linker Script?</a></h4>
<p>Before we explain how to write the linker script, we should answer the question : &quot;Why write the liner script?&quot;<br />
The linker functions include :
- Resolving External symbols
- Section Merging
- Section Placement</p>
<p>We are writing the linker script so that we can instruct the linker on how it will do the section merging and section placement.</p>
<p><strong>Section merging</strong> is the process of combining similar elf sections from different files: For example if A.o and B.o were to be linked together to form C.o then the linker will merge the .text section in both A and B ie.  A.text_section + B.text_section = C.text_section</p>
<p><strong>Section placement</strong> is the process of specifying the virtual address of the different sections within the elf file. For example you may place the text section at 0x00 or 0x800... you name it. By default the linker places the different segments in adjacent to each other... but if you do this section placement process manually, you can set paddings between segments or jumble things up.</p>
<p>You can follow this tutorial <a href="http://bravegnu.org/gnu-eprog/lds.html">here</a> :</p>
<ul>
<li>Tell the linker which architecture you are targeting</li>
<li>You define the entry address of the elf file</li>
<li>Define all the memory that we have : RAM and ROM or just one of them </li>
</ul>
<p>Here is the Linker script example :</p>
<pre><code class="language-bash">/*
  define the architecture that the linker understands.  
  for any RISC-V target (64-bit riscv is the name of the architectut or 32-bit).

  We will further refine this by using -mabi=lp64 and -march=rv64gc
*/
OUTPUT_ARCH( &quot;riscv&quot; )

/*
We're setting our entry point to a symbol
called _start which is inside of boot.S. This
essentially stores the address of _start as the
&quot;entry point&quot;, or where CPU instructions should start
executing.

In the rest of this script, we are going to place _start
right at the beginning of 0x8000_0000 because this is where
the virtual machine and many RISC-V boards will start executing.
*/
ENTRY( _start )

/*
The MEMORY section will explain that we have &quot;ram&quot; that contains
a section that is 'w' (writeable), 'x' (executable), and 'a' (allocatable).
We use '!' to invert 'r' (read-only) and 'i' (initialized). We don't want
our memory to be read-only, and we're stating that it is NOT initialized
at the beginning.

The ORIGIN is the memory address 0x8000_0000. If we look at the virt
spec or the specification for the RISC-V HiFive Unleashed, this is the
starting memory address for our code.

Side note: There might be other boot ROMs at different addresses, but
their job is to get to this point.

Finally LENGTH = 128M tells the linker that we have 128 megabyte of RAM.
The linker will double check this to make sure everything can fit.

The HiFive Unleashed has a lot more RAM than this, but for the virtual 
machine, I went with 128M since I think that's enough RAM for now.

We can provide other pieces of memory, such as QSPI, or ROM, but we're
telling the linker script here that we have one pool of RAM.
*/
MEMORY
{
  ram   (wxa!ri) : ORIGIN = 0x80000000, LENGTH = 128M
}

/*
PHDRS is short for &quot;program headers&quot;, which we specify three here:
text - CPU instructions (executable sections)
data - Global, initialized variables
bss  - Global, uninitialized variables (all will be set to 0 by boot.S)

The command PT_LOAD tells the linker that these sections will be loaded
from the file into memory.

We can actually stuff all of these into a single program header, but by
splitting it up into three, we can actually use the other PT_* commands
such as PT_DYNAMIC, PT_INTERP, PT_NULL to tell the linker where to find
additional information.

However, for our purposes, every section will be loaded from the program
headers.
*/
PHDRS
{
  text PT_LOAD;   
  data PT_LOAD;
  bss PT_LOAD;
}

/*
We are now going to organize the memory based on which
section it is in. In assembly, we can change the section
with the &quot;.section&quot; directive. However, in C++ and Rust,
CPU instructions go into text, global constants go into
rodata, global initialized variables go into data, and
global uninitialized variables go into bss.
*/
SECTIONS
{
  /*
    The first part of our RAM layout will be the text section.
	Since our CPU instructions are here, and our memory starts at
	0x8000_0000, we need our entry point to line up here.
  */
  .text : {
	  /* In the GNU Linker Script Language, the PROVIDE keyword instructs the linker to declare a new symbol and assign it a value 

	    PROVIDE allows me to create a symbol called _text_start so
		I know where the text section starts in the operating system.
		This should not move, but it is here for convenience.
		The period '.' tells the linker to set _text_start to the
		CURRENT location ('.' = current memory location). This current
		memory location moves as we add things.
	  */

    PROVIDE(_text_start = .);
	/*
	  We are going to layout all text sections here, starting with 
	  .text.init. 
	  The asterisk in front of the parentheses means to match
	  the .text.init section of ANY object file. Otherwise, we can specify
	  which object file should contain the .text.init section, for example,
	  boot.o(.text.init) would specifically put the .text.init section of
	  our bootloader here.

	  Because we might want to change the name of our files, we'll leave it
	  with a *.

	  Inside the parentheses is the name of the section. I created my own
	  called .text.init to make 100% sure that the _start is put right at the
	  beginning. The linker will lay this out in the order it receives it:

	  .text.init first
	  all .text sections next
	  any .text.* sections last

	  .text.* means to match anything after .text. If we didn't already specify
	  .text.init, this would've matched here. The assembler and linker can place
	  things in &quot;special&quot; text sections, so we match any we might come across here.
	*/
    *(.text.init) *(.text .text.*)

	/*
	  Again, with PROVIDE, we're providing a readable symbol called _text_end, which is
	  set to the memory address AFTER .text.init, .text, and .text.*'s have been added.
	*/
    PROVIDE(_text_end = .);
	/*
	  The portion after the right brace is in an odd format. However, this is telling the
	  linker what memory portion to put it in. We labeled our RAM, ram, with the constraints
	  that it is writeable, allocatable, and executable. The linker will make sure with this
	  that we can do all of those things.

	  &gt;ram - This just tells the linker script to put this entire section (.text) into the
	         ram region of memory. To my knowledge, the '&gt;' does not mean &quot;greater than&quot;. Instead,
			 it is a symbol to let the linker know we want to put this in ram.

	  AT&gt;ram - This sets the LMA (load memory address) region to the same thing.this linker script, we're loading
			   everything into its physical location. We'll l LMA is the final
	           translation of a VMA (virtual memory address). With et the kernel copy and sort out the 
			   virtual memory. That's why &gt;ram and AT&gt;ram are continually the same thing.

	  :text  - This tells the linker script to put this into the :text program header. We've only
	           defined three: text, data, and bss. In this case, we're telling the linker script
			   to go into the text section.
	*/
  } &gt;ram AT&gt;ram :text
   /*
     The global pointer allows the linker to position global variables and constants into
	 independent positions relative to the gp (global pointer) register. The globals start
	 after the text sections and are only relevant to the rodata, data, and bss sections.
   */
   PROVIDE(_global_pointer = .);
   /*
     Most compilers create a rodata (read only data) section for global constants. However,
	 we're going to place ours in the text section. We can actually put this in :data, but
	 since the .text section is read-only, we can place it there.

	 NOTE: This doesn't actually do anything, yet. The actual &quot;protection&quot; cannot be done
	 at link time. Instead, when we program the memory management unit (MMU), we will be
	 able to choose which bits (R=read, W=write, X=execute) we want each memory segment
	 to be able to do.
   */
  .rodata : {
    PROVIDE(_rodata_start = .);
    *(.rodata .rodata.*)
    PROVIDE(_rodata_end = .);
	/*
	   Again, we're placing the rodata section in the memory segment &quot;ram&quot; and we're putting
	   it in the :text program header. We don't have one for rodata anyway.
	*/
  } &gt;ram AT&gt;ram :text

  .data : {
	/*
	   . = ALIGN(4096) tells the linker to align the current memory location (which is
	   0x8000_0000 + text section + rodata section) to 4096 bytes. This is because our paging
	   system's resolution is 4,096 bytes or 4 KiB.

	   As a result, the current memory address is rounded off to the next nearest address that has a value that is a multiple of 4096
	*/
    . = ALIGN(4096);
    PROVIDE(_data_start = .);
	/*
	   sdata and data are essentially the same thing. However, compilers usually use the
	   sdata sections for shorter, quicker loading sections. So, usually critical data
	   is loaded there. However, we're loading all of this in one fell swoop.
	   So, we're looking to put all of the following sections under the umbrella .data:
	   .sdata
	   .sdata.[anything]
	   .data
	   .data.[anything]

	   ...in that order.
	*/
    *(.sdata .sdata.*) *(.data .data.*)
    PROVIDE(_data_end = .);
  } &gt;ram AT&gt;ram :data

  .bss : {
    PROVIDE(_bss_start = .);
    *(.sbss .sbss.*) *(.bss .bss.*)
    PROVIDE(_bss_end = .);
  } &gt;ram AT&gt;ram :bss

  /*
     The following will be helpful when we allocate the kernel stack (_stack) and
	 determine where the heap begins and ends (_heap_start and _heap_start + _heap_size)/
	 When we do memory allocation, we can use these symbols.

	 We use the symbols instead of hard-coding an address because this is a floating target.
	 Floating target means that the address space layout keeps on changing, do it becomes hard to hardcode physical adresses.
	 The heap size is not known at compile time
	 As we add code, the heap moves farther down the memory and gets shorter.

	 _memory_start will be set to 0x8000_0000 here. We use ORIGIN(ram) so that it will take
	 whatever we set the origin of ram to. Otherwise, we'd have to change it more than once
	 if we ever stray away from 0x8000_0000 as our entry point.
  */
  PROVIDE(_memory_start = ORIGIN(ram));
  /*
     Our kernel stack starts at the end of the bss segment (_bss_end). However, we're allocating
	 0x80000 bytes (524 KiB) to our kernel stack. This should be PLENTY of space. The reason
	 we add the memory is because the stack grows from higher memory to lower memory (bottom to top).
	 Therefore we set the stack at the very bottom of its allocated slot.
	 When we go to allocate from the stack, we'll subtract the number of bytes we need.
  */
  PROVIDE(_stack = _bss_end + 0x80000);
  PROVIDE(_memory_end = ORIGIN(ram) + LENGTH(ram));

  /* 
     Finally, our heap starts right after the kernel stack. This heap will be used mainly
	 to dole out memory for user-space applications. However, in some circumstances, it will
	 be used for kernel memory as well.

	 We don't align here because we let the kernel determine how it wants to do this.
  */
  PROVIDE(_heap_start = _stack);
  PROVIDE(_heap_size = _memory_end - _stack);
}

</code></pre>
<p>Our Linker script is ready !!!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-the-riscv-virtual-environment"><a class="header" href="#setting-up-the-riscv-virtual-environment">Setting up the Riscv Virtual environment</a></h1>
<p><a href="./theory_on_Qemu.html">Crude</a></p>
<p>We will be using the <a href="https://www.qemu.org/docs/master/system/target-riscv.html">Qemu RISC-V System emulator</a> to emulate a RISCV-CPU microcontroller. </p>
<h4 id="how-to-install-qemu-riscv-system-emulator-on-linux-mint"><a class="header" href="#how-to-install-qemu-riscv-system-emulator-on-linux-mint">How to install Qemu RISCV system Emulator on Linux-Mint</a></h4>
<p>At the command type</p>
<pre><code class="language-bash">sudo apt install qemu-user
sudo apt install qemu-system-misc
</code></pre>
<h4 id="qemu-configurations"><a class="header" href="#qemu-configurations">Qemu Configurations</a></h4>
<p>For QEMU’s RISC-V system emulation, you must specify which board model you want to emulate with the -M or --machine option; there is no default. In our case we will emulate the <a href="https://www.qemu.org/docs/master/system/riscv/virt.html">‘virt’ Generic Virtual Platform </a>as our target board model</p>
<p>When using the sifive_u or virt machine there are three different firmware boot options: </p>
<ol>
<li>-bios default - This is the default behaviour if no -bios option is included. This option will load the default OpenSBI firmware automatically. The firmware is included with the QEMU release and no user interaction is required. All a user needs to do is specify the kernel they want to boot with the -kernel option </li>
<li>-bios none - QEMU will not automatically load any firmware. It is up to the user to load all the images they need. </li>
<li>-bios --file - Tells QEMU to load the specified file as the firmware.</li>
</ol>
<p>We will use the following Qemu configurations ;</p>
<pre><code class="language-bash">// we define some variables 
QEMU=qemu-system-riscv64  // we are using the Riscv Qemu emulator. qemu-system-riscv64 is a variable containing the path to the QEMU executable
MACH=virt                 // we will target the Virt Riscv Machine 
CPU=rv64                  // we will use a 64-bit CPU
CPUS=4                    // The Board will have 4 CPUs... 4 HARTS
MEM=128M                  // The RAM memory will be 128 MBs
DRIVE=hdd.dsk             // This is the path to our virtual harddrive

$(QEMU) -machine $(MACH) 
        -cpu $(CPU) 
        -smp $(CPUS)     // specifies the number of CPUs to emulate
        -m $(MEM)        // specifies the amount of RAM in MBs 
        -nographic       // disables graphical output, so QEMU runs in a terminal window.
        -serial mon:stdio // connects the virtual machine motherboard's serial port to the host's system terminal. Ie, our Linux terminal. This enables us to use the terminal as a console to the virtual machine.
        -bios none       // we not depend on any firmware becaue our machine is virtual. We can just direclty load the OS image to memory. 
        -kernel $(OUT)  // This specifies the path to the kernel image file
        -drive if=none,format=raw,file=$(DRIVE),id=attic // explained below
        -device virtio-blk-device,scsi=off,drive=attic     // explained below
</code></pre>
<pre><code class="language-bash">-drive if=none,format=raw,file=$(DRIVE),id=attic
</code></pre>
<p><strong>'if=none'</strong> meant that Qemu should not create an interface between the hard drive and the Kernel image. An example of an interface is SATA interface.</p>
<p>'<strong>format=raw</strong>' means that the hard drive image should consist of raw bytes to represent data on the disk. The disk should no have extra metadata or compressions.
Other possible values for the format option include:</p>
<ul>
<li>qcow2: This is the default format for disk images in QEMU/KVM, and it supports features like compression, snapshots, and encryption.</li>
<li>mdk: This is a format used by VMware virtualization software.</li>
<li>vpc: This is a format used by Microsoft Virtual PC.</li>
<li>raw: This is similar to format=raw, but it includes a 512-byte header that specifies the disk geometry and other information.</li>
</ul>
<p>The choice of disk image format depends on the specific needs of your virtualization environment. For example, if you need to support snapshots or compression, you would likely choose qcow2. If you need to import or export the image to another virtualization platform, you may need to choose a format that is compatible with that platform.</p>
<pre><code class="language-bash">-device virtio-blk-device,scsi=off,drive=attic
</code></pre>
<p><strong>'-device'</strong> is a Qemu command for attaching new devices to the motherboard of the virtual machine.</p>
<p><strong>virtio-blk-device,scsi=off,drive=attic</strong> implies that we are adding a block device that adheres to VIRTIO protocol. '<strong>scsi=off</strong>' disables the SCSI (Small Computer System Interface), this is because we intend to write a custom virtio block driver. '<strong>drive=attic</strong>' specifies the Identifier of the new device that is being attached.</p>
<h4 id="creating-a-virtual-hard-disk"><a class="header" href="#creating-a-virtual-hard-disk">Creating a virtual hard disk</a></h4>
<p>In the configurations above, it was specified that a virtual hard disk would get attached to the motherboard. It was specified that its path would be ./hdd.dsk</p>
<p>To create this hard disk we use a tool called <a href="https://man7.org/linux/man-pages/man8/losetup.8.html">Losetup</a>. This tool converts a normal text file into a virtual block hard drive.</p>
<p>Losetup creates Loop devices. A loop device is a file that emulates a block device.</p>
<p>Losetup comes pre-installed in any standard linux distribution. To check its documentation, type this in the terminal:</p>
<pre><code class="language-bash">man losetup
</code></pre>
<p>To create a virtual disk within your development working dierctory, write the following command in your terminal: </p>
<pre><code class="language-bash">dd if=/dev/zero of=hdd.dsk count=32 bs=1M  
</code></pre>
<p>where : </p>
<ul>
<li>'<strong>if=/dev/zero</strong>: This option specifies the input file to use for the dd command. In this case, the input file is /dev/zero, which is a special file that produces an endless stream of zeroes when read.</li>
<li><strong>of=hdd.dsk</strong>: This option specifies the output file to create for the dd command. In this case, the output file is called hdd.dsk.</li>
<li><strong>count=32</strong>: This option specifies the number of blocks to copy from the input file to the output file. In this case, 32 blocks of data will be copied.</li>
<li><strong>bs=1M</strong>: This option specifies the block size to use for the dd command. In this case, the block size is 1 megabyte (1M).</li>
</ul>
<p>An alternative set of commands would be :</p>
<pre><code class="language-bash">fallocate --length 32M hdd.dsk  // create a new file called hdd.dsk and allocate to it 32 MB
sudo losetup /dev/loop0 hdd.dsk // convert hdd.dsk into a  virtual hard drive whose mount point is at /dev/loop0
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-the-build-automation-tool"><a class="header" href="#setting-up-the-build-automation-tool">Setting up the Build automation tool</a></h1>
<p>Our build tool will be cargo.<br />
We will not use third party build tools like Makefiles.<br />
It is better to not use 3rd parties.</p>
<p>So create a .cargo folder withing the repo.<br />
Create a config.toml inside the folder</p>
<p>So you have : project/.cargo/config.toml.
Inside this file, paste the following configurations : </p>
<pre><code class="language-toml">[build]
target = &quot;riscv64gc-unknown-none-elf&quot;
rustflags = ['-Clink-arg=-Tsrc/lds/virt.lds']

[target.riscv64gc-unknown-none-elf]
runner = &quot;qemu-system-riscv64 -machine virt -cpu rv64 -smp 4 -m 128M -drive if=none,format=raw,file=hdd.dsk,id=attic -device virtio-blk-device,scsi=off,drive=attic -serial mon:stdio -nographic -bios none -kernel &quot;	
</code></pre>
<p>The [build] section has configs that affect the compilation process. We tell the compiler our target platform. And tell the linker the path to the linker script.</p>
<p>The [target.riscv64gc-unknown-none-elf] section has the configs that will be considered only if we are compiling for the riscv64gc-unknown-none-elf target.<br />
THe &quot;runner&quot; specifies the cmd command that will be executed when we call &quot;Cargo run&quot;. There is a space after -kernel. This is because cargo will automatically specify the executable, whose name is configured through Cargo.toml. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="writing_a_bare_metal_rust_executable"><a class="header" href="#writing_a_bare_metal_rust_executable">writing_a_bare_metal_rust_executable</a></h1>
<p>references :</p>
<ol>
<li>https://os.phil-opp.com/minimal-rust-kernel/#a-minimal-kernel</li>
</ol>
<h2 id="1-no_std"><a class="header" href="#1-no_std">1. NO_STD</a></h2>
<p>A bare metal executable is a rust program that can run on a piece of hardware without needing an operating system.</p>
<p>Since we are building our own operating system, we need to write it as a program that is not dependent on another operating system.<br />
Normal Rust programs depend on the rust standard library. The Rust standard library itself contains functions that call OS-specific system calls. So we cannot use the Rust std library.</p>
<p>We use the core Rust Library which is not OS-specific.</p>
<p>we add the attribute #![no_std]</p>
<h2 id="2-no_main"><a class="header" href="#2-no_main">2. NO_MAIN</a></h2>
<p>Libc is a common C standard library that has been in use for a long time. It has been implemented for very many operating systems.<br />
Rust is a new language. It is very hard to implement the rust_std for all operating systems. To save on labour and allow compatibility, Rust creators decided to make the Rust Library to use libC functions instead of recreating the functions in pure Rust. Though there are some parts of the Rust std library that do not depend on libc.</p>
<p>Now that it is clear that rust_std depends on libc, when a rust bin is executed, the following events happen.</p>
<ol>
<li>The executable program is stored in memory</li>
<li>The CPU points to the first instruction of the executable (the etry point). In this case, the entry point is the C runtime.</li>
<li>The C runtime sets up its environment in preparation for the libc functions that will get called by the rust_std functions</li>
<li>After the C runtime has set up the execution environment for the libc functions, it points to the entry point of the Rust Runtime.</li>
<li>The entry point of the Rust Runtime is marked with a language item called &quot;start&quot; ie [start]</li>
<li>So the Rust runtime creates an executable environment for executing the Rust functions.</li>
<li>After the Rust runtime has finished setting up things, it looks for the &quot;main&quot; function.</li>
<li>Main starts executing</li>
</ol>
<p>Our bare metal program does not depend on the C runtime. So this sequence of events is quite irrelevant to us.<br />
What we will do is that we will inform the compiler that we wont follow this sequence by #![no_main] and then declare our own entry point.</p>
<p>To declare our own entry point, we will export a function out of the crate... like so :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[no_mangle]
pub extern &quot;C&quot; fn _start()
<span class="boring">}</span></code></pre></pre>
<p>But that is not enough, we need to tell the linker the name of our entry_point function. We do this by writing a linker script.<br />
The linker will place the code as the first part of the .text section and update the elf header sections to reflect this info.</p>
<pre><code class="language-lds">...
OUTPUT_ARCH( &quot;riscv&quot; )


ENTRY( _start )

MEMORY
{
  ram : ORIGIN = 0x80000000, LENGTH = 128M
}
</code></pre>
<h2 id="3-panic-handler"><a class="header" href="#3-panic-handler">3. Panic Handler</a></h2>
<p>Rust panics when a violatio happens. Rust requires you to define a function that will always get called after a panic happens.<br />
That function is tagged by the #[panic_handler] attribute</p>
<p>The panic_handler function never returns anything, it diverges</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use core::panic::PanicInfo;
#[panic_handler]
fn my_custom_function( panic_info: &amp;PanicInfo)-&gt; !{
    println!(&quot;message : {}&quot;, panic_info.message())
    println!(&quot;location : {}&quot;, panic_info.location())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="4-the-eh_personality--aka-error_handling-personality"><a class="header" href="#4-the-eh_personality--aka-error_handling-personality">4. The eh_personality  (aka error_handling personality)</a></h2>
<p>Rust requires you to define a function that will always get called when it wants to unwind and free a stack.<br />
This function is tagged with #[eh_personality] attribute.</p>
<p>When a panic happens, the program stops (theoretically). The program can decide to free the stack or just abort and let the underlying OS clear the stack.<br />
The thing is, to clear the stack, you have to unwind it. To unwind the stack, you have to use some hard functions...Functions that depend on some OS functionalities. This is a chicken-egg problem.</p>
<p>So we resort to aborting.</p>
<p>To specify this behaviour, you can tweak the cargo file as follows : </p>
<pre><code class="language-toml">[profile.release]
panic = &quot;abort&quot;

[profile.dev]
panic = &quot;abort&quot;
</code></pre>
<p>By default the settings are usually :</p>
<pre><code class="language-toml">[profile.release]
panic = &quot;unwind&quot;

[profile.dev]
panic = &quot;unwind&quot;
</code></pre>
<p>Now, the #[eh_personality] tag is a tag that is pegged to the function that gets called when a rust program wants to unwind its stack. eg</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[eh_personality]
fn custom_unwind(){
    // do some unwinding statements ... MAgiC!
}
<span class="boring">}</span></code></pre></pre>
<p>BUT since we have specified that our program will always abort... AND that it will never call the unwind function, we are no longer required to define the unwinding function</p>
<h2 id="5-compile-to-a-bare_metal-target"><a class="header" href="#5-compile-to-a-bare_metal-target">5. Compile to a bare_metal target</a></h2>
<p>[undone]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-bootloader"><a class="header" href="#the-bootloader">The Bootloader</a></h1>
<p>For a long time I thought the way the CPU worked was some dark magic.</p>
<p>The CPU is an indiscriminate machine, it is just a bunch of circuits that repeatedly do the operations below from the moment the CPU is powered on : </p>
<ul>
<li>It reads the memory address storeed in the the program counter register. Let us call this memory address Address_X.</li>
<li>Fetches the instruction pointed to by Address_X.</li>
<li>Executes the instruction using one or more of its circuits/gates</li>
<li>repeats infinitely</li>
</ul>
<p>It is up to us to occasionally make the program counter to point to the instrutions we want executed.<br />
To control this machine, we have to change the value in its control status registers.<br />
When you look at it from this perspective, it no longer seems like dark magic.<br />
It is a machine that can be controlled by modifying the value in relevant registers.</p>
<p>Each CPU comes with its own assembly language.</p>
<p>We will be using RISCV assembly to write the assembly code for the bootloader.<br />
The definition of what a bootloader is supposed to do varies from OS to OS. For example, some bootloaders can perform hardware power_on tests while others just leave that work to the CPU firmware. </p>
<p>A typical bootloader essentially does the following operations:</p>
<ol>
<li>Find the memory address of the Kernel program by searching through the external memory devices that have been plugged into the Motherboard.</li>
<li>Loads the Kernel image onto the RAM. Note that it only loads the required sections, it might not load the entire image.</li>
<li>Prepare the values found in the CPU registers to suit the execution of the kernel. For example making the Stack pointer to point to the stack of the kernel. </li>
<li>Transfering control to the kernel. This is done by making the program counter point to the entry_point of the kernel</li>
</ol>
<p>The Bootloader in this chapter is much simpler. It does not have to look for the kernel image in a secondary memory like a hard-disk, instead, the Kernel and the bootloader are one program. 
The Basic Bootloader in this case is a software program that :
- Finds the memory location of the kernel's entry_point (the first kernel instruction). This instruction address is still part of the bootloader. Remember that (Kernel + bootloader) == 1 united program
- Loads the kernel image to memory
- Transfers control to the kernel</p>
<p>The Bootloader has many definitions depending on the additional functionalities it has :
- Dealing with the convertion of CPI from real mode to protected mode and finally to 64-bit mode
- Having a user interface that gives its users the option to choose among multiple kernel images
- Having the bootloader code implemented in seperate parts.<br />
- Having the bootloader do some tasks that were originally meant for the firmware eg. Power_on_self tests
We do not care about the above functionalities... for now.</p>
<h3 id="the-boot-process"><a class="header" href="#the-boot-process">The Boot process</a></h3>
<p>References : </p>
<ul>
<li><a href="https://os.phil-opp.com/minimal-rust-kernel/#the-boot-process">Booting in x86 CPU</a></li>
<li><a href="https://osblog.stephenmarz.com/ch1.html">Booting In RISCV</a></li>
</ul>
<p>When the machine is powered on : </p>
<h4 id="the-firmware"><a class="header" href="#the-firmware">The Firmware</a></h4>
<p>The CPU begins its fetch-execute cycle. Typically, the PC register of the CPU points to a memory address in the ROM.<br />
The ROM contains hardcoded firmware code. In x86 this firmware can be BIOS or UEFI. 
The firmware code performs Power-on-tests on all pluged devices.<br />
The firmware code initializes the hardware : for example it maps the dedicated I/O MMIO memory and enumerates the RAM.<br />
After setting up the execution environment, it scans the plugged in secondary memory devices... Depending on the partitioning scheme used in those devices, it looks for valid bootloaders. At this point some firmware give the user a chance to choose which bootloader they would prefer among the ones the firmware has discovered</p>
<h4 id="the-bootloader-1"><a class="header" href="#the-bootloader-1">The Bootloader</a></h4>
<p>After the Firmware has set up the execution environment, It makes the CPU pointer ton point at the entr point of the boot_code.<br />
In our case, the bootloader will do the following functions :</p>
<ol>
<li>Pick only one CPU to complete the execution of the bootloader code. This is because at the beginning we do not want any kind of paralellism. Moreover, the bootcode is a simple code that can easily be done by one CPU, adding parallelism increases unnecessary complexity. It means we will have to do interprocessor communications at the start. That is just unnecessary overengineering to save a nano_secog of a nanosecond of a nanosecond</li>
<li>Clear the uninitialized memory sections : the heap and BSS section.</li>
<li>Transfer control to the Kernel code found in memory</li>
</ol>
<h2 id="bootloader-pseudo-code"><a class="header" href="#bootloader-pseudo-code">Bootloader pseudo code</a></h2>
<p>algorithm inputs : No inputs<br />
algorithm outputs : No outputs, it just calls another function that never returns.</p>
<ol>
<li>
<p>set the necessary assembler directives</p>
<ul>
<li>notify the assembler that the code should not use compressed code</li>
<li>define the memory sections
- .text.init section : the .text.init section is different from other .text sections because the .text.init section contains initialization code that gets executed before the main function and other init sections. (.text.init) section must be executed before all other sections
- .data section</li>
</ul>
</li>
<li>
<p>Choose HART 0 as the main and only core that should continue to execute the boot_code</p>
<ul>
<li>if the Hart ID is not 0, subject that core to an endless sleep</li>
</ul>
</li>
<li>
<p>Confirm that the HART is in machine mode</p>
</li>
<li>
<p>Clear the BSS section, we need no surprises.</p>
</li>
<li>
<p>Set up the regiters of the CPU to be ready to execute the kernel code</p>
</li>
<li>
<p>Call mret</p>
<ol>
<li>
<p>Set up some general registers</p>
<ol>
<li>fetch the global pointer so that we ge to access the data sections more confidently, update the global_pointer register</li>
<li>update the stack pointer to point to the bottom of the kernel stack</li>
<li>update the return address to point to an Endless sleep function... the kernel should not return in the first place</li>
</ol>
</li>
<li>
<p>Set up the control registers</p>
<ol>
<li>the mstatus register 
<ol>
<li>set previous MPP to Machine mode, this is because we intend to run the kernel in Machine mode for some time before moving to S-Mode. When we call mret instruction, the CPU will be run in the mode specified in MPP</li>
<li>set MIE mstatus bit to 1, this enables the CPU to catch interrupts</li>
<li>set MPIE mstatis bit to 1, this enables the kernel to be able to receive interrupts once we transfer cotrol to it using mret.</li>
</ol>
</li>
<li>The machine_interrupt_enable register
<ol>
<li>enable all the interrupts (Software, Timer and external)</li>
</ol>
</li>
<li>the machine_trap_vector 
<ol>
<li>Let the mtvec register point to a trap vector point... defined globally in assembly language</li>
</ol>
</li>
<li>the MEPC should point to the kmain function</li>
</ol>
</li>
</ol>
<ul>
<li></li>
</ul>
</li>
</ol>
<ul>
<li>Setup the CPU status to suit the jump to kernel code :
<ul>
<li>set the medeleg register  : we will not delegate any exception, we will handle all exceptions in Machine mode</li>
<li>set the mideleg register  : we will not delegate any interrupt to lower levels, we will handle all interrupts in Machine mode</li>
<li>set the mstatus to allow software interrupts and external interrupts</li>
<li>Set the MIE register to handle only </li>
<li>set the stack pointer to point at the bottom of the stack</li>
<li>set the mstatus register :
<ul>
<li>allow interrups to be allowed in both machine mode and supervisor mode by setting the MPP(Machine Previous Protection) to the value 3 </li>
</ul>
</li>
<li>set the MEPC ; machine exception program Counter to point to the kernel entry point.</li>
<li>Set the MTVEC : Machine Trao Vector to point to the Exception handling code</li>
</ul>
</li>
<li>Call MRET</li>
</ul>
<p><strong>Why are we calling WFI?</strong><br />
We are calling the WFI instruction to put all the other CPU cores to sleep. Our OS only uses one HART (Cpu).</p>
<p>The WFI (Wait for interrupt instruction) - This RISCV instruction powers off the CPU and only leaves a small circiut running. THis circuit continuously checks if an interrupt signal has been sent to the powered off CPU. If an interrupt is detected, the CPU gets powered on.  It is kind of a 'sleep' instruction. It can be used to save power when the CPU is idle.
We are calling the WFI instruction to put all the other CPU cores to sleep. Our OS only uses one HART (Cpu).</p>
<p><strong>Why are we disabling Riscv Compressed instruction?</strong></p>
<ul>
<li>So that we gain simplicity in debugging. </li>
</ul>
<p>In RISCV, the assembler usually encodes each assembly instruction into 32 bits. But this is not always the case, you can instruct the assembler to use compressed instructions. Compressed instructions are only 16 bits long. Not all assembly instructions get encoded to 16 bits... just a select few. This ensures memory efficient code.<br />
However, it makes it hard to debug code because the not ALL instructions are 32 bits as before.</p>
<p>we achieve this by using either of the two assembly directives : </p>
<pre><code class="language-riscv">.option norvc      // No RiscV compressed instructions
.option rvc        // Yes to RiscV compressed instructions
</code></pre>
<p><strong>Why do we need to load the global pointer when writing the Bootloader?</strong>
So that the bootloader code gets to use the correct global data associated to the Operating system image.</p>
<p>The global pointer references the base address of global data in the memory map.<br />
To access any global data, you have to know the base address + offset.<br />
The bootloader typically gets executed as a seperate program from the operating system. So the bootloader may have a different memory map from the memory map of the kernel.<br />
Considering that the bootloader needs to use the operating system's global data, we make the global pointer that the bootloader references to be the gp found in the memory map of the kernel.</p>
<p>Now this operation is delicate; we are accessing another memory map to access global data that may or may not be the same as the data we are trying to change. We need to make this operation explicit, no surprises. So we temporarily kill all code optimizations when doing this operation by using the directives :</p>
<pre><code class="language-riscv">.option push       // save previous assembly directives... because in the next few lines we may use contradicting directives
.option norelax    // no optimization
.option pop        // restore previous assembly directives
</code></pre>
<h2 id="designs"><a class="header" href="#designs">Designs</a></h2>
<h4 id="initial-program-flow-diagram"><a class="header" href="#initial-program-flow-diagram">Initial Program Flow diagram</a></h4>
<body>
  <pre class="mermaid">
        graph TD 
        A[Qemu ELF loader] -->|loads Elf file containing kernel loader| B[kernel Loader] 
        B --> |prepares the CPU registers for kernel, calls kernel entry point| C[Kernel Runs] 
  </pre>
</body>
<h4 id="kernel-loader-sequence-of-events"><a class="header" href="#kernel-loader-sequence-of-events">Kernel loader sequence of events</a></h4>
<pre class="mermaid">    graph TD 
        A[set assembler directives] --&gt; B[Look for HART 0] ;
        B --&gt; C{Is the Core HART 0?} ;
        C --&gt;|Yes| D[Clear Kernels BSS section];
        C --&gt;|No| E[Put HART to sleep];
        D --&gt; F[initialize CPU registers for kernel];
        F --&gt; G[summon kmain];
</pre>
<div style="break-before: page; page-break-before: always;"></div><p>Qemu --&gt; Boot --&gt; kinit --&gt; kmain<br />
Kinit messes around with physical memory in Machine mode while kmain messes with virtual memory in Supervisor mode<br />
Kinit gets us to kmain.
under kinit we do not accept any interrupts : This allows us to setup our machine without any disturbance from other cores or the PLIC </p>
<ul>
<li>we make mepc point to kinit</li>
<li>we make the return address of kinit point to the asm funtion that will transition us to kmain ; kinit returns ()</li>
<li>we call mret and jump into kinit rust</li>
</ul>
<p>And what are we doing under kinit? <a href="the_bootloader_2.html#kinit">here</a></p>
<ul>
<li>when kinit returns, we are on the function to transition to kmain</li>
<li>we set the mstatus register
<ul>
<li>set MPP to 01 (supervisor)</li>
<li>set Previous machine interrupt-enable bit is 1 (MPIE=1 [Enabled])</li>
<li>wet Previous interrupt-enable bit is 1 (SPIE=1 [Enabled]).</li>
</ul>
</li>
<li>setnputs : No inpu mtvec to mtrap_vector</li>
<li>set mepc to kmain</li>
<li>set which specific interrupts are allowed by setting the MIE register
<ul>
<li>1 &lt;&lt; 1    : Supervisor software interrupt enable (SSIE=1 [Enabled])</li>
<li>1 &lt;&lt; 5    : Supervisor timer interrupt enable (STIE=1 [Enabled])</li>
<li>1 &lt;&lt; 9    : Supervisor external interrupt enable (SEIE=1 [Enabled])</li>
<li>0xaaa = MEIP/SEIP and MTIP/STIP and MSIP/SSIP</li>
</ul>
</li>
<li>set return register to point to Shutdown. If main returns () ... shutdown</li>
</ul>
<h2 id="kinit"><a class="header" href="#kinit">kinit</a></h2>
<p>Before kinit make sure that all linker variables are accessible in Rust 
Make kinit global for the linker using &quot;extern&quot;</p>
<p>What does kinit do?</p>
<ul>
<li>initialize the UART system
<ul>
<li>declare a single instance of the UART object</li>
<li>initialize the UART instance (configure the UART registers for communication)</li>
</ul>
</li>
<li>Initialize the page allocator system
<ul>
<li>clear the page destriptors in the heap</li>
<li>define ALLOC_Start, where the Pages Start</li>
</ul>
</li>
<li>Initialize byte allocation system (Borrow this)
<ul>
<li>define the number of kernel pages (512 pages)</li>
<li>create a kernel heap by allocating those pages, and zeroing them out</li>
<li>set the first AllocStruct</li>
</ul>
</li>
<li>Allocate Space for the Root Page Table immediately after the kernel Heap</li>
<li>Set up the values of the paging 
<ul>
<li>Get position of the root table that was created during kmem::init()</li>
<li>Identity Map all the known kernel addresses specified in the linker script</li>
<li>Map the Kernel Heap </li>
<li>Map the UART</li>
<li>Map the PLIC</li>
<li>Map the CLINT</li>
<li>Define the SATP value</li>
</ul>
</li>
<li>Define the address of the TRAP_FRAME for our current HART. Each HART gets one TRAP_FRAME (it is global)
<ul>
<li>Store the address of the TRAP_FRAME in the mscratch register</li>
<li>Store the address of the TRAP_FRAME in the sscratch register</li>
<li>Map the TRAP_FRAME address for this HART</li>
</ul>
</li>
<li>initialize the page memory allocation system</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-communications"><a class="header" href="#setting-up-communications">Setting Up Communications</a></h1>
<p>References :<br />
- <a href="https://docs.rust-embedded.org/book/peripherals/index.html">Chapter 3 : How to write Drivers for Peripherals</a><br />
- <a href="https://osblog.stephenmarz.com/ch2.html">Communications : the UART Driver</a><br />
- <a href="https://www.lammertbies.nl/comm/info/serial-uart">The UART specifications</a><br />
- <a href="http://caro.su/msx/ocm_de1/16550.pdf">The UART Datasheet</a></p>
<p>Our OS will communicate with peripherals. Peripherals are external devices...external cirtuits... things that are not originally part of the Motherboard.<br />
In this chapter, the peripherals that we will be attaching will be the console output and the keyboard for input.<br />
The connection between the microcontroller and both of these peripherals is a serial connection.</p>
<h3 id="why-a-serial-connection"><a class="header" href="#why-a-serial-connection">Why a serial connection?</a></h3>
<p>Inside the motherboard and the peripherals, there are parallel connections connecting the  internal components.<br />
Parallel connections are wide. manufacturing wide cables is expensive. So people resorted to just manufacturing single line cables.<br />
And then they created algorithms to convert parallel bits to a stream of contiguous bits ... and vice versa.</p>
<h3 id="the-uart-driver"><a class="header" href="#the-uart-driver">The UART Driver</a></h3>
<p>The UART device is a hardware device that stands in between a parallel connection and a serial connection. It converts a serial signal to a parallel signal and vice-versa.<br />
It stands between the motherboard and the peripheral devices that use serial connections ; eg mouse, keyboard and console output.</p>
<h4 id="why-are-we-not-using-the-usb-driver"><a class="header" href="#why-are-we-not-using-the-usb-driver">Why are we not using the USB driver?</a></h4>
<p>The USB also does the conversion of parallel signals to serial signals. The USB has higher transfer speeds than the UART connection. The USB can do 20 Gbps while the UART does around 1 Mbps.<br />
However, we used the UART because of its simplicity in configuration : this is a learning project, we cannot afford the complex nature of trying to configure the USB.</p>
<p>In the case of Qemu, the console output and the keyboard use the same UART device. This is because the transmit_out channel is connected to the console output AND the receive channel is connected to the keyboard.</p>
<p><img src="./images/UART/UART_system_components_layout.png" alt="the UART system layout of components" />
<img src="./images/UART/UART_ecosystem.png" alt="the UART system" /></p>
<h4 id="theory-of-parallel-to-serial-conversion"><a class="header" href="#theory-of-parallel-to-serial-conversion">Theory of parallel to serial conversion</a></h4>
<p>Before we discuss the actual registers, let's discuss the theory of parallel-to-serial conversion.</p>
<ol>
<li>On the Sender's side
The serializer gets configured by the host system. The baud rate is set, the size of a data frame is set and interrupt handling is configured.
The serializer hardware receives parallel input... let's say 8 bits.<br />
The serializer sequentially pushes each bit of that parallel input into a shift register. Whether it begins from the Most Significant bit or the LSB is up to the specifications of the communication protocol being used.<br />
The serializer packages the shift register bits as a data frame, with a startbit, a stop-bit and maybe a parity bit for error checking.<br />
The serializer sequentially pushes the bits out of the shift register and into the transmission channel.<br />
The way it pushes the bits into the line can be either FIFO or FILO.<br />
The rate at which it pushes the bits into the transmission line is called the Baud Rate. It is measured in bits per second. In this case, baud rate is the same as bit rate because each bit change equals a bit transmission.</li>
</ol>
<p>2 On the receiver's end
It is assumed that the receiver has the same configurations as the sender: same baud rate, same data_frame size.<br />
The deserializer receives the serial data frame.<br />
checks the parity bit for error detection</p>
<h4 id="the-uart-registers"><a class="header" href="#the-uart-registers">The UART Registers</a></h4>
<p>The UART emulated in Qemu is the NS16550A UART chipset. We control the UART using MMIO programming. The Base address of the UARTs begins at 0x1000_0000 and each UART device is given an offset of 0x100 (256 bytes)</p>
<p>The UART has 8 physical registers that can be interpreted as 12 logical registers... this is because some of the physical registers can be used differently under different contexts. For example Register 000 can be used as an input register when the UART is idle, but when the UART is not idle, the same register will be treated as an output register.</p>
<p>Below is a diagramatic representation of the UART registers :<br />
<img src="./images/UART/UART%20registers%20official%20DOCS.png" alt="The UART Registers" /><br />
Notice that there are registers that share the same physical space. For exapmple.... see below<br />
<img src="images/UART/UART_registers_expreted.png" alt="" /></p>
<p>From the image, there are only 8 bytes of spaces used to represent all the 12 registers. This is becuse there are registers that share byte space : </p>
<ol>
<li>The Receive Buffer Register(RBR), The Transmitter Holding Register(THR) and the  Divisor Latch Least Significant Byte (DLL) can occupy the same byte space.</li>
<li>The interrupt Status Register and the FIFO control Register occupy the same byte space.</li>
<li>The Line Status Register and the Prescaler Division  can occupy the same byte space.</li>
<li>The Interrupt Enable Register and the Divisor Latch Most significant Byte share the same byte space.</li>
</ol>
<p>All registers are 8 bits long. We will not discuss the DLL, DLM and PSD registers.</p>
<h5 id="1-the-line-control-register"><a class="header" href="#1-the-line-control-register">1. The Line Control Register</a></h5>
<p>The line control Register is used to configure the UART communications.<br />
Using this register, you can set the format of the data frames being transported.<br />
Using this register, you can set the size of the data frame.<br />
Using this register, you can determine whether we will be able to set a custom baud rate or use the default baud rate.</p>
<ul>
<li>Bits [1:0] are used to set the word_length ie. the length of the Data frame. The legal values are :
<img src="images/UART/word_lenth.png" alt="" /></li>
<li>Bit [2] is used to set the number of stop bits to be included in the transmission frame : 0==one stop_bit and  1 == two stop bits
<img src="images/UART/parity_bit_values.png" alt="" /></li>
<li>Bits [5-3] are used to set the parity type of the data frame to be transmitted</li>
<li>Bit[7] is the DLAB bit. Setting this to 1 means that we get to acces the DLL, DLM and PSD registers to set the baud rate</li>
</ul>
<h5 id="2-the-line-status-register"><a class="header" href="#2-the-line-status-register">2. The Line Status Register</a></h5>
<p>This register contains the info about the communicatiuon line. If any error occur, they are also reflected in this register: </p>
<p>undone (dexcribe the rest of the registers)</p>
<p>Regit</p>
<h4 id="initializing-the-communication-between-the-2-devices"><a class="header" href="#initializing-the-communication-between-the-2-devices">Initializing the Communication between the 2 devices.</a></h4>
<p>Initializing the communications between the 2 devices means that we configure the UART protocol.<br />
We need to define :</p>
<ol>
<li>The maximum amount of bits that can be contained in the buffer at a time. The 2 devices need to agree on this so that data does not get lost if one of the devices buffer is too small.</li>
<li>Set the order of reading and writing to the communication buffer. In our case, we set the order to FIFO (First in First Out).</li>
<li>Define how the CPU and the UART will communicate with each other when the buffer is ready to be read or written to. You can choose between 2 methods ; Poll driven communication od Interrupt driven communication.<br />
In Interrupt driven communication, the UART device sends interrupt signals to the CPU whenever the read buffer is full. And consequently, the CPU invokes the appropriate interrupt handler. You have the option to involve the PLIC or not.<br />
In the Poll driven communication, the CPU will occasionally check if the buffer is ready to be read or written to.</li>
<li>Set the data transfer speed between the 2 devices - the baud rate. This is to avoid data loss.</li>
</ol>
<p>But here is a relief : If we set the DLAB (Divisor Latch access bit) to zero, then :</p>
<ul>
<li>The Prescaler Division Register becomes inaccessible. Meaning that the Line status register does not have to share the byte space.</li>
<li>The DLL register becomes inaccessible, meaning the RHR and THR don't have to share the byte space.</li>
<li>The DLM register becomes inaccessible, meaning that the Interrupt Enable Register does not have to share the byte space.</li>
</ul>
<p>That is the path we will take, we will set the DLAB bit to zero so that we access the registers in a more simple manner. The reason that makes it okay to disregard setting the DLL, DLM and PDS registers is because we do not need them. These three registers are used to set the Baud Rate of the UART device.</p>
<p>We are in Qemu, this is a virtual space. This means that we are not dealing with real physical devices. Because of this fact, it is not necessary to set the Baud Rate. The machine emulates the maximum baud rate available.</p>
<p>The Baud Rate formula is as follows :<br />
Divisor = UART_device_clock_frequency / ( Baud_rate x Prescaler_Division_value)</p>
<p>where :</p>
<ul>
<li>Divisor is a 16 bit value whose first 8 bits get stored in the DLM register and last 8 bits get stored in the DLL register. If we do not set the DLL and DLM bit, the Divisor value is assumed to be  65,536 (2^16).</li>
<li>UART_device_clock_frequency is the clock speed of the particular UART implementation. eg 16MHz</li>
<li>The Baud_rate is the rate at which data transfers, this is what we are trying to calulate.</li>
<li>The Prescaler_Division_value is found in the Prescaler_Division register. It is represented using 4 bits, its value ranges from 1 to 16. If we do not set the prescaler division value, 16 is assumed to be the default. We are okay with 16</li>
</ul>
<p>We will use this registers to :</p>
<ol>
<li>Initialize the communication between the 2 devices</li>
<li>fetch and write data to the communication buffers</li>
</ol>
<h5 id="pseudocode-for-initializing-the-uart-communication"><a class="header" href="#pseudocode-for-initializing-the-uart-communication">Pseudocode for initializing the UART communication</a></h5>
<p>input : the UART base address (hopefully from a struct abstract)
output : No_output</p>
<ol>
<li>set the word length to 8 bits
<ul>
<li>access the lCR register</li>
<li>set bits [1:0] to [1:1]</li>
</ul>
</li>
<li>Enable FIFO reads and writes
<ul>
<li>access the FIcaler Division RegisFO control Register</li>
<li>turn the FIFO enable bit to 1</li>
</ul>
</li>
<li>Set the interrupt capability 
<ul>
<li>Access the Interupt enable Register</li>
<li>enable the data ready interrupt [set bit 0 to the value 1]</li>
<li>enable the transmitter empty interrupt [set bit 1 to the value 1]</li>
</ul>
</li>
<li>Set the Baud rate
<ul>
<li>since we are setting the BAUD rate at 2400, the divisor value is 592</li>
<li>use bitmasking to separate the most sigificant bytes from the least significant bytes</li>
<li>Set the DLAB bit to 1 in order to allow setting the Baud rate</li>
<li>store the two seperate values in the DLL and DLM respectively</li>
</ul>
</li>
</ol>
<h4 id="reading-from-the-uart"><a class="header" href="#reading-from-the-uart">Reading from the UART</a></h4>
<p>When the buffer is full, the UART sends an interrupt to the PLIC. The PLIC calls the interrupt handler. The Interrupt handler invokes a UART read (This function that we are writing now). The read input can be directly displayed in the console output or stored in a buffer</p>
<h5 id="pseudocode"><a class="header" href="#pseudocode">Pseudocode</a></h5>
<p>input : the UART base address (hopefully from a struct abstract)
output : Option&lt;None, BYTE&gt;</p>
<ol>
<li>Access the Line Status Register</li>
<li>Access the data_ready status bit.</li>
<li>If the data Bit is 1, continue to step 5</li>
<li>If the data bit is 0, return None</li>
<li>Read the RHR buffer and return the read byte.</li>
</ol>
<p>When the interrupt handler calles read function, it stores the inputs in a curcular buffer. From this buffer you can choose hich inputs to display to the console or process</p>
<h5 id="control-flow-diagram-of-the-uart-read"><a class="header" href="#control-flow-diagram-of-the-uart-read">Control flow diagram of the UART read</a></h5>
<pre class="mermaid">    graph TD
        A[ Access the Line Status Register] --&gt; B[Access the data_ready status bit];
        B --&gt; C{Is the Data Ready?} ;
        C --&gt; |Yes| D[Read the RHR buffer];
        D --&gt; E[return the read byte];
        C --&gt; |No| E[return None type]
</pre>
<h4 id="writing-the-uart"><a class="header" href="#writing-the-uart">Writing the UART</a></h4>
<p>When the buffer is empty, the UART sends an interrupt to the PLIC. The PLIC calls the interrupt handler. The Interrupt handler checks if the UART_console_write_buffer is empty. If the Buffer is empty, the Interrupt handler does not call the UART_write function.
If the UART_console_write_buffer is not empty, the interrupt handler calls the console_output_feeder.</p>
<h5 id="pseudocode-1"><a class="header" href="#pseudocode-1">Pseudocode</a></h5>
<p>input : the UART base address (hopefully from a struct abstract), the byte to be written
output : Result&lt;Ok, Err&gt;</p>
<ol>
<li>Access the Line Status Register</li>
<li>Confirm that the THR is empty</li>
<li>If the data Bit is 1, continue to step 5</li>
<li>If the data bit is 0, return Err</li>
<li>Write to the THR buffer and return an OK().</li>
</ol>
<p>When the interrupt handler calles read function, it stores the inputs in a curcular buffer. From this buffer you can choose hich inputs to display to the console or process</p>
<h5 id="control-flow-diagram-of-the-uart-read-1"><a class="header" href="#control-flow-diagram-of-the-uart-read-1">Control flow diagram of the UART read</a></h5>
<pre class="mermaid">    graph TD
        A[ Access the Line Status Register] --&gt; B[Access the THR_Empty status bit];
        B --&gt; C{Is the THR Empty?} ;
        C --&gt; |Yes| D[Write to the THR buffer];
        D --&gt; E[return an OK];
        C --&gt; |No| F[return Err]
</pre>
<h4 id="uart-buffer-management"><a class="header" href="#uart-buffer-management">UART Buffer management</a></h4>
<p>Data coming from the keyboard can either be displayed to the console or stored for further processing. For this reason we will have a ring buffer for storing read input. Whatever you choose to do with this data is up to you</p>
<p>With this input buffer, we can implement scanf. 
We can also confortably implement a function that continuously reacts keyboard input. (eg, continuous console display)</p>
<p>Data coming from the programs might also need to be displayed . That is why we have the UART_console_write_buffer. Strings from the user or kernel programs get stored in this queue. From there, when we receive a THR_empty interrupt, </p>
<h4 id="driver-exposed-api"><a class="header" href="#driver-exposed-api">Driver exposed API</a></h4>
<ul>
<li>Clear input buffer (everyone)</li>
<li>Read input buffer (everyone)</li>
<li>Read line  (Everyone)</li>
<li>Read word  (Everyone)</li>
<li>Write word (Everyone)</li>
<li>Write line (Everyone)</li>
<li>Determine interrupt type (User PLIC)</li>
</ul>
<h4 id="interaction-with-the-plic"><a class="header" href="#interaction-with-the-plic">Interaction with the PLIC</a></h4>
<p>The PLIC receives interrupts from the UART whenever the Data Buffer is full_and_ready_to_be_read. When the PLIC receives this interrupt, it does not really know what the interrupt is all about. So it needs to read in the UART Line Status Register and determine which event caused the interrupt. This situation is valid only when the UART is set to send interrupts because of many reasons.</p>
<p>However in our case, the PLIC sends interrupts only when Data Buffer is full_and_ready_to_be_read, so we do not need a function to determine the type of interrupt. the type of interrupt is always &quot;data Ready&quot;</p>
<h3 id="principles-when-abstracting-hardware"><a class="header" href="#principles-when-abstracting-hardware">Principles when abstracting hardware</a></h3>
<ol>
<li>The struct used to abstract the hardware MMIO map should be compiled in a predictable manner (C ABI)</li>
<li>We should always use volatile reads and writes to the MMIO registers</li>
<li>In software, we should be able to share any number of read-only accesses to these peripherals</li>
<li>If some software should have read-write access to a peripheral, it should hold the only reference to that peripheral</li>
</ol>
<h4 id="why-use-the-c-abi"><a class="header" href="#why-use-the-c-abi">Why use the C ABI?</a></h4>
<p>The C ABI is used for the sake of compatibility and predictability. For example, the Rust struct fields can get re-ordered while the C structs field order is static. For example, if a struct field has 2 integer fields and 1 boolean, normally the field order would be as declared. But if the boolean is accessed more frequently, it might be re-odered and placed at the beginning to improve cache locality.<br />
In this case, we need the struct order as static.</p>
<h4 id="why-use-volatile-reads-and-writes"><a class="header" href="#why-use-volatile-reads-and-writes">Why use volatile reads and writes?</a></h4>
<p>When you write to the same memory address for a couple of times, the compiler might optimize the code and just consider the last write value. Meaning you will lose a couple of writes due to compiler optimization.</p>
<p>When you read from the same memory a couple of times, the compiler might optimize the reads using a cache;
Volatile read the image, there are onl</p>
<h4 id="why-we-wont-use-global-mutable-variables-to-abstract-our-uart-driver"><a class="header" href="#why-we-wont-use-global-mutable-variables-to-abstract-our-uart-driver">Why we won't use global mutable variables to abstract our UART driver</a></h4>
<p>In Rust, global mutable variables are considered unsafe. You have to enclose any interactions with them using the unsafe block.<br />
They are unsafe because :</p>
<ol>
<li>They can lead to data races when multiple threads have r/w access to it. --&gt; </li>
<li>They can lead to memory unsafety. 
<ul>
<li>If the global variable is uninitialized and a read operation is executed, it may cause a null dereferencing problem. There is a chance of this happening because the thread that was taking care of initialization did not execute before the thread that read the global variable.</li>
<li>If the global variable is not initialized in a timely manner, a write to the location may cause a bufferoverflow.</li>
</ul>
</li>
<li>It is hard to organize code that is full of global mutable variables. It is hard to debug such a program.</li>
</ol>
<p>So ... moral lesson : Try your best to never use global mutable variables.</p>
<p>We can implement the 3rd and 4th <a href="setting_up_comunications.html#principles-when-abstracting-hardware">Principles</a> using the Singleton structure. Our singleton strategy is not thread safe (undone)</p>
<h1 id="i2c"><a class="header" href="#i2c">I2C</a></h1>
<ul>
<li>packet-wise, has start and stop bits. Not streaming</li>
<li></li>
</ul>
<h1 id="spi"><a class="header" href="#spi">SPI</a></h1>
<ul>
<li>continuous, not packet-wise. data does not have  start and stop bits</li>
<li>data can be transferred without interruption. Any number of bits can be sent or received in a continuous stream.... NOT packetwise</li>
<li></li>
</ul>
<h1 id="uart"><a class="header" href="#uart">UART</a></h1>
<ul>
<li>packet-wise, has start and stop bits. Not streaming</li>
<li></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="communications_theory"><a class="header" href="#communications_theory">communications_theory</a></h1>
<p>[references]</p>
<ol>
<li><a href="https://www.circuitbasics.com/basics-uart-communication/">UART communication protocol</a> - just read this </li>
<li>https://www.analog.com/en/analog-dialogue/articles/uart-a-hardware-communication-protocol.html</li>
</ol>
<p>[undone] : outline the process</p>
<p>You need to configure the UART devices on both the receiver and sender.</p>
<p>Configuration :</p>
<ol>
<li>Set the baudrate : communicating devices need to be within 10% range of difference</li>
<li>Are we going to include the parity bit in our transmissions?</li>
<li>How are the Interrupts handled? </li>
</ol>
<h2 id="configuring-the-baud-rate"><a class="header" href="#configuring-the-baud-rate">Configuring the Baud rate:</a></h2>
<p>The Baud Rate formula is as follows :<br />
Divisor = UART_device_clock_frequency / ( Baud_rate x Prescaler_Division_value)</p>
<p>where :</p>
<ul>
<li>Divisor is a 16 bit value whose first 8 bits get stored in the DLM register and last 8 bits get stored in the DLL register. If we do not set the DLL and DLM bit, the Divisor value is assumed to be  65,536 (2^16).</li>
<li>UART_device_clock_frequency is the clock speed of the particular UART implementation. eg 16MHz</li>
<li>The Baud_rate is the rate at which data transfers, this is what we are trying to calulate.</li>
<li>The Prescaler_Division_value is found in the Prescaler_Division register. It is represented using 4 bits, its value ranges from 1 to 16. If we do not set the prescaler division value, 16 is assumed to be the default. We are okay with 16</li>
</ul>
<p>We will use this registers to :</p>
<ol>
<li>Initialize the communication between the 2 devices</li>
<li>fetch and write data to the communication buffers</li>
</ol>
<p>Now, there is a conflict, The DLL bit occupies the same space that the THR and RHR occupy.<br />
This problem is solved by doing the baud rate configuration once. After that the space is left for RHR and THR.<br />
That space is only usable for the DLL only if the DLAB bit is set to one. When the DLAB bit is set to 1, the UART realized that it is configuration time and dedicates the register to store the DLL bit before it does the bitrate calculation.</p>
<p>So the DLAB bit acts as a switch  :
When the DLAB bit = 1 then...</p>
<ol>
<li>register 000 stores the DLL value</li>
<li>register 000 stores the DLM value</li>
<li>register 101 stores the Prescaler value</li>
</ol>
<p>Once the baud rate configuration is complete, the DLAB bit is set to 0, and the register space that was used for DLL and DLM becomes available for accessing the THR and RHR, respectively. The UART controller switches back to data transmission/reception mode.</p>
<h2 id="parity--bit"><a class="header" href="#parity--bit">Parity  Bit</a></h2>
<p>No we do not care about this. It is important, but we are in a virtual environment, why would bits get lost in transmission? We are in a perfect world.</p>
<h2 id="interrupt-handling"><a class="header" href="#interrupt-handling">Interrupt handling</a></h2>
<p>THe UART is conected to the CPU via the PLIC. The UART send out 5 kinds of interrupts to the CPU.</p>
<ol>
<li>
<p>Receiver Data Available Interrupt (RDA): This interrupt is triggered when data is received and available in the receiver buffer (RBR/RHR) to be read by the CPU.</p>
</li>
<li>
<p>Transmitter Holding Register Empty Interrupt (THRE): This interrupt is triggered when the transmitter buffer (THR) is empty and ready to accept new data to be transmitted.</p>
</li>
<li>
<p>Receiver Line Status Interrupt (RLS): This interrupt is triggered when a line status error occurs during data reception, such as framing errors, parity errors, or overrun errors.</p>
</li>
<li>
<p>Modem Status Interrupt (MS): This interrupt is triggered when there are changes in the modem status signals, such as changes in the CTS (Clear To Send) or DSR (Data Set Ready) signals.</p>
</li>
<li>
<p>Receiver Time-Out Interrupt (RTO): This interrupt is triggered when the receiver is idle for a specified duration, indicating the end of a data transmission.</p>
</li>
</ol>
<p>In our case, We only care about the first 2 interrupts.</p>
<p>To enable the UART device to be able to send interrupts to the CPU, we have to tweak the bits of the Interrupt enable register.</p>
<ul>
<li>FIFO buffer : where you store bytes if you have not processes the current byte in buffer</li>
<li>The UART 16655 has a max 16 byte FIFO buffer, but you can set it to one byte. This is called FIFO depth</li>
<li>We do not currently care about Modem conreol signals</li>
<li>I do not understand this DMA operation? Do we need to enable it?</li>
<li>No clock, fully asynchronous</li>
</ul>
<h2 id="registers"><a class="header" href="#registers">Registers</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="theory_on_paging"><a class="header" href="#theory_on_paging">theory_on_paging</a></h1>
<p>references :</p>
<ul>
<li>https://os.phil-opp.com/paging-introduction/</li>
</ul>
<p>Supervisor mode in riscv introduces a paging system that pages the physical memory.<br />
But before we discuss the paging system that is specific to RISCV, we need to understand paging theory by itself.</p>
<h2 id="virtual-addresses-and-phsical-addresses"><a class="header" href="#virtual-addresses-and-phsical-addresses">Virtual addresses and Phsical addresses</a></h2>
<p>Before we discuss paging, let's first find out what virtual addresses and physical addresses are.</p>
<p>A <strong>Physical memory address</strong> is the constant and full address of a physical space in memory. It is the address pf a physical space in memory. Each physical memory address is unique. And each physical address represents a single unique physical space.</p>
<p>A <strong>virtual memory address</strong> is like a variable that contains the Physical memory address as its value. Just like in programming, variables are unique only in a certain scope. Different variables can contain the same value... the same way different virtual addresses can represent the same physical memory addresses.</p>
<p>A virtual address can be translated to its equivalent physical address.<br />
We need virtual addresses so that we can manipulate memory just like how we manipulate variables.<br />
Virtual Addresses are a way of abstracting the physical memory. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="segmentation"><a class="header" href="#segmentation">segmentation</a></h1>
<p>If the CPU has 16-bit registers, it means that by default, it can only access 2^16 physical memory addresses.  That means that ideally, if each physical address in your computer represents one byte then you can only use 16 KiB of RAM.<br />
If the CPU has 32-bit registers, it means that by default, it can only access 2^32 physical memory addresses.  That means that ideally, if each physical address in your computer represents one byte then you can only use 4 GiB of the RAM.<br />
If the CPU has 64-bit registers, it means that by default, it can only reference 2^64 physical memory addresses. That means that ideally, if each physical address in your computer represents one byte then you can only use 16777216 TB of RAM.</p>
<p>Now the problem is that the amount of RAM usable by the CPU is determined by the register size of the CPU. This is not good.<br />
So people came up with something called segmentation.</p>
<p>Under segmentation...</p>
<ol>
<li>You insert RAM of any size to your computer, do not give a hoot about the register size of your CPU.</li>
<li>Segment the whole RAM into segments, The number of segments should be equal to the number of addresses that your CPU can ideally access. For example if I am using a 16-bit CPU, I would divide the RAM into 2^16 segments.</li>
<li>Hardwire the CPU to only access the start address of each segment by default. Now the inner addresses contained in a segment have become inaccessible by the CPU.</li>
<li>Come up with new registers that store the offset if an address in relation to a segment start.</li>
<li>Develop a special hardwired circuit that takes in the segement_start address AND the offset_value and directly reads from the inner sections of the segments</li>
<li>MAGIC!! now you can access all RAM's physical memory addresses.</li>
</ol>
<p><img src="images/paging/Segmentation_to_the_rescue.jpg" alt="Segmentation circuit saves the day" /></p>
<p>Now the work of the CPU  was to pass the Offset and memory address to the segmentation circuit... and in turn it will get a value from memory.<br />
You can say that (memory + offset )== virtual address. Remember that a virtual addres is an address that can be translated to a physical address.</p>
<h3 id="segmentation-becomes-weak"><a class="header" href="#segmentation-becomes-weak">Segmentation becomes weak</a></h3>
<p>Segmentation was doing fine for some time.<br />
BUT segmentation allowed you to use contiguous virtual addresses to reference contiguous physical memory addresses.<br />
You could not use contiguous virtual addresses to reference NON-contiguous physical memory addresses. This means that if you had to allocate memory, you had to find contiguous physical memory addresses... if there were only holes in memory, you HAD TO DEFRAGMENT the entire physical memory.</p>
<p>Defragmentation is time_expensive and introduces performance non_determinism.</p>
<p>Happy path : There is enough contiguous space<br />
<img src="images/paging/1.png" alt="" /></p>
<p>Now there is no enough contiguous space :
<img src="images/paging/2.png" alt="" /></p>
<p>You are forced to de-fragment the physical memory so everything fits:
<img src="images/paging/3.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paging"><a class="header" href="#paging">paging</a></h1>
<p>Paging says : Divide the virtual address space into very tiny blocks called pages. Divide the physical memory into tiny blocks too, called frames. Make sure that a page can map to a frame. Each page can be individually mapped to a frame, which makes it possible to split larger memory regions across non-continuous physical frames.</p>
<p><img src="images/paging/4_no_defragmentation_needed.png" alt="" />
And here goes.... No defragmentation needed because physical memory allocation need not be done massively contiguously. The size allocations are small, so they can fill fragmentation holes,</p>
<h2 id="page-tables"><a class="header" href="#page-tables">Page Tables</a></h2>
<p>We end up having millions of pages that can be individually mapped to individual frames.<br />
This mapping information is stored in a page table. The CPU motherboard may provide physical hardware that implements these Page tables... OR you can implement the page tables on your own using code. </p>
<p>Here is a visual representation of Page Tables :
<img src="images/paging/page_tables_basic.png" alt="" /></p>
<p>From the Image, you can see that the individual Programs just get allocated pages in a contiguous fashion... they just get pages that are indexed procedurally. And those virtual page indexes are infact identical to virtual page indexes used in other programs.<br />
Just like how variables are only meant to be unique within a scope, virtual page addresses only need to be unique only within a program.</p>
<p>Each progam is isolated from all other programs. So it is an isolated scope.<br />
Each program gets a Page Table that contains information about how each used page maps to a frame. You can also throw in the Access_flags to the Page Table if you want to.</p>
<p>So the OS has to store the Page Table index associated with each process.<br />
When the CPU needs to execute a process, it must use the specific Page table associated with that process.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-memory-management"><a class="header" href="#setting-up-memory-management">Setting Up Memory Management</a></h1>
<p>Our OS is dealing with different kinds of memory. We have : </p>
<ol>
<li>The RAM</li>
<li>The I/O memory</li>
<li>The Secondary Memory (The HDD, Hard Disk)</li>
</ol>
<pre class="mermaid">    graph TD
        A[Kernel] --&gt;|contains| B[RAM Memory Manager];
        B --&gt;|that Manages| C[RAM] ;
        A[Kernel] --&gt;|contains a | D[ Secondary Memory Manager];
        D --&gt;|that uses a| E[File System] ;
        E --&gt;|and a | F[Hard Disk Driver] ;
        F --&gt;|to manage the | G[Physical Hard Disk];
        A[Kernel] --&gt;|contains| H[MMIO device Drivers];
        H --&gt;|that Abstract| I[Specific Regions of the I/O Memory] ;
</pre>
<p>We have to manage each of these memory types. In this case management means :</p>
<ol>
<li>Safely Abstracting the physical memory bytes using software.</li>
<li>Define methods of correctly allocating memory.</li>
<li>Define methods of correctly deallocating memory.</li>
<li>Defining methods of accessing the correct memory sections.</li>
</ol>
<p>Each memory type has different implementations of the above three functions.<br />
For example, we need to define a file system as a way of managing the Secondary memory. 
As for the RAM, we need to define a Virtual Paging System that has access control capabilities.</p>
<p>We will not manage I/O memory that much, we will use structs to abstract the different MMIO regions. This feature is not essential. [undone]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-ram-management"><a class="header" href="#the-ram-management">The RAM Management</a></h1>
<h3 id="how-much-ram-do-we-have"><a class="header" href="#how-much-ram-do-we-have">How much RAM do we have?</a></h3>
<p>Qemu is a virtual environment... meaning we get to define how much RAM our machine has. We are humble and efficient... so we choose 128 MBs only... right?</p>
<p>The RAM that we are dealing with is only 128 MBs as specified in our Qemu configuration : You can see a &quot; -m 128M&quot; setting in the build and run Config file :</p>
<pre><code class="language-toml">[target.riscv64gc-unknown-none-elf]
runner = &quot;qemu-system-riscv64 -machine virt -cpu rv64 -d guest_errors,unimp -smp 4 -m 128M -drive if=none,format=raw,file=hdd.dsk,id=foo -device virtio-blk-device,scsi=off,drive=foo -serial mon:stdio -bios none -device virtio-rng-device -device virtio-gpu-device -device virtio-net-device -device virtio-tablet-device -device virtio-keyboard-device  -kernel &quot;
</code></pre>
<p>If you want to add more RAM, go ahead... so wasteful... look at you.</p>
<h3 id="how-will-we-manage-the-ram-"><a class="header" href="#how-will-we-manage-the-ram-">How will we manage the RAM ?</a></h3>
<p>To manage the RAM we need to do the following tasks well. Really well:</p>
<ol>
<li>Safely Abstracting the physical memory, so that we can manipulate it.</li>
<li>Define methods of correctly allocating memory.</li>
<li>Define methods of correctly deallocating memory.</li>
<li>Defining mechanisms that make sure that programs only access the correct memory sections.</li>
<li>Provide a clean API for all of the above functions</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="abstracting-the-ram"><a class="header" href="#abstracting-the-ram">Abstracting the RAM</a></h1>
<p>For you to abstract any object in the universe, you need to first understand the behaviour and characteristics of that object... or at least know some facts about it.<br />
For you to abstract RAM, you need to know the physical layout of the RAM.<br />
As described ib the Qemu Virt Docs, the RAM starts at address 0x8000_000 till infinity. You set the end of the RAM by specifying the RAM length in the Qemu configurations.<br />
By the way, Here is the part of the documentation that shows the memory layout of the Machine : [undone] : qemu/hw/riscv/virt.c</p>
<p>Qemu loads the kernel binary file in the RAM. And that binary occupies the entire RAM. This is because in the linker script used to make the kernel binary, the kernel heap extends till the end of the RAM.<br />
The RAM layout is as follows, the order and size of different sections have been specified in the linker script :</p>
<div class="table-wrapper"><table><thead><tr><th>Section_Name</th><th>Start_address</th><th>Size</th><th>End_Address</th></tr></thead><tbody>
<tr><td>kernel text</td><td>_text_start (0x80000000)</td><td>(_text_end - _text_start)</td><td>_text_end</td></tr>
<tr><td>kernel rodata</td><td>_rodata_start</td><td>(_rodata_end - _rodata_start)</td><td>_rodata_end</td></tr>
<tr><td>kernel data</td><td>_data_start</td><td>(_data_end - _data_start)</td><td>_data_end</td></tr>
<tr><td>kernel bss</td><td>_bss_start</td><td>(_bss_end - _bss_start)</td><td>_bss_end</td></tr>
<tr><td>kernel Stack</td><td>_stack_start</td><td>524 KB</td><td>_stack_end</td></tr>
<tr><td>kernel Heap</td><td>_heap_start</td><td>_heap_size(_memory_end - _heap_start)</td><td>_heap_end = _memory_end</td></tr>
</tbody></table>
</div>
<p>From the layout, we can say that the memory is divided into 2 distinctions : </p>
<ol>
<li>The part occupied by the kernel code and data (small part)</li>
<li>The kernel heap (big part)</li>
</ol>
<h5 id="the-kernel-small-part"><a class="header" href="#the-kernel-small-part">The Kernel small Part</a></h5>
<p>This part will be abstracted by : </p>
<ul>
<li>extracting all start and end addresses from the linker script and making them available in our Rust modules as accessible variables. THis would make it easy to reference them.</li>
<li>Using a struct to represent this kernel address info</li>
</ul>
<h5 id="the-heap"><a class="header" href="#the-heap">The Heap</a></h5>
<p>The Heap needs to be represented in 4096_byte pages. Each page will have a corresponding descriptor.<br />
This means that there will be an array of descriptors. The array will contain (heap_size/ page_size) items. </p>
<p>Each descriptor will be a byte long. An Enum Value. The Value can only be :</p>
<ol>
<li>Empty</li>
<li>Not Empty and is the First_allocated_page for a contiguous allocation.</li>
<li>Not Empty and Not the First Bit</li>
<li>Empty and The_Last Page for a contiguous allocation</li>
</ol>
<p>As usual we provide all required getters and setters</p>
<h4 id="why-are-we-using-descriptors-to-keep-track-of-the-status-of-a-page"><a class="header" href="#why-are-we-using-descriptors-to-keep-track-of-the-status-of-a-page">Why are we using descriptors to keep track of the status of a page?</a></h4>
<p>So in our page allocation system we need to achieve the following :</p>
<ol>
<li>Keep track of all free pages</li>
<li>Keep track of all allocated pages</li>
<li>Keep track of pages that have been used together</li>
<li>Provide a function that receives a request of allocating certain number of contiguous pages... and it returns the address of the first page of the available contiguous pages</li>
<li>Provide a function that receives an address of occupied pages and completely frees them.</li>
</ol>
<p>We could implement our tracking system using one of the following methods :</p>
<ol>
<li>
<p>Treating each page as a linked list node.
<img src="./images/raw/linked_list_representation_of_heap.jpg" alt="linked_list_representation_of_heap" />
At the top of the Node, we have a pointer to the next node. So we create 2 lists... used nodes and unused nodes</p>
<ul>
<li>This method helps because there is no need for contiguous allocation of pages. This consequently means that we will not have <a href="./fragmentation_issues.html">fragmentation issues</a></li>
<li>However this method has a couple of disadvantages :
<ul>
<li>It takes more space. This is because you have to store the address of the next node + information as to whether it is the last block or not.</li>
<li>The fact that memory allocated is not contiguous means referencing memory gets complicated. You cannot simply use straight foward offsets. You have to use virtual offsets that adds upon performance inefficiency.</li>
<li>The pages are dirty. The top part of the page has the address of the next node. If a process wants to read or write to the page, it has to consider that some information contained in the page is useless to it. And when a couple of pages are read, the process has to filter out the next_address and start concatenating necessary data. This is unnecessary work</li>
<li>There is no direct access within the linked list... you have to traverse it from the beginning each time. Thi is a very huge DEFFIFIENCY because we are dealing with the RAM. Ram data access is expected to be fast. If we has used an array or some tree structure, the CPU would have had faster access speeds.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>We could use a 2-bit bitmap : THis is the fastest and most memory efficient method
<img src="./images/raw/bitmap_representation_of_heap_memory.jpg" alt="bitmap_representation_of_heap_memory" /></p>
</li>
<li>
<p>We could use a bunch of descriptors. This would be the same as that of a bitmap execution. It is just that each descriptor takes 1 byte to describe a page while in the bitmap uses only 2 bits to describe a page (empty, taken, first, last)
<img src="./images/raw/descriptor_representation_of_heap_memory.jpg" alt="descriptor_representation_of_heap_memory" /></p>
</li>
</ol>
<p>More discissions about memory tracking are found <a href="./memory_tracking_mechanisms.html">here</a></p>
<p>We opt to use descriptors because :</p>
<ul>
<li>Even though Bitmapping is more memory efficient than using descriptors...Using descriptors is simpler to implement. Using Bitmapping would cut our memory usage by 75%. But for now...this is a learning project.... simplicity is our first priority. </li>
<li>Using a Linked List has only one advantage ; it solves the defragmentation issue. The defragmentation issue is that : &quot;Other methods allocate memory contiguously, meaning that there are bound to be 'holes'. Holes are free meory sapaces that are not contiguous. So one is forced to rearrange the whole memory in order to combine the holes. This defragmentation process causes a performance letdown + it causes non_determinism in terms of time, some embedded applications are time-crucial&quot;.<br />
Other than solving defragmentation problem, it is quite memory inefficient, performance inefficient and hard to implement. If we implement a virtual paging mechanism on top of descriptors... fragmentation problems in the descriptor method becomes solved.</li>
</ul>
<p>We need to find a method that :</p>
<ol>
<li>Allocates physical memory contiguously (for the most part, )</li>
<li>The method needs to have a solution to defragmentation... without actually defragmenting anything.</li>
<li>Is fast</li>
<li>memory efficient</li>
<li>Predictable (we are in the embedded space for cryig out loud... people... time matters)</li>
</ol>
<p>Combining virtual paging and using descriptors fulfills all the above requirements </p>
<p>[undone] How about the deterministic_heap method you were proposing?</p>
<h4 id="so-what-is-this-descriptor-method-of-tracking"><a class="header" href="#so-what-is-this-descriptor-method-of-tracking">So what is this descriptor method of tracking?</a></h4>
<p>A descriptor is an enum that Describes the status of a page. The value of the enum can be one of the following values :
- page is empty
- page has been taken and it's the first node for a certain contiguous allocation
- page has been taken and it's neither the first nor the last node for a certain contiguous allocation
- page has been taken and it's the last node for a certain contiguous allocation</p>
<ol>
<li>We divide the heap into pages that are aligned to 4KiB.</li>
<li>We segment the heap into : 
<ul>
<li>Pages used to store descriptors  - segment 1</li>
<li>Pages used to store data         - segment 2</li>
</ul>
</li>
<li>Under the segment 1, we store an array of descriptors. The number of descriptors is equal to the number of pages that can fit in the segment 2 of the heap.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="allocating--and-deallocating-ram-memory"><a class="header" href="#allocating--and-deallocating-ram-memory">Allocating  and Deallocating RAM Memory</a></h1>
<p>Allocating means giving out free memory to processes or whatever.<br />
We need to decide on the smallest unit to allocate at a time.</p>
<p>We have 3 choices ; we can allocate page-wise or byte-wise or both. 
For this project, I chose Page-wise allocation, Byte-wise allocations will be implemented in a future day... probably never..ha ha .</p>
<p>A page in our system is 4096 bytes long. ie 4 KiB</p>
<h4 id="the-allocation-method"><a class="header" href="#the-allocation-method">The allocation method</a></h4>
<p><strong>Algorithm name :</strong> alloc 
<strong>Inputs to alloc algorithm :</strong> the number of free pages required (required_pages)<br />
<strong>Outputs of alloc algorithm :</strong> the address of the first page of a contiguous block of free pages (starter)
main goal : return an address to the first page of a free contiguous set of pages  : A RESULT VALUE (pointer/ error)</p>
<p>Steps:</p>
<ol>
<li>Confirm that the number of required pages is more than zero.
<ol>
<li>If number is zero or less
<ol>
<li>throw an <a href="./errors.html">Error M1</a></li>
<li>return the error to the calling function.</li>
</ol>
</li>
<li>If the number is more than zero... continue to step 2</li>
</ol>
</li>
<li>Traverse the array of descriptors found in the heap</li>
<li>Try to Find a block of contiguous free pages
<ol>
<li>If you find a block... skip to step 4</li>
<li>If you traverse the whole array and you do not find space ... skip to step 5</li>
</ol>
</li>
<li>Do the folowing :
<ul>
<li>update the descriptors that represent the block</li>
<li>return the pointer to the first page of the block</li>
</ul>
</li>
<li>Do the following
<ul>
<li>return an <a href="./errors.html">error_M2</a> indicating that there is no free contiguous space.</li>
</ul>
</li>
</ol>
<h4 id="the-de-allocation-method"><a class="header" href="#the-de-allocation-method">The de-allocation method</a></h4>
<p>algorithm : dealloc
inputs to dealloc algorithm : the address of the first page of a contiguous block of pages that needs to be freed (starter)
Outputs to dealloc algorithm : The Result Type (Ok/Error)
main goal : deallocate </p>
<p>Steps:</p>
<ol>
<li>Check if the starter address is valid or not.
<ul>
<li>If the starter address is a null pointer... go to step 2</li>
<li>If the starter address is an out of range address... go to step 3</li>
<li>If the starter address is a valid address...go to step 4</li>
</ul>
</li>
<li>Return a Result_Error showing that the process tried to deallocate a null pointer : Error_M3</li>
<li>Return a Result_Error showing that the process tried to deallocate a non-existent memory location : Error_M4</li>
<li>Loop through the allocated block page by page :
<ul>
<li>For every page...
<ul>
<li>clear the data by zero-ing the bytes within the page</li>
<li>Change the status of the corresponding descriptor to 'empty'</li>
</ul>
</li>
</ul>
</li>
<li>After the loop, return a successful message Result (ok) type</li>
</ol>
<h4 id="api"><a class="header" href="#api">API</a></h4>
<ul>
<li>dealloc function</li>
<li>alloc function</li>
</ul>
<h4 id="testing-this-module"><a class="header" href="#testing-this-module">Testing this module</a></h4>
<p>This module does 3 tasks. So we need to test all the 3 tasks.</p>
<ol>
<li>Task 1 was : abstracting the heap into descriptors and pages.</li>
<li>Task 2 was : writing a function that returns the address of the first page associated with a free block of contiguous pages</li>
<li>Task 3 was : writing a function that frees a contiguous block of contiguous pages</li>
</ol>
<h5 id="test-1--testing-task-1-"><a class="header" href="#test-1--testing-task-1-">Test 1 : Testing task 1 :</a></h5>
<p>In this test, we compare preconfigured data that we calculated in theory and hope that our alocation function produces similar data.<br />
Confirm if the following values are similar:</p>
<ul>
<li>
<p>The heap_start address</p>
</li>
<li>
<p>The heap_end address</p>
</li>
<li>
<p>The number of data pages</p>
</li>
<li>
<p>The number of descriptors</p>
</li>
<li>
<p>number of descriotors == number of data pages</p>
</li>
<li>
<p>Confirm that all the addresses of each page are a divisible of 4096</p>
</li>
<li>
<p>Confirm that all descriptors are initially set to 'empty'</p>
</li>
</ul>
<h5 id="test-2--testing-task-2-the-allocating-function"><a class="header" href="#test-2--testing-task-2-the-allocating-function">Test 2 : Testing Task 2, the allocating function</a></h5>
<ul>
<li>confirm that submitting a zero to the function returns the appropriate Error </li>
<li>Confirm that submitting a value more than the number of pages found in a &lt;128 MB heap will give the appropriate error.</li>
<li>confirm that certain descriptor change after allocation : we should have a &quot;first&quot; and &quot;end&quot; and possibly a middle. But not 2 consecutive &quot;firsts&quot; or &quot;ends&quot;.</li>
</ul>
<h5 id="test-3--testing-task-3-the-deallocation-function"><a class="header" href="#test-3--testing-task-3-the-deallocation-function">Test 3 : Testing Task 3, the deallocation function</a></h5>
<ul>
<li>confirm that submitting a null pointer yields the appropriate error</li>
<li>confirm that submitting a pointer that is not within the heap range yields the appropriate error</li>
<li>Confirm that all the data pages are indeed zeroed after deallocation and that they contain no garbage data or residue data.</li>
<li>Confirm that all descritors involved in the deallocation process are updated to 'empty'</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li>AllocationList - </li>
<li>The descriptor in this case is the AllocList Structure. It has 64 bits. It describes the state and size of a block. A block is a contiguous set of bytes.</li>
<li>The first bit of the AllocList structure states whether the block is taken or free. (1 == taken)</li>
<li>The rest of the 63 bits represent the size </li>
<li>the smallest unit that can be allocated is 8 bytes because we set the alignment order at 3. When you request 3 bytes, you will get 8 bytes in return.</li>
<li>The allocStrust</li>
</ul>
<p>Look at the kmem.rs code in ch5 </p>
<div style="break-before: page; page-break-before: always;"></div><p>In the previous section we did the following :</p>
<ol>
<li>We abstracted the heap memory as pages that are associated to descriptors.</li>
<li>We provided the allocation and deallocation functions</li>
</ol>
<p>In the previous module we were dealing with Physical addresses. We were executing code while the CPU was in Machine mode. This means that the memory management unit was turned off. Our code was referencing live physical addresses. eg memory_start = 0x8000000</p>
<p>In this module we want to abstract the physical RAM. We will virtualize all the memory addresses of the RAM. In real life the RISCV board provides a hardware implementation of a Memory management unit. So this unit provides a way to create virtual addresses and a way to declare access rights to those addresses. The MMU can operate in Bare Mode, SV39 mode or SV48 mode.</p>
<p>Here is a theory discussion of the <a href="./theory_on_MMU_implementation_in_riscv.html">theory on MMU implementation of Riscv</a>.</p>
<p>As discussed in the theory :</p>
<ol>
<li>For us to use the MMU hardware, we need to activate it, choose a mode and finally switch our cpu from machine mode to either Supervisor mode or Usermode.</li>
<li>We need to map all the linker_initialized memory locations.</li>
<li>When mapping the heap, we need to set aside kernel heap and user_program heap.  Isolating the two is good for security... and modularity. It means the kernel will always have a dedicated heap and that it will not compete for space with the rest of the user programs.</li>
<li>We need to implement an access control mechanism.</li>
<li>Each process should get a dedicated virtual address space</li>
</ol>
<p>We will satisfy the above needs as follows :</p>
<h4 id="develop-a-virtual-mmu-instead-of-using-the-physical-mmu"><a class="header" href="#develop-a-virtual-mmu-instead-of-using-the-physical-mmu">Develop a Virtual MMU instead of using the physical MMU</a></h4>
<p>We will not use the MMU hardware. Meaning that our kernel will continue executing in machine mode. Our user programs will also execute in machine mode. Instead of using the MMU hardware, we will implement a virtual MMU that works in SV39 mode.
Using the MMU hardware could have given us many advantages :</p>
<ol>
<li>
<p>Performance: Physical MMUs provide faster and more efficient memory access than virtual MMUs. This is because physical MMUs are implemented in hardware, which makes them faster than software-based virtual MMUs. The physical MMU has caches that make memory translation process much faster.</p>
</li>
<li>
<p>Security: Physical MMUs provide better security than virtual MMUs. Physical MMUs can be used to implement hardware-based memory protection, which prevents unauthorized access to memory. This is not possible with virtual MMUs because they rely on software to implement memory protection.</p>
</li>
<li>
<p>Reliability: Physical MMUs are more reliable than virtual MMUs. Since physical MMUs are implemented in hardware, they are less prone to software bugs and errors, which can cause system crashes or data corruption.</p>
</li>
<li>
<p>Scalability: Physical MMUs are more scalable than virtual MMUs. As the size of physical memory increases, physical MMUs can be easily expanded to accommodate the increased memory, whereas virtual MMUs may require significant changes to the operating system and software.</p>
</li>
</ol>
<p>With all this advantages, it is obvious that using the hardware MMU is the right choice. So why use a virtual MMU?<br />
Learning and understanding how the hardware MMU works in detail takes time. It will be faster to just understand how the MMU works from a high level and implement it in software form. Most of the details involved around understanding the hardware MMU are centered around space optimization. As a result, there is much bitmasking and predefined procedures to follow.</p>
<p>For now, for the sake of implementation time, using the virtual MMU is the way to go.<br />
We will borrow SV39 mode concepts.</p>
<h4 id="major-tasks"><a class="header" href="#major-tasks">Major Tasks</a></h4>
<ol>
<li>Abstract the SATP register; This will help us identify : The process address space</li>
<li>Abstract the root table, parent table and child tables</li>
<li>Abstract the entries
<ul>
<li>branch and leaf entries</li>
</ul>
</li>
<li>Define functions to Map the Linker defined memory locations</li>
<li>Define functions to Map kernel heap addresses.</li>
<li>Define functions to Map user heap addresses</li>
<li>Define function to translate Linker defined memory locations</li>
<li>Define function to translate kernel heap addresses</li>
<li>Define function to translate user heap addresses</li>
<li>Define the API of the MMU
<ul>
<li>The exposed functions</li>
<li>The exposed structs</li>
<li>The success responses and error messages {this means you have to program }</li>
</ul>
</li>
</ol>
<h3 id="quick-detour"><a class="header" href="#quick-detour">Quick Detour....</a></h3>
<p>We will not implement the sv39 emulation due to time constraints. Such a disappointment. All that reading for nothing.  We will leave that for future Implementations.<br />
We will implement a one to one mapping. Something like this : </p>
<p><strong>The Translation Table.</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Process_with_access_rights</th><th>Access Parameter</th><th>Virtual memory</th><th>Physical Memory</th></tr></thead><tbody>
<tr><td>0(kernel)</td><td>R/E</td><td>0x80000000(text_section)</td><td>0x80000000</td></tr>
<tr><td>0(kernel)</td><td>R/W/E</td><td>0X80002000(stack_end)</td><td>0X80002000</td></tr>
<tr><td>0(kernel)</td><td>R/W/E</td><td>0x80005000 (Kernel_heap_start)</td><td>0x80005000</td></tr>
<tr><td>0(kernel)</td><td></td><td></td><td></td></tr>
<tr><td>1(init)</td><td></td><td></td><td></td></tr>
<tr><td>1(init)</td><td></td><td></td><td></td></tr>
</tbody></table>
</div>
<p>Our system is not fine_grained, meaning that to access a specific page, you have to first find the first allocated page for that associated with the contiguous block.<br />
Do not Let the transalation Table fool you, the Kernel process has first class access rights to all other processes. THis is dangerous, it will be fixed in the future [undone] </p>
<p>Kernel virtual addresses are mapped using a different mechanism from the one used to map user processes.<br />
Mapping is the process of populating an atomic entry in the the Translation table.</p>
<h4 id="the-mapping-procedure-for-processes-that-are-not-kernel-related"><a class="header" href="#the-mapping-procedure-for-processes-that-are-not-kernel-related">The Mapping procedure (For processes that are not Kernel related):</a></h4>
<ul>
<li>The OS decides to create space for a user process.</li>
<li>The OS realizes that it has to create space for each of the following process elements: 
<ul>
<li>The different LOAD-ABLE sections of the user process elf_file</li>
<li>The stack</li>
<li>The process structure </li>
</ul>
</li>
<li>It calls the allocator to help allocate pages for each of the above sections. The Elf sections should fall under a contiguous memory space. The allocator returns the physical start addresses for each element.</li>
<li>Now the kernel has the physical memories it can use to populate part of the Translation Table. The Translation table now looks as follows :</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Process_with_access_rights</th><th>Access Parameter</th><th>Virtual memory</th><th>Physical Memory</th></tr></thead><tbody>
<tr><td></td><td></td><td></td><td>0xsome_address (text_section)</td></tr>
<tr><td></td><td></td><td></td><td>0xsome_address (data_section)</td></tr>
<tr><td></td><td></td><td></td><td>0xsome_address (bss_section)</td></tr>
<tr><td></td><td></td><td></td><td>0xsome_address (other_loadable_sections)</td></tr>
<tr><td></td><td></td><td></td><td>0xsome_address (Stack_start)</td></tr>
<tr><td></td><td></td><td></td><td>0xsome_address (Process Structure)</td></tr>
</tbody></table>
</div>
<ul>
<li>The kernel also has information about which processes have access to the allocated sections. ie 
<ul>
<li>The elf sections belong to the user process. Only the kernel and the user process have access to these sections</li>
<li>The stack belongs to the user process. Only the kernel and the user process have access to these sections.</li>
<li>The Process Struture belongs to the Kernel.</li>
</ul>
</li>
<li>So our Mapper function updates the table as follows :</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Process_with_access_rights</th><th>Access Parameter</th><th>Virtual memory</th><th>Physical Memory</th></tr></thead><tbody>
<tr><td>1(hello_world)</td><td></td><td></td><td>0xsome_address (text_section)</td></tr>
<tr><td>1(hello_world)</td><td></td><td></td><td>0xsome_address (data_section)</td></tr>
<tr><td>1(hello_world)</td><td></td><td></td><td>0xsome_address (bss_section)</td></tr>
<tr><td>1(hello_world)</td><td></td><td></td><td>0xsome_address (other_loadable_sections)</td></tr>
<tr><td>1(hello_world)</td><td></td><td></td><td>0xsome_address (Stack_start)</td></tr>
<tr><td>0(kernel)</td><td></td><td></td><td>0xsome_address (Process Structure)</td></tr>
</tbody></table>
</div>
<ul>
<li>
<p>Now using an Elf reader, you extract info about the R/W/exeute access of each section of the elf file and update the relevant info in the Translation Table : 
| Process_with_access_rights | Access Parameter | Virtual memory | Physical Memory                          |
|----------------------------|------------------|----------------|------------------------------------------|
| 1(hello_world)             | R/E              |                | 0xsome_address (text_section)            |
| 1(hello_world)             | R/W              |                | 0xsome_address (data_section)            |
| 1(hello_world)             | R/W              |                | 0xsome_address (bss_section)             |
| 1(hello_world)             | applicable_right |                | 0xsome_address (other_loadable_sections) |
| 1(hello_world)             |                  |                | 0xsome_address (Stack_start)             |
| 0(kernel)                  |                  |                | 0xsome_address (Process Structure)       |</p>
</li>
<li>
<p>The stack is a read_write... is it? I really don't know.  The Process structure is also a read_write : </p>
</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Process_with_access_rights</th><th>Access Parameter</th><th>Virtual memory</th><th>Physical Memory</th></tr></thead><tbody>
<tr><td>1(hello_world)</td><td>R/E</td><td></td><td>0xsome_address (text_section)</td></tr>
<tr><td>1(hello_world)</td><td>R/W</td><td></td><td>0xsome_address (data_section)</td></tr>
<tr><td>1(hello_world)</td><td>R/W</td><td></td><td>0xsome_address (bss_section)</td></tr>
<tr><td>1(hello_world)</td><td>applicable_right</td><td></td><td>0xsome_address (other_loadable_sections)</td></tr>
<tr><td>1(hello_world)</td><td>R/W</td><td></td><td>0xsome_address (Stack_start)</td></tr>
<tr><td>0(kernel)</td><td>R/W</td><td></td><td>0xsome_address (Process Structure)</td></tr>
</tbody></table>
</div>
<ul>
<li>
<p>now we are left with the virtual memory coulumn. Each Virtual memory address belonging to a particular process needs to be unique when compared to other virtual addresses assigned to the same process. However, virtual memory addresses belonging to different processes can be similar because they are under different processes.</p>
</li>
<li>
<p>The Virtual addresses for the different elf sections can be extracted from the elf file using the elf_reader. So the Translation Table becomes :<br />
| Process_with_access_rights | Access Parameter | Virtual memory              | Physical Memory                          |
|----------------------------|------------------|-----------------------------|------------------------------------------|
| 1(hello_world)             | R/E              | elf text_section            | 0xsome_address (text_section)            |
| 1(hello_world)             | R/W              | elf data_section            | 0xsome_address (data_section)            |
| 1(hello_world)             | R/W              | elf bss_section             | 0xsome_address (bss_section)             |
| 1(hello_world)             | applicable_right | elf other_loadable_sections | 0xsome_address (other_loadable_sections) |
| 1(hello_world)             | R/W              |                             | 0xsome_address (Stack_start)             |
| 0(kernel)                  | R/W              |                             | 0xsome_address (Process Structure)       |</p>
</li>
<li>
<p>If the elf file specifies the virtual address of the stack... use it.</p>
</li>
<li>
<p>If the elf file does not specify the virtual address for the Stack, you assign it a virtual address that does not coincide with any of the elf virtual addresses. To make sure of this, you pick a virtual address that comes right after the end of the virtual addresses specified in the elf file. The chosen address needs to be aligned to a multiple of 4096.</p>
</li>
</ul>
<p>So the Translation Table now is updated to this :<br />
| Process_with_access_rights | Access Parameter | Virtual memory                  | Physical Memory                          |
|----------------------------|------------------|---------------------------------|------------------------------------------|
| 1(hello_world)             | R/E              | elf text_section                | 0xsome_address (text_section)            |
| 1(hello_world)             | R/W              | elf data_section                | 0xsome_address (data_section)            |
| 1(hello_world)             | R/W              | elf bss_section                 | 0xsome_address (bss_section)             |
| 1(hello_world)             | applicable_right | elf other_loadable_sections     | 0xsome_address (other_loadable_sections) |
| 1(hello_world)             | R/W              | (elf stack) OR (after_elf addr) | 0xsome_address (Stack_start)             |
| 0(kernel)                  | R/W              | 0xsome_address_X                | 0xsome_address_X (Process Structure)     |</p>
<ul>
<li>Now what remains is the Kernel virtual address. Drum rolls.... Boom! : The Kernel does not use virtual addresses that are different from the physical addresses. Kernel virtual addresses are straight foward : a one-to-one mapping. This decision is NOT a bad decision. This is because you can treat the Kernel virtual addresses as normal virtual addresses. for example, multiple virtual addresses can point to a single physical address.</li>
</ul>
<h4 id="the-translation-process"><a class="header" href="#the-translation-process">The Translation Process</a></h4>
<p>When a process wants to access the RAM, it uses virtual addresses.  The instructions inside the elf binary reference virtual memory addresses. The Data inside the elf binary file also reference virtual addresses. So when the CPU executes a user_program instruction, it needs to first translate </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling_the_physical_mmu"><a class="header" href="#handling_the_physical_mmu">Handling_the_Physical_MMU</a></h1>
<p>Developing the Virtual MMU turneed out harder than expected. We had to figure out how to :</p>
<ul>
<li>Create a custom Page translation table</li>
<li>Make the CPU consult the MMU every time the CPU executes an instruction</li>
<li>Design our own page fault mechanisms.</li>
</ul>
<p>It is better to just use already made hardware. We get a performance boost. We get to use a standard method of handling transltion, a method that is tried and tested. Designing a custom MMU was hard... we failed but.. Well, we learn a lot... moving on.</p>
<p>We need to cover the following tasks :</p>
<ol>
<li>Page allocations</li>
<li>Page deallocations</li>
<li>Program the Memory Management unit
<ol>
<li>Handle Mapping Virtual Adresses to Physical Addresses.</li>
<li>Handling Page Faults</li>
<li>Handling page Translations during CPU instruction executions.</li>
</ol>
</li>
</ol>
<h2 id="page-allocation-and-deallocation"><a class="header" href="#page-allocation-and-deallocation">Page allocation and deallocation</a></h2>
<p>As discused earlier, we will be allocating pages from the heap of the RAM. Each page is 4096 bytes long. And each page has a descriptor that describes the page status. Each descriptor is 1 byte long. The number of descriptors is equal to the number of pages found in the head. The descriptors are stored in an array found in the kernel code. The array of descriptors is not part on the heap. </p>
<p>The Heap is virtually divided into 2, The kernel Heap and the User Heap</p>
<h3 id="allocation-algorithm"><a class="header" href="#allocation-algorithm">Allocation Algorithm</a></h3>
<p>algorithm : alloc
inputs to alloc : the number of free pages required a user process or the kernel(required_pages)
Outputs to alloc : the address of the first page of a contiguous block of free pages (starter_address)
main goal : return an address to the first page of a free contiguous set of pages  : A RESULT VALUE (pointer(usize)/ error)</p>
<p>Steps:</p>
<ol>
<li>Confirm that the number of required pages is more than zero.
<ol>
<li>If number is zero or less
<ol>
<li>Throw an <a href="./errors#error_m1.html">Error M1</a> to show that the allocator was given a zero.</li>
<li>Return the error to the calling function.</li>
</ol>
</li>
<li>If the number is more than zero... continue to step 2</li>
</ol>
</li>
<li>Traverse the array of descriptors found in the kernel code.</li>
<li>Try to Find a block of contiguous free pages
<ol>
<li>If you find a block... skip to step 4</li>
<li>If you traverse the whole array and you do not find space ... skip to step 5</li>
</ol>
</li>
<li>Do the folowing :
<ul>
<li>update the descriptors that represent the block</li>
<li>return the pointer to the first page of the block</li>
</ul>
</li>
<li>Do the following
<ul>
<li>return an <a href="./errors#error_m2.html">error_M2</a> indicating that there is no free contiguous space.</li>
</ul>
</li>
</ol>
<h3 id="deallocation-algorithm"><a class="header" href="#deallocation-algorithm">Deallocation Algorithm</a></h3>
<p>algorithm : dealloc
inputs to dealloc algorithm : the address of the first page of a contiguous block of pages that needs to be freed (starter)
Outputs to dealloc algorithm : The Result Type (Ok/Error)
main goal : deallocate </p>
<p>Steps:</p>
<ol>
<li>Check if the starter address is valid or not.
<ul>
<li>If the starter address is a null pointer... go to step 2</li>
<li>If the starter address is an out of range address... go to step 3</li>
<li>If the starter address is a valid address...go to step 4</li>
</ul>
</li>
<li>Return a Result_Error showing that the process tried to deallocate a null pointer : <a href="./errors#error_m3.html">Error_M3</a></li>
<li>Return a Result_Error showing that the process tried to deallocate a non-existent memory location : <a href="./errors#error_m4.html">Error_M4</a></li>
<li>Loop through the allocated block page by page :
<ul>
<li>For every page...
<ul>
<li>clear the data by zero-ing the bytes within the page</li>
<li>Change the status of the corresponding descriptor to 'empty'</li>
</ul>
</li>
</ul>
</li>
<li>After the loop, return a successful message Result (ok) type</li>
</ol>
<h2 id="programming-the-memory-management-unit"><a class="header" href="#programming-the-memory-management-unit">Programming the Memory Management unit</a></h2>
<p>The instructions in the Elf files of programs typically reference virtual addresses. The CPU cannot execute an instruction that has a virtual address. For this reason, everytime the CPU encouters a virtual address in an instruction, so the CPU uses the MMU ciruitry to translate the virtual address into a physical address.</p>
<p>You can program the MMU to enforce access rights to certain physical memory addresses, such that a translation will fail if an access right is being violated. If the translation fails, a page fault is thrown by the MMU and the interrupt handler handles it. In this case, the MMU acts as a memory protector, ensuring translations only happen when all access rights are adhered to.</p>
<p>In Machine Mode, RISCV provides a mechanism of protecting memory called Physical Memory Protection(PMP). But PMP does not scale well.</p>
<p>A virtual Memory Management system is provided In Supervisor mode</p>
<ul>
<li>leaf node</li>
</ul>
<h3 id="sv39-system"><a class="header" href="#sv39-system">SV39 system</a></h3>
<p>Terms : </p>
<ol>
<li>The Virtual Address </li>
<li>The Physical Address</li>
<li>The Page table Entry (PTE)</li>
</ol>
<p>A page table contains 512 PTEs. ie (2^9) entries
Each PTE is 8 bytes long. Which means the table occupies (512 x 8)bytes = 4096 bytes</p>
<p>Rust:</p>
<ul>
<li>map</li>
<li>unmap </li>
<li>Translate</li>
</ul>
<p>Abstract a table entry</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Entry {
	pub entry: i64,
}


<span class="boring">}</span></code></pre></pre>
<p>Abstract a translation table</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Table {
	pub entries: [Entry; 512],
}
<span class="boring">}</span></code></pre></pre>
<h3 id="mapping-function"><a class="header" href="#mapping-function">Mapping function</a></h3>
<p>inputs : 
- mutable Root_table (we will update the root table)
- Virtual address meant to be mapped
- Physical address to be mapped
- protection bots (flags)
- the level of page table (by default 0)</p>
<p>No outputs</p>
<ul>
<li>Make sure the bits are not zero. Zero means that the entry is a branch. we want a leaf.</li>
<li>Extract the VPNs (3) from the virtual address</li>
<li>Extract the PPNs from the Physical address</li>
<li>Access the root table</li>
<li>Use the VPN[2] index to access the entry table in the root (mutably) - call this entry x</li>
<li>Check if the x points to a valid memory</li>
<li>If No
<ul>
<li>allocate a new page, store the page address at addr_1</li>
<li>update the entry x
<ul>
<li>store addr_1 into x</li>
<li>shift x to the right by 2 bits</li>
<li>add the valid bit in the LSB</li>
</ul>
</li>
</ul>
</li>
<li>If yes, store the address of the next table at addr_1
<ul>
<li>Remove the valid bit from the copy value of x</li>
<li>Shift the x by 2 bits to the left</li>
<li>store this value into addr 1</li>
</ul>
</li>
<li>Use the VPN[1] index to access the entry table in the Table_1 call it entry y</li>
<li>Check if the y points to a valid memory</li>
<li>If No
<ul>
<li>allocate a new page, store the page address at addr_2</li>
<li>update the entry y
<ul>
<li>store addr_2 into y</li>
<li>shift y to the right by 2 bits</li>
<li>add the valid bit in the LSB</li>
</ul>
</li>
</ul>
</li>
<li>If Yes
<ul>
<li>Remove the valid bit from the copy value of y</li>
<li>Shift the y by 2 bits to the left</li>
<li>store this value into addr_2</li>
</ul>
</li>
<li>Use the VPN[0] index to access the entry table in the Table_2 whose address is addr_2 - call it entry z</li>
<li>Create an temporary entry called PHYSICAL</li>
<li>Store the PPNs in it</li>
<li>Store the Flags in it, make it valid</li>
<li>store it at Table_2 VPN[0]</li>
</ul>
<p>Use a loop in the future... this was just for clarity</p>
<h4 id="unmapping"><a class="header" href="#unmapping">Unmapping</a></h4>
<p>You can unmap a specific irtual address. Work this algorithm out.<br />
You can also unmap the whole system, like the way stephen did. But this is undesirable.</p>
<h4 id="translating"><a class="header" href="#translating">Translating</a></h4>
<p>This function takes in a virtual address and returns a physical address. If things go wrong, it may throw a page fault or return (None) to notify you that such a translation does not exist.</p>
<p>Inputs : </p>
<ul>
<li>Root_Table as read</li>
<li>Virtual_address as read_only</li>
</ul>
<p>Outputs : Option<address></p>
<ul>
<li>extract each part of the VPN from the virtual address as VP1, VP2 and VP3</li>
<li>access the root_table</li>
<li>access the target entry in root using index VP2</li>
<li>Enter a loop that traverses PTEs starting from the root table
<ul>
<li>read the target PTE</li>
<li>check if it is valid, else throw a page fault</li>
<li>Check if the entry is a branch or a leaf</li>
<li>If the entry is a branch, continue traversing</li>
<li>If the entry is a leaf
<ul>
<li>extract the offset from the virtual address</li>
<li>extract the physical address by 
<ul>
<li>shifting entry to the left by 2 bits</li>
<li>Clearing out the last 2 bits</li>
<li>ANDING the offset to the page physical address </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>If you reach this point, it means you have not found a translation, return (None)</li>
</ul>
<p>Now the kernel is currently using the physical addresses. TO turn the MMu on, you set the SATP MODE field to 8. 
But if we turn the MMU ON , Both the kernel and user processes will be forced to use virtual addresses ONLY. This means that we have to MAP all existing memory addresses befor we turn the MMU on. 
So we will map the kernel elf sections, kernel stack, required MMIO addresses.<br />
As a result, the kernel code meant to run in machine mode will also run in Supervisor mode</p>
<p>To make things easy, we identity map the virtual addresses and Virtual addresses. This means that for non_user addresses, there will be no time wasted on translation</p>
<h4 id="identity-mapping-a-range-of-functions"><a class="header" href="#identity-mapping-a-range-of-functions">identity mapping a range of functions</a></h4>
<p>The goal is to map a couple of pages at a time. Pages that have not been mapped will not be accessible. So if there is any page that has elements inside, it needs to get mapped. For example kernel_data_section is found in a certain page, we don't care if it is in the middle or at the start of a page... bottomline, that page needs to be mapped.<br />
All page start addresses are aligned to 4096. That is why we defined the allign function. However the align function ony returns a multiple value that is greater or equal to the input value.</p>
<p>Algorithm : map_pages_in_a_range
inputs :
- the mutable root table
- The start known address
- The End known address
- The security flags</p>
<p>No outputs</p>
<ul>
<li>Find the true start_address of the page that contains start_known_address  : Call it true_start</li>
<li>Find the true end_address : this is the address of the next page that has not been touched by the End known address
<ul>
<li>You can get this address by using the 'align function' on the End known address.</li>
</ul>
</li>
<li>Calculate the number of pages that are found within true start and true end, this number of pages will help us in the looping process of mapping mapping.</li>
<li>Loop for each page that needs to be mapped
<ul>
<li>For each page : page::map(root, memaddr, memaddr, bits, 0);</li>
<li>increment the memaddr by 4096</li>
</ul>
</li>
</ul>
<p>Sometimes mapping may overlap, but that will cause no harm, because our mapping function only maps pages that have not been mapped. If it meets a mapped page, it moves along[undone]</p>
<p>Then we map all the setions and UART mem addresses : ORDER</p>
<ol>
<li>Export global data from Linker and bring them to Rust</li>
<li>Initialize Page allocation system</li>
<li>Allocate the Root table in the Heap. We could have defined a predefined array table in our code. But if we did that , the memory address of the array would be hard to pinpoint. For this reason, we just use the heap, where we can know the exact page and address that the root table occupies. (4096 bytes, 512 entries)</li>
<li>Update the SATP register ... (without turning it on). Here is the SATP Layout :
<ul>
<li>insert the 44 bit PPN in a abstract SATP (usize)</li>
<li>Insert the mode bit (setting the mode bit does not automatically turn the MMU on out of the blue, this mode bit will work only id we are in supervisor mode)</li>
<li>To actually switch from Machine mode to Supervisor mode, you have to modify the SPIE and MPP fields in the mstatus register. Afterwards, you call mret to restore the MPP, SPIE and update the PC register to point at the MEPC value.w</li>
<li>Use inline web assembly to modify the real SATP : asm!(&quot;csrw satp, $0&quot; :: &quot;r&quot;(satp_val));
<img src="images/RISCV/satp_register.png" alt="SATP register layout" /></li>
</ul>
</li>
</ol>
<ul>
<li>you can store your translation tables anywhere, I chose the heap</li>
<li>The SPIE bit in the sstatus or msstatus indicates whether supervisor interrupts were enabled prior to trapping into supervisor
mode.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="actual_implementation"><a class="header" href="#actual_implementation">Actual_implementation</a></h1>
<ul>
<li>get a grasp of core::mem and core::ptr [undone]</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>An <strong>event trigger</strong> in this case means the action of an instruction causing the CPU to stop executing the current process and start executing the interrupt handler.</p>
<p>An <strong>Interrupt</strong> is an <em>event trigger</em> caused by an external device. This trigger is asynchronous (random).<br />
A <strong>Trap</strong> is a <em>deliberate</em> event trigger that is found in the program getting executed by the CPU. For example, a deliberate system call to access the file system.<br />
An <strong>exception</strong> is a <em>random</em> event trigger caused by the program that was being executed by the kernel. For example, a division by zero.</p>
<p>Some event triggers are <strong>synchronous</strong>, meaning that the currently executing instruction is the one that caused the event trigger... whether is was done deliberately or randomly we don't care. </p>
<p>Other event triggers are <strong>asynchronous</strong>, meaning that the event trigger was not caused by the currently executing instruction. It was caused by something other tan the currently executing instruction</p>
<p>Here are the asynchronous triggers :
<img src="./images/mcause_asynchronous_interrupts.png" alt="" /></p>
<p>Here are the synchronous triggers :
<img src="./images/mcause_synchronous_interrupts.png" alt="" /></p>
<h2 id="risc-v-interrupt-system"><a class="header" href="#risc-v-interrupt-system">RISC-V Interrupt System</a></h2>
<p>The riscv system allows you to handle traps in all the CPU modes : usermode, supervisor mode and machine mode.<br />
But our system will handle all OS interrupts and traps in machine mode..</p>
<p>When an interrupt happens, the cpu : </p>
<ol>
<li>updates the mcause register</li>
<li>updates the mepc register</li>
<li>updates mtval register </li>
<li>saves the context of the current program . We save this context in a trap frame. We store the trap frame in a mscratch register.</li>
<li>calls the interrupt handling function.<br />
The address of the interrupt handling function is stored in the mtvec register (Machine Trap Vector). A vector is a fancy word for saying &quot;pointer to a function&quot;</li>
</ol>
<p>We will use Direct mode of handling interrupts, we wil not use the vectored approach.<br />
Below is the the structure of the mtvec register :<br />
<img src="./images/mtvec.png" alt="mtvec register layout" /></p>
<p>THe mcause register store information about :</p>
<ol>
<li>The type of interrupt (whether it is synchronous or asynchronous)  0 == Synchronous, 1 == Asynchronous</li>
<li>The code specifying the cause of interrupt eg code 12 == instruction page fault</li>
</ol>
<p>we will handle rhe flow of interrupt handling using the following files :</p>
<ol>
<li>boot.s</li>
<li>trap.s</li>
<li>trap.rs</li>
</ol>
<h3 id="boots"><a class="header" href="#boots">boot.s</a></h3>
<p>boot.s contains the assembly code that </p>
<ol>
<li>Initializes CPU registers and makes the mecp register point to the entry point of the kernel code ie kmain. so that if we call mret... the kernel code starts executing.</li>
<li>It makes the mtvec point to the asm_trap_vector function. The asm_trap_vector has been defined in the trap.s file</li>
</ol>
<h3 id="traps"><a class="header" href="#traps">trap.s</a></h3>
<p>This file defines the assembly code that does the folowing :</p>
<ol>
<li>Saving the context of the interrupted process.</li>
<li>Set up the suitable context for the m_trap function.</li>
<li>Call the entry point of the Rust code that defines how each interrupt is handled.</li>
<li>Restore the context of the interrupted process.</li>
<li>Return control to the interrupted process.</li>
</ol>
<h3 id="traprs"><a class="header" href="#traprs">trap.rs</a></h3>
<p>This file defines the Rust functions and structures to handle interrupts</p>
<ul>
<li>defines the entry point function</li>
<li>defines the structure of the interrupts</li>
<li>defines the error handler for each interrupt (adjust the program counter appropriately)</li>
</ul>
<p><strong>We are going to borrow the RISCV macro, no-one has time to learn that stuff</strong></p>
<h3 id="the-trap-frame"><a class="header" href="#the-trap-frame">The Trap Frame</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(C)]
#[derive(Clone, Copy)]
pub struct TrapFrame {
	pub regs:       [usize; 32], // 0 - 255
	pub fregs:      [usize; 32], // 256 - 511
	pub satp:       usize,       // 512 - 519
	pub trap_stack: *mut u8,     // 520
	pub hartid:     usize,       // 528
}
<span class="boring">}</span></code></pre></pre>
<p>We store the SATP because it might change when a context switch happens. Maybe the SATP will reference  different Root table for another RAM, Maybe the ASID will change, The ASID tells us which process was executing (process ID) [unsure]</p>
<h2 id="rust-function-to-handle-interrupts"><a class="header" href="#rust-function-to-handle-interrupts">Rust function to handle Interrupts</a></h2>
<p>inputs : 
1. cause : helps us determine the type and code of the interrupt
2. epc   : helps us determine the latest value of the program counter
3. tval  : some error handling functions need the matval eg in a page fault, the mtval registe contains the address of the faulty page
4. </p>
<p>Output : the program counter to the next instruction after the error handler has done its thing</p>
<p>Steps :
1. Check if the interrupt is synchronous or asynchronous. We divide this way for modularities sake. Also it is a standard convention.
- If the interrupt is synchromous, go to step 2
- If the interrupt is asynchromous, go to step 6
2. Extract the cause code from the mcause
3. Make the return address point to the mepc... since this was the latest program counter
4. use a switch to call specific error handling functions based on <a href="handling_interrupts_and_traps.html#table-x">table x</a>
5. return the updated program counter
6. Extract the cause code from the mcause
7. Make the return address point to the mepc... since this was the latest program counter
8. use a switch to call specific error handling functions based on <a href="handling_interrupts_and_traps.html#table-y">table y</a>
9. return the updated program counter</p>
<h6 id="table-x"><a class="header" href="#table-x">Table x</a></h6>
<p><img src="./images/mcause_synchronous_interrupts.png" alt="" /></p>
<h6 id="table-y"><a class="header" href="#table-y">Table y</a></h6>
<p><img src="./images/mcause_asynchronous_interrupts.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-interrupts-and-traps"><a class="header" href="#handling-interrupts-and-traps">Handling Interrupts and Traps</a></h1>
<h4 id="terms"><a class="header" href="#terms">Terms</a></h4>
<p>An <strong>event trigger</strong> in this case means the action of an instruction causing the CPU to stop executing the current process and start executing the interrupt handler.</p>
<p>An <strong>Interrupt</strong> is an <em>event trigger</em> caused by an external device. This trigger is asynchronous (random).<br />
A <strong>Trap</strong> is a <em>deliberate</em> event trigger that is found in the program getting executed by the CPU. For example, a deliberate system call to access the file system.<br />
An <strong>exception</strong> is a <em>random</em> event trigger caused by the program that was being executed by the kernel. For example, a division by zero.</p>
<p>Some event triggers are <strong>synchronous</strong>, meaning that the currently executing instruction is the one that caused the event trigger... whether is was done deliberately or randomly we don't care. </p>
<p>Other event triggers are <strong>asynchronous</strong>, meaning that the event trigger was not caused by the currently executing instruction. It was caused by something other tan the currently executing instruction</p>
<p>Here are the asynchronous triggers :
<img src="./images/mcause_asynchronous_interrupts.png" alt="" /></p>
<p>Here are the synchronous triggers :
<img src="./images/mcause_synchronous_interrupts.png" alt="" /></p>
<p>In RiscV, if the CPU receives an interrupt or trap or exception, the CPU switches to MACHINE MODE. After switching to machine mode, it starts executing the function pointed to by the MTVEC. This function being pointed to is the interrupt handler.</p>
<p>The MTVEC stores a physical address. Not a virtual address. In Machine mode, memory accesses are through physical memory addresses.</p>
<p>Here is the layout of the MTVEC :<br />
<img src="images/RISCV/MTVEC_register.png" alt="MTVEC register" /></p>
<p><img src="images/RISCV/MTVEC_mode_field_values.png" alt="MTVEC mode field values" /></p>
<p>As you can see the mtvec has two fields : The Base and the Mode. The Base stores the physical address of a handler function. The Mode specifies whether the interrupt handling mechanosm is direct or vectored.</p>
<p><strong>Direct Mode</strong> 
Direct mode means that the address found in the Base is the root handler function. And that if the CPU receives any interrupt it will always point to that address only. The address points to one error handling function that stipulates how each interrupt is handled.</p>
<p><strong>Vectored Mode</strong><br />
Under this method, the CPU first determines the cause of the interrupt and then calls a specific error handling function. It achieves this by calling the funtion at address : (Base_address + 4 x Cause_code)</p>
<p>A direct mode mtvec means that all traps will go to the exact same function, whereas a vectored mode mtvec will go to different functions based on what caused the trap. 
For simplicity, we will use the Direct Mode, It is not nice to mess around with physical addresses when its unnecessary.</p>
<p>Considering the mode field uses 2 bits, we have to make the BASE address have two dispensable zeroes at the end. Meaning the BASE address needs to be aligned to 4.</p>
<ol>
<li></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>References :</p>
<ul>
<li><a href="https://sifive.cdn.prismic.io/sifive%2F834354f0-08e6-423c-bf1f-0cb58ef14061_fu540-c000-v1.0.pdf#%5B%7B%22num%22%3A164%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C0%2C630%2C0%5D">SiFive FU540-C000 Manual, Chapter 10 : Platform-Level Interrupt Controller(PLIC)</a></li>
</ul>
<p>We had previously defined the framework for handling interrupts all interrupts. But External interrupts need some special attention.  Why? becaus they come from an external environment. We need to choose how to interact with outside things. THink of it like creating an API for commuicating with external devices </p>
<p>An external interrupt in our case is an interupt that was not generated by the code running in the subject CPU. In our case, the external interrut is either:</p>
<ol>
<li>An interrupt coming from an external device such as a UART through the PLIC</li>
<li>A platform level internal interrupt</li>
</ol>
<h5 id="insert-image"><a class="header" href="#insert-image">insert Image</a></h5>
<p><img src="./images/plic_cpu_structure.png" alt="(PLIC and CLINT)interraction with CPU" /></p>
<p>The PLIC interfaces with the CPU via a physical Interrupt pin. The interrupt pin is enabled, disabled and configured using the MIE register (the machine interrpt Enable register) </p>
<p>The meie bit is found within the MIE register. It enables the acceptance of external interrupts.</p>
<p>Whenever we see that this pin has been triggered (an external interrupt is pending), we can query the PLIC to see what caused it.</p>
<p>Configuring the PLIC means :
1. Prioritizing certain interrupt sources in certain conditions.
2. Ignoring certain interupt sources under certain conditions.</p>
<p>The PLIC is not part of the core CPU. It is just a bunch of external circuits that interact with the CPU via an interrupt pin.<br />
We need to program the PLIC in order to  :
- filters out some external interrupts
- prioritize some external interrupts.</p>
<p>The PLIC can be programmed via MMIO programmning... meaning we can use Rust Code to control the PLIC.  The PLIC has a bunch of registers exposed in the I/O dedicated memory.<br />
Here are the Registers :</p>
<div class="table-wrapper"><table><thead><tr><th>Register</th><th>Address</th><th>Description</th></tr></thead><tbody>
<tr><td>Priority</td><td>0x0c00_0000 - 32bits long</td><td>Sets the priority of a particular interrupt source</td></tr>
<tr><td>Pending</td><td>0x0c00_1000 - 32bits long</td><td>Contains a list of interrupts that have been triggered (are pending)</td></tr>
<tr><td>Enable</td><td>0x0c00_2000 - 32bits long</td><td>Enable/disable certain interrupt sources</td></tr>
<tr><td>Threshold</td><td>0x0c20_0000 - 32bits long</td><td>Sets the threshold that interrupts must meet before being able to trigger.</td></tr>
<tr><td>Claim (read)</td><td>0x0c20_0004 - 32bits long</td><td>Returns the next interrupt in priority order.</td></tr>
<tr><td>Complete (write)</td><td>0x0c20_0004 - 32bits long</td><td>Completes handling of a particular interrupt.</td></tr>
</tbody></table>
</div>
<p>0x0c00_1000 minus 0x0c00_2000  == 4096 bytes difference. But the registers are 32 bits long. Don't let the 4096 byte difference fool you.... I don't know why there are 4096 gaps or any gaps at all.<br />
The PLIC is connected to the external devices and controls their interrupts through a programmable interface at the PLIC base address (registers shown in the table above).<br />
The PLIC is connected to multiple external devices. So we do not have to burden the CPU to have multiple pins. This makes our motherboard more modular. You can insert different PLICs without having to change the core CPUs.</p>
<p>We will use <a href="https://github.com/qemu/qemu/blob/master/include/hw/riscv/virt.h">this file</a> to see the virtual structure of Riscv in Qemu. Inside it we can see this rust enumeration that shows which PLIC pin each external device is attached to. :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum {
    UART0_IRQ = 10,
    RTC_IRQ = 11,
    VIRTIO_IRQ = 1, /* 1 to 8 */
    VIRTIO_COUNT = 8,
    PCIE_IRQ = 0x20, /* 32 to 35 */
    VIRT_PLATFORM_BUS_IRQ = 64, /* 64 to 95 */
};

<span class="boring">}</span></code></pre></pre>
<p>Our basic external interrupt will come from the UART. According to the enumeration above, The UART is attached to pin 10 of the PLIC.<br />
This means that Interrupt 10 is the UART external interrupt</p>
<h4 id="claimcomplete-rgister"><a class="header" href="#claimcomplete-rgister">Claim/Complete Rgister</a></h4>
<p>The claim process gets us the next interrupt after prioritization has already happened. For example, if the UART is interrupting
and it's next, we will get the value 10 if we claim.<br />
Interrupt 0 is a &quot;null&quot; interrupt and is hardwired to 0. So if we read 0 after performing a claim... it means that there are currently no interrupts in the buffer.</p>
<p>The Complete register is used by the processor to inform the PLIC that it has finished handling a specific interrupt. When the processor writes the number of the handled interrupt to the Complete register, the PLIC removes the interrupt from the pending list and updates its internal state. If you write a wrong value, the PLIC does nothing... it just ignores your garbage value.</p>
<p>The PLIC can differenciate when we either write or read the Claim/Complete register. </p>
<h4 id="the-plic_int_enable"><a class="header" href="#the-plic_int_enable">the plic_int_enable</a></h4>
<p>The plic_int_enable register is a bit-encoding register that controls the interrupt enable status of each interrupt source in the PLIC. The register has a bit for each interrupt source, where each bit corresponds to a specific interrupt source number. If a bit is set to 1, the corresponding interrupt source is enabled, and interrupts from that source can be forwarded to the processor for handling. If a bit is set to 0, the interrupt source is disabled, and interrupts from that source will not be forwarded to the processor.</p>
<p>The plic_int_enable register is typically set by the operating system during system initialization to enable or disable specific interrupt sources based on the system's requirements. The register can also be modified by an interrupt handler to dynamically enable or disable specific interrupt sources during runtime.</p>
<p>For example, if you have a system with multiple devices that generate interrupts, you can use the plic_int_enable register to enable interrupts only from the devices that are required for a specific task. By enabling only the necessary interrupt sources, you can reduce the system's interrupt load and improve overall performance.</p>
<p>Overall, the plic_int_enable register is an important part of the PLIC's operation, as it allows the operating system to control which interrupt sources are enabled or disabled, and which interrupts are forwarded to the processor for handling.</p>
<h4 id="the-threshold-register"><a class="header" href="#the-threshold-register">The Threshold register</a></h4>
<p>The Threshold register is used to configure the interrupt priority threshold for the PLIC. The PLIC assigns a priority level to each interrupt request it receives from different devices, and the Threshold register determines which interrupts are sent to the processor for handling.</p>
<p>The Threshold register contains a priority threshold level, which is used to filter the pending interrupts in the PLIC. Interrupt requests with a priority level equal to or higher than the threshold level will be sent to the processor, while interrupts with a lower priority level will be blocked until the threshold level is changed.</p>
<p>The threshold register is typically set by the operating system during system initialization or by an interrupt handler to dynamically adjust the interrupt priority level. By setting the threshold level, the operating system or the interrupt handler can control the priority of the interrupts and avoid overwhelming the processor with a large number of lower priority interrupts.</p>
<h4 id="interrupt-source-priority-registers"><a class="header" href="#interrupt-source-priority-registers">Interrupt Source Priority registers</a></h4>
<p>The priority levels of different interrupt sources can be set during system initialization by configuring the PLIC's registers, including the Interrupt Source Priority registers, which assign a specific priority level to each interrupt source.</p>
<p>It is upto the kernel programmer to define :</p>
<ul>
<li>which interrupts they are willing to handle</li>
<li>The priority of each interrupt</li>
<li>The threshhold under different conditions</li>
</ul>
<p>So after defining all the above three things, you can pass those definitions to interact with the PLIC interface.<br />
For example you can just send a threshold value to the PLIC when the CPU gets overwhelmed.</p>
<h5 id="plicrs"><a class="header" href="#plicrs">plic.rs</a></h5>
<ul>
<li>Abstract the plic starting addresses to the registers (Each register is 4-bytes (u32) regardless of the differences between the starting addresses)</li>
<li>functions include :
<ul>
<li>claim the next pending interrupt</li>
<li>inform the plic of complete interrupt handling (return correct id) </li>
<li>inform the PLIC of the threshold required by kernel</li>
<li>Check if interrupt X is pending</li>
<li>Enable a specific an external interrupt</li>
<li>Set a given interrupt priority to the given priority.</li>
</ul>
</li>
</ul>
<p>initialization:</p>
<ul>
<li>inform the PLIC of the threshold required by kernel</li>
<li>enable interrupts - plic_int_enable</li>
<li>set priority of interrupts (0-7) - Interrupt Source Priority register</li>
</ul>
<p>The PLIC will signal Our OS through the asynchronous cause 11 (Machine External Interrupt). That is how the mcause will record an interrupt from the PLIC -- 11 asynchronous.</p>
<p>Now that the mcause just says &quot;11&quot;, this info does not specify which specific interrupt happened... it jus says a Machine External Interrupt happened. For us to know which specific External Interrupt happened, we read the claim/complete register.</p>
<p>We need to use a singleton design to implement every module that gets shared between more than one module. Modules need to be thread safe</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-processes"><a class="header" href="#setting-up-processes">Setting Up processes</a></h1>
<p>What are processes in this context? - a process is a program in action. It is program code and data that has been copied from the hard disk and put in the RAM in a format that the CPU can interact with.</p>
<p>we have 2 kinds of processes that we need to set up :</p>
<ol>
<li>Kernel processes  --&gt; kernel functions that get packaged and summoned as a process</li>
<li>User Processes    --&gt; non_kernel code that gets loaded into RAM from the Hard disk</li>
</ol>
<h3 id="why-do-we-need-to-wrap-some-kernel-functions-as-processes"><a class="header" href="#why-do-we-need-to-wrap-some-kernel-functions-as-processes">Why do we need to wrap some kernel functions as processes?</a></h3>
<h3 id="a-program-in-memory-a-program-in-action"><a class="header" href="#a-program-in-memory-a-program-in-action">A program in memory, A program in Action</a></h3>
<p>So how does a program in memory look like?<br />
Well... it depends on the implementation of the particular OS. In our case, all the memory associated with a process include :</p>
<p>A. <strong>The Process Structure</strong> - The process structure contains the metadata about the process.<br />
B. <strong>The process stack</strong> - The process stack is where local variables and function addresses get temporarily stored at runtime. It is like an ordered buffer of instructions for the CPU to fetch.<br />
C. <strong>The process' <em>loadable</em> elf sections</strong> - This is where the code and data of the program are defined. The stack gets its data from here.<br />
D. <strong>The process trap_frame</strong> - when the CPU performs a context switch, the context of the interrupted process gets stored in the Trap Frame</p>
<h4 id="a-the-process-structure"><a class="header" href="#a-the-process-structure">A. The Process Structure</a></h4>
<p>Each process has a process structure. A process structure is a data structure that represents the state and attributes of a process in an operating system. A process structure contains metadata about a process.<br />
It typically contains information such as the process ID (PID), the program counter (PC), memory allocation details, and other resources associated with the process.</p>
<p>The process structure is used by the operating system code to manage and control the execution of processes on the system. It allows the operating system to keep track of the resources used by each process, schedule them for execution, and provide them with access to system resources such as I/O devices and memory.</p>
<p>Under the process structure, we have the following pieces of information :</p>
<ol>
<li>The process ID</li>
<li>The process state</li>
<li>The program counter (keeps track of current instruction step)</li>
<li>The address to the process' stack</li>
<li>The TrapFrame associated with that process</li>
<li>The process data section -- this is completely optional, I really do not know why we need this. maybe I will know in the future</li>
</ol>
<p>here is a rust presentation of the Process structure :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(C)]
pub struct Process {
  frame:           TrapFrame,
  stack:           *mut u8,
  program_counter: usize,
  pid:             u16,
  root:            *mut Table,
  state:           ProcessState,
}
<span class="boring">}</span></code></pre></pre>
<h4 id="1-the-process-state"><a class="header" href="#1-the-process-state">1. The Process state</a></h4>
<p>The process state indicates whether a process is currently executing, waiting for input/output (I/O), waiting for a resource, or in some other state.<br />
In our case, the process state will indicate whether the process is : Running, Waiting, Sleeping or Dead.<br />
THis information is important because it will help us know :</p>
<ul>
<li>Which process to kill</li>
<li>Which process to remove from the CPU</li>
<li>Which process is ready to be loaded to the CPU.</li>
</ul>
<p><strong>A Running process</strong> is a process that is currently getting executed by the CPU. To be precise, the program counter is referencing addresses found within the process text section.</p>
<p><strong>A Sleeping process</strong> is a process that voluntarily stopped and is taking a timeout in order to wait for its turn to use the CPU. This process will run perfectly if given a chance to get adressed by the CPU program counter.</p>
<p><strong>A waiting Process</strong> is a process that has involuntarily stopped because a certain condition has not been met. For example an I/O operation.</p>
<p><strong>A dead process</strong> is a process that is non_existent or it has finished executing and has been terminated by the operating system. The PCB for a terminated process is typically removed from memory. </p>
<h4 id="the-trap-frame-1"><a class="header" href="#the-trap-frame-1">The Trap Frame</a></h4>
<p>If the CPU gets an interrupt, it immediately switches to machine mode and calls the interrupt handler function found in the MTVEC register... the interrupt handler saves the context of the interrupted process in a structure called a Trapframe.<br />
Here is the structure of a trapframe :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span>
<span class="boring">fn main() {
</span>#[repr(C)]
#[derive(Clone, Copy)]
pub struct TrapFrame {
  pub regs:       [usize; 32], // 0 - 255
  pub fregs:      [usize; 32], // 256 - 511
  pub satp:       usize,       // 512 - 519
  pub trap_stack: *mut u8,     // 520
  pub hartid:     usize,       // 528
}

<span class="boring">}</span></code></pre></pre>
<p>Each process has its own trapframe. This is so because there is a static queue of Process structures declared in the heap memory. And each Process structure contains a Trap Frame inside.</p>
<h4 id="initializing-a-process"><a class="header" href="#initializing-a-process">Initializing a process</a></h4>
<p>Creating a process is all about imprinting a program in the RAM. So our fisrt objective would be to allocate free memory in the RAM in order to store the : Process Structure, Process Trap frame, Process Stack and Process elf sections. </p>
<p>But there is already space for the Process structure in the Process_List Queue... so we just update one of the empty slots in this queue with the new proces information.</p>
<p>The trap frame has been declared as part on the Process structure, so there is already some space for it.</p>
<p>Since we are wrapping up a kernel process, we don't have elf sections, instead we will reference the function address as the entrypoint of the process.</p>
<p>We allocate 2 pages for the process stack. We only store the pointer to the stack in the Process structure.</p>
<p>We are dealing with virtual addresses. And as you recall, each process is given a unique ID. And each ID is associated with a unique virtual address space. Or rather, each process has a unique record of which pages it has used.</p>
<p>This table is stored as part of the Process Block Structure [undone]</p>
<p>Here is the process of starting a kernel process: [undone]</p>
<ol>
<li>Pass a kernel function address to Process_constructor, let's call this address k_func</li>
<li>create an empty process structure</li>
<li>Fill the process structure with the following default values :
<ul>
<li>Navigate into the Trapframe and set all values within the trapframe to zero</li>
<li>make the address pointing to the stack to point to a null pointer</li>
<li>make program counter point to a null address</li>
<li>make the process state to be dead</li>
<li>make the pid point to an invalid address</li>
</ul>
</li>
<li>Allocate a couple of pages </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-block-driver"><a class="header" href="#the-block-driver">The Block Driver</a></h1>
<p>Theory : Virtio Protocol</p>
<p>VirtIO protocol is a communication protocol. It defines how a virtual machine communicates with a hypervisor.<br />
Okay... now you are asking &quot;what do virtual machines and hypervisors have to do with us? we were just here chilling and talking about bare-metal operating systems!&quot;</p>
<p>The thing is that Virtio protocol defines Virtio devices. Virtio devices are virtual devices that have a simple standard interface. For example, it defines network cards and hard disks. Qemu uses provides us with this devices.<br />
In our case, we will act ad the VM  and Qemu will act as the Hypervisor.</p>
<p>We will deal with virtio block devices. We will write a driver for a Virtio Block device.</p>
<p><img src="./images/Virtio%20MMIO%20Register%20Layout%20part%201.png" alt="Virtio memory layout part 1" />
<img src="./images/Virtio%20MMIO%20register%20layout%20part%202.png" alt="Virtio memory layout part 2" />
<img src="./images/virtio%20device%20IDs.png" alt="Virtio device type IDs" /></p>
<p>For the QEMU emulator, it puts virtio devices (backwards) from 0x1000_1000 to 0x1000_8000. If we only have one device, it should be attached at 0x1000_8000<br />
Qemu supports 8 Virtio buses. Each having 4096 byte register space.</p>
<p>We use MMIO programming to communicate with the underlying Virtio device
MMIO registers for Virtio in Riscv are 32 bits only</p>
<p><em><strong>Abstracting the MMIO Bus address full of registers</strong></em></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Below are offsets
#[repr(usize)]
pub enum MmioOffsets {
  MagicValue = 0x000,
  // Version = 0x004,
  DeviceId = 0x008,
  VendorId = 0x00c,
  HostFeatures = 0x010,
  HostFeaturesSel = 0x014,
  GuestFeatures = 0x020,
  GuestFeaturesSel = 0x024,
  GuestPageSize = 0x028,
  QueueSel = 0x030,
  QueueNumMax = 0x034,
  QueueNum = 0x038,
  QueueAlign = 0x03c,
  QueuePfn = 0x040,
  QueueNotify = 0x050,
  InterruptStatus = 0x060,
  InterruptAck = 0x064,
  Status = 0x070,
  Config = 0x100,
}


<span class="boring">}</span></code></pre></pre>
<ul>
<li><em>probe function</em> - We first need to scan the buses dedicated to connecting the virtio devices (from 0x1000_1000 to 0x1000_8000) and check which devices is attached to each bus 
<ul>
<li>loop through the buses in a bus-wise fashion</li>
<li>confirm that indeed each bus is a virtio bus address by reading the magic value</li>
<li>check if a device is connected to the bus. (if device_id &gt; 0)</li>
<li>Check the type of device attached to the bus (device ID 2 == block device)</li>
<li>If we find that the bus is a virtio bus and device id 2, we can configure this device as a block device. </li>
</ul>
</li>
</ul>
<p><em><strong>configuring the device (initializing_the_device)</strong></em><br />
Before we use the device(any device in the world), we must set the right configurations by adjusting the values in the MMIO registers.<br />
These steps are commonly referred to as the &quot;initialization handshake&quot;. They are used to establish communication between the driver and the device.</p>
<p>The steps for configurating a device are laid out in the virtio specification as (<a href="https://docs.oasis-open.org/virtio/virtio/v1.1/cs01/virtio-v1.1-cs01.html#x1-920001">source</a>):</p>
<p>pre-requisites : </p>
<ul>
<li><a href="https://docs.oasis-open.org/virtio/virtio/v1.1/cs01/virtio-v1.1-cs01.html#x1-100001"><strong>Device Status Register</strong> fields</a> </li>
<li></li>
</ul>
<ol>
<li>Reset the device by writing 0 to the Device status register. Resetting means setting values to known default values. This removes any uncertainities or unpredictable behaviour, we begin our communication on a clean slate. No surprises.</li>
</ol>
<ul>
<li>Reading from the Device status register returns the current device status flags.</li>
<li>Writing non-zero values to this register sets the status flags, indicating the OS/driver progress. </li>
<li>Writing zero (0x0) to this register triggers a device reset. The device sets QueuePFN to zero (0x0) for all queues in the device.</li>
</ul>
<ol start="2">
<li>Set the ACKNOWLEDGE status bit to the status register.</li>
</ol>
<ul>
<li>within the device status register, there is a single bit called the Acknowledge status bit.</li>
<li>The Acknowledge status bit value is 1 when both the driver and device agree that they are connected to each other and can continue with initialization. This bit answers the question &quot;Are we connected to each other? And are all of us okay with this connection?&quot;</li>
<li>The driver sets the Bit to 1 to acknowledge that it has access to the device registers and has found it to be a valid virtio device(connected).</li>
<li>The device reads, the bit and also resets it to 1 to confirm that it is okay with the connection.</li>
<li>The device cannot reset the bit to 0. If an error occurs within the device and the device is not okay with the connection, it sends a message through other status bits such as DEVICE_NEEDS_RESET or DEVICE_FAILED bits, to indicate that the initialization process has failed and needs to be restarted in order to create a proper connection.</li>
</ul>
<ol start="3">
<li>Set the DRIVER status bit to the status register </li>
</ol>
<ul>
<li>The driver status bit indicates that the Guest has a driver that can drive the device.</li>
<li>After the driver sets the Acknowledge bit to 1, it sets the DRIVER status bit to 1 to indicate that it is in control of the device and is ready to negotiate the features that the device supports. The device then acknowledges this by setting the ACKNOWLEDGE status bit to 1 and waits for the driver to read the device features from the host_features register.</li>
</ul>
<ol start="4">
<li>Read device features from <strong>host_features register</strong>.</li>
</ol>
<ul>
<li>Here are the <a href="https://docs.oasis-open.org/virtio/virtio/v1.1/cs01/virtio-v1.1-cs01.html#x1-2420003">Features of the virtio block device</a></li>
<li>Each virtio device offers all the features it understands. During device initialization, the driver reads this and tells the device the subset that it accepts. The only way to renegotiate is to reset the device.</li>
<li>This allows for forwards and backwards compatibility: if the device is enhanced with a new feature bit, older drivers will not write that feature bit back to the device. Similarly, if a driver is enhanced with a feature that the device doesn’t support, it see the new feature is not offered. </li>
<li>Feature bits are allocated as follows:
0 to 23
Feature bits for the specific device type 
24 to 37
Feature bits reserved for extensions to the queue and feature negotiation mechanisms 
38 and above
Feature bits reserved for future extensions.
Note: For example, feature bit 0 for a network device (i.e. Device ID 1) indicates that the device supports checksumming of packets.</li>
</ul>
<ol start="5">
<li>Negotiate the set of features and write what you'll accept to <strong>guest_features register</strong>.</li>
</ol>
<ul>
<li>The Guest_features register contains Write_only Flags representing device features understood and activated by the driver.</li>
<li>Fill this out so as to establish compatibility between</li>
</ul>
<ol start="6">
<li>
<p>Set the FEATURES_OK status bit to the status register '</p>
<ul>
<li>The FEATURES_OK status bit Indicates that the driver has acknowledged all the features it understands, and feature negotiation is complete. </li>
<li>The driver sets the FEATURES_OK status bit to indicate to the Virtio device that it has negotiated a set of compatible features. </li>
<li>During device configuration, after the driver negotiates the set of features and writes what it will accept to the guest_features register, the device checks if it can support the negotiated features. If the device does not support all the features negotiated by the driver, it may set the FEATURES_OK status bit to 0 to indicate that it cannot accept the negotiated features</li>
</ul>
</li>
<li>
<p>Re-read the status register to confirm that the device accepted your features. The driver might have reset the Features_OK register to 0.</p>
</li>
<li>
<p>Perform device-specific setup - The driver performs any device-specific setup required to initialize the Virtio device, such as configuring queues and memory mappings.</p>
<ul>
<li>Set <strong>Queue Number register</strong>.
<ul>
<li>The QueueNum is a register in the device's configuration space that specifies the maximum number of virtqueues supported by the device. By default, the Queue Number is set to 1...such that there is only one virtual queue occupying all of the buffer space.</li>
<li>But you may need multiple queues to implement parallelism. So you will have multiple queues under the same buffer space.</li>
<li>The <strong>Queue_Num_Max register</strong> is provided by the device (Read only), it specifies the total amount of parallel queues you can have </li>
</ul>
</li>
<li>VIRTIO_RING_SIZE is a constant defined in the VirtIO specification and represents the maximum number of elements (or &quot;entries&quot;) in a VirtIO ring (default == 1024). A VirtIO ring is a data structure used by the VirtIO device and the driver to communicate with each other. It is a circular buffer of fixed size that is divided into two sections: the &quot;available ring&quot; and the &quot;used ring&quot;. </li>
</ul>
</li>
</ol>
<p>Actual :
- step 7 :
- read Queue_Num_Max register and see the max number of queues that is supported for parallelism
- We randomly chose 1024 (VIRTIO_RING_SIZE) to be the number of parallel queues
- We calculate the number of pages required to store the Queue structure for the block device. We add PAGE_SIZE - 1 to the size of the Queue structure (which is obtained using the size_of function), then divides the result by PAGE_SIZE to round up to the nearest page. The addition of PAGE_SIZE - 1 is to ensure that the calculation rounds up to the nearest page, so that there is no wasted memory.
- We create space for the virtual queue in our driver. We do this by allocating pages that will be used by the queue
- allocate memory for the queue and stores the physical address of the queue in the queue_pfn </p>
<ol>
<li>Set the DRIVER_OK status bit to the status register. The device is now LIVE.
<ul>
<li>The DRIVER_OK status bit indicates that the driver is set up and ready to drive the device. </li>
<li>The driver sets the DRIVER_OK status bit to 1 to indicate to the Virtio device that initialization is complete and it is ready to use the device.</li>
</ul>
</li>
</ol>
<p><em><strong>Communicating</strong></em>
Now that the device is LIVE, we can start making requests by using the virtio rings.<br />
The virtio descriptor/ring system is generic; however, we have a protocol when making block requests. We will make a block request using three descriptors: 
(1) block request header<br />
(2) block request buffer<br />
(3) block request status.</p>
<p>The Block request header specifies the type of operation and the sector number<br />
After the header, we store the data buffer. For reads, the device will write to this piece of memory, and for writes, the device will read from this piece of memory. It's important to note that these must be physical addresses, since the block device bypasses the MMU.</p>
<p>Finally, we have a status field. The device will write the result of the request to this 8-bit field. There are currently only three responses we can get: 0-success, 1-failure, 2-unsupported operation. That doesn't give us a lot of information, but if we get a 0, we can reasonably assume that our request was properly handled.</p>
<p>When making a request to the device, the memory we use to store the request must remain valid until the device has finished processing the request and has sent back a response. If we use memory that is allocated on the stack, it will be automatically deallocated when the function exits, which could lead to undefined behavior if the device tries to access that memory later.</p>
<p>To avoid this issue, we need to allocate heap memory to store the request. This memory will stay resident until we explicitly deallocate it, so we can be sure that it will remain valid until we have received a response from the device.</p>
<p>We will grab three open descriptors from the virtio queue, populate it with the header, buffer, and status, and then we write the virtqueue's number (0) into the queue_notify register to tell the device to start working on the request. </p>
<p>I will import the block driver... this block driver has too much detail</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-calls"><a class="header" href="#system-calls">System calls</a></h1>
<h3 id="the-ecall-instruction"><a class="header" href="#the-ecall-instruction">The ecall instruction</a></h3>
<p>In assembly code, the ecall instruction takes no arguments like how other functions do : For example:</p>
<pre><code class="language-riscv">li a7, 93   // as you can see the Load immediate instruction takes 2 arguments (a7 and 93)
ecall       // but ecall has no arguments
</code></pre>
<p>In accordance to the RISCV Calling convention, when a system call is made :
- The identification code of the system call gets saved in the a7 register. eg the identification code for the exit system call is number 1. If you use the RISCv system calling convention, you will have the luxury of having syscalls and their corresponding codes defined for you. You are free to tweak them or add your own syscall types and codes.
- The arguments of the syscall gets stored in registers a0-a5.
- If the arguments are more than six, the remaining arguments get stored in the stack.
- The return result from the syscall get stored in the a0 register.</p>
<p>The ecall does not directly make the CPU to switch modes... but it gets used in the process of swiching CPU modes.<br />
The ecall however makes a software interrupt. The CPU receives the software interrupt. And since the interrupt handler is defined in either Supervisor mode or Machine mode, the CPU is forced to switch mode and elevate its priviledges to the appropriate level. And as usual after the interrupt handler has done its thing it <em>returns the CPU back to the normal state</em> using the mret OR sret instruction.</p>
<p><em>returns the CPU back to the normal state</em> means that the interrupt handler calls the mret instruction... or the sret instruction or the uret instruction.<br />
When the interrupt handler is called, it first saves the context of the interrupted process. It saves all the CPU registers in a struct called a TrapFrame. The information about the Location of the Trap Frame can be found in the process's structure.<br />
The Program Counter register in the trapFrame contains the address of the next instruction that was supposed to get executed if the interrupt had not happened.<br />
The MEPC register or SPEC register or EPC register in the trapFrame contains the address of the instruction that caused the interrupt. In our case, it contains the address to ecall.<br />
It is up to the trap handler to make the MEPC/ EPC/ SEPC to point to the next appropriate instruction address because... </p>
<p><strong>Because when we call mret/ sret/ uret....</strong>
- <strong>the CPU copies the value of MEPC to the trapframe PC.</strong>
- The CPU gets restored to the previous priviledge status</p>
<p>[undone] : The ABI<br />
[undone] : the syscalls we are accomodating<br />
[undone] : The WASI syscalls<br />
[undone] : how to implement {syscalls, WASI_integration}</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filesystem"><a class="header" href="#filesystem">Filesystem</a></h1>
<h2 id="theory"><a class="header" href="#theory">Theory</a></h2>
<p><strong>What is a file?</strong><br />
A file is a named collection of static data.<br />
Data can be a byte, a couple of bytes or gigabytes.<br />
The data collection needs to be static so as to store data persistently. So in this case, files can be redefined as : A named data collection in a persistent storage device.</p>
<p><em><strong>File Types</strong></em><br />
There are different types of files. Types emmerge due to either &quot;the file format&quot; or &quot;the file purpose&quot;.<br />
For example an elf file has a different format to a .exe file... but they serve the same purpose.<br />
A data file has the same format as a system file... but they erver different purposes</p>
<p>File Types include :</p>
<ol>
<li>Regular Data Files : These are files that contain user-level information. It can contain Binary data or ASCII data.</li>
<li>Directory Files : These are files that describe the structure of the file system</li>
</ol>
<p><em><strong>Block device</strong></em><br />
A block is a kind of storage device that allows data to be read and written to it in fixed sizes called blocks. A block is a large group of bytes. From this, it it clear that a block storage device consisting of many many bytes. For example a HDD device or SSD device.</p>
<p><em><strong>A Character device</strong></em><br />
A character device is also a storage device that only allows data to be read and written to it character-wise... or rather byte-wise.</p>
<p>Block devices are suitable in situations where we need performance. This is because Character devices force the CPU to do more I/O operations. I/O operations are expensive as hell.  We always need performance</p>
<p>Character devices are suitable when we need data in a stream-wise fashion... a continuous fashion. Eg data from a keyboard or a mouse. Keyboards and Mice are character devices.</p>
<p><em><strong>A disk Partition</strong></em>
A disk is a fancy name for a hardisk ... a hardisk is a type of a block device.<br />
A partition scheme is a specification of how to logically seperate bytes in a hard disk into seperate sections called partitions.</p>
<p>We have many partitioning specifications eg MBR, UEFI, GPT LVM<br />
If you use a partitioning software to partition your disk, your disk will typically have 3 parts...</p>
<ol>
<li>The partial_boot code : The partial boot code will help the firmware locate the bootloader that may be found in one of the partitions</li>
<li>The partition table </li>
<li>The actual Partitions</li>
</ol>
<p>The Partition table contains metadata about the partitions. It typically contains info such as :</p>
<ul>
<li>The file systems found in each partition</li>
<li>The status flags of each partition eg : Bootable flag, writable flag</li>
<li>The size of each partition</li>
<li>The memory boundaries of each partition</li>
<li>The type of each partition (primary? swap? extended? logical? )</li>
</ul>
<p><em><strong>A File system</strong></em><br />
A file system is a software definition of how the bytes in a block device are organized. To be more specific a file system is a software definition of how the bytes in a <strong>partition</strong> are organized.<br />
We have many file systems in existence...each with their own usecase and trade-offs. For example : NTFS, ExFAT, EXT4, BTRFS and Minix 3 File system.</p>
<p>We will focus on the Minix 3 file system... but with tweaks</p>
<h3 id="minix-3-file-system-our-variation"><a class="header" href="#minix-3-file-system-our-variation">Minix 3 File System (our Variation)</a></h3>
<p>The minix 3 Filesystem organizes the bytes within a partition as follows :<br />
<img src="./images/Minix%203%20filesystem.png" alt="Minix 3 file system" /></p>
<p><strong>A block</strong> is a group of bytes. Like in minix 3, a block typically means 1024 contiguous bytes. But this is not always the case. For example, the superblock is only 32 contiguous bytes.</p>
<p><strong>A zone</strong> is a group of blocks. Like in Minix 3, a zone usually contains 4 contiguous blocks.</p>
<p><strong>A bitmap</strong> is an array of bits.</p>
<h4 id="the-boot-block"><a class="header" href="#the-boot-block"><strong>The Boot block</strong></a></h4>
<p>This section might contain boot code for an operating system. If there is no operating system...this section is filled with zeros. The bootblock is 1024 bytes long. Sometimes the 1024 byte is not enough... in such a case, part of the boot code will get stored in the Data zone.</p>
<h4 id="the-superblock"><a class="header" href="#the-superblock"><strong>The SuperBlock</strong></a></h4>
<p>The Superblock is 32 bytes long.<br />
The Superblock contains metadata about the entire Filesystem. Here is a struct showing the data contained in a superblock :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(C)]
pub struct SuperBlock {
  pub ninodes:         u32,  // the total number of index nodes
  pub pad0:            u16,
  pub imap_blocks:     u16,  // 
  pub zmap_blocks:     u16,
  pub first_data_zone: u16,
  pub log_zone_size:   u16,
  pub pad1:            u16,
  pub max_size:        u32,
  pub zones:           u32,
  pub magic:           u16,  // identifies the file system type (Minix == 0x4d5a)
  pub pad2:            u16,
  pub block_size:      u16,
  pub disk_version:    u8,
}
<span class="boring">}</span></code></pre></pre>
<p>The superblock does not change once the partition gets mounted. This is because the superblock contains static information such as &quot;the number of inodes&quot; which will only change when you resize the disk.</p>
<p>Our file system code only needs to read from this Superblock structure. Recall that we can find this structure after the boot block. The default block size of the Minix 3 file system is 1,024 bytes, which is why we can ask the block driver to get us the super block using the following.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// descriptor, buffer, size, offset
// synchronous_read ... it's synchronous because we suspend the calling code until the read operation is done.
// If it were an asynchronous read... the calling code would have continued doing other things as it waits for the I/O operation to end.
syc_read(desc, buffer.get_mut(), 512, 1024);
<span class="boring">}</span></code></pre></pre>
<p>The super block itself is only about 32 bytes; however, recall that the block driver must receive requests in sectors which is a group of 512 bytes. Yes, we waste quite a bit of memory reading the super block, but due to I/O constraints, we have to.  This is why I use a buffer in most of my code. We can point the structure to just the top portion of the memory. If the fields are aligned correctly, we can read the superblock by simply referencing the Rust structure. This is why you see #[repr(C)] to change Rust's structure to a C-style structure. </p>
<p>The Superblock is 32 bytes but our block read function reads 512 bytes at once. So we will just read the 512 bytes and store them in a buffer... from there we can just disect it using offsets of u8</p>
<h4 id="the-padding-to-even-out-a-block-to-1024-bytes"><a class="header" href="#the-padding-to-even-out-a-block-to-1024-bytes">The Padding to even out a block to 1024 bytes</a></h4>
<p>Considering the superblock</p>
<h4 id="the-z-map-and-imap"><a class="header" href="#the-z-map-and-imap">THe Z map and imap</a></h4>
<p>The Z map is a bunch of 1-bit arrays. Each bit represents an existing zone. 1 means the zone is taken and 0 means the zone is free.<br />
The Imap is a bunch of 1-bit arrays. Each bit represents an existing inode. 1 means the zone is taken and 0 means the zone is free.</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>If you make a function a process, it can be executed independently if it is pure non_dependent code.<br />
Our file block driver initially did synchronous reads. Synchronous reads bad for performance.<br />
We need to mak our file reads asynchronous. Asynchronous reads are good for performance.<br />
We can make our file reading asynchronous by turning the file reading operation into a kernel process... instead of a procedural function. This kernel process will only be ready to get executed if we receive an interrupt from the block device.(the block device sends interrupts to signal that it has finished a read operation).</p>
<p>CAN I JUST USE THE PREDEFINED MINIX FILESYSteM?????!!!!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="user-processes"><a class="header" href="#user-processes">User Processes</a></h1>
<h4 id="how-do-we-write-userspace-programs"><a class="header" href="#how-do-we-write-userspace-programs">How do we write userspace programs?</a></h4>
<p>To write code for a user program, we need a standard library. The Std library contains general functions whose backends are connected to specific OS system calls or datastructures. For example the printf() function calls screen_output syscalls in the background.</p>
<p>We need to write our own Standard library.<br />
But how do we do that? Is it as complex as it sounds? I don't know ha ha ha wueeh! It's getting worse and more interesting... who knew?...I am so fucked. What was I thinking when choosing this project. Funny thing is, I don't care if I fail...meeh... what's going on.</p>
<p>There are a couple of ways that we can run user programs... from now on let us assume that that user is hello_world.</p>
<p>We can make hello world part of the kernel. Such that if we modify the hello_worl, we have to recompile the whole OS. This means that the kernel will be like a crate and the system Interface will be the crate API. This is what people in embedded used to typically do, the kernel is just a library for the bare metal applications.</p>
<p>We will use this method before we write a standard library, write program loader and implement the file system. The second method below needs all the three things above to be completed</p>
<p>In the second method we compile the OS and load it in the RAM of a RISCV machine. We store a precompiled std library in the hard-disk. We store programs that depend on the std library on the hard disk too. Those programs get compiled while getting linked to the std library. </p>
<p>we then load the program using the kernel's program loader</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overall-design"><a class="header" href="#overall-design">Overall Design</a></h1>
<p>Here we have the Computer components</p>
<ol>
<li>CPU(Hart0)</li>
<li>RAM</li>
<li>MMIO Memory</li>
<li>HDD Memory</li>
<li>Screen</li>
<li>Keyboard</li>
<li>PLIC  (Platform Level Interrupt Controller)</li>
<li>CLINT (core Local Interruptor)</li>
<li>UART</li>
</ol>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<!-- 
##### Late desigs

Let us be on the same page:
1. People romantisize heroic statements
2. Someone who understands something gains the ability to explain things well
3. You can only say you truly understand something if you can recreate it. (given enough time and resources).

heroic things like ; 
    - do hard things
    - be innovative
    - Implement time travelling

The thing is, these things are ugly on the ground. People fail terribly, no matter how obssessed they were.  
An innovator faces the risk of being wrong and poor.  They may be a liitle lucky and be right but poor. It is very hard to be truly innovative.

If you were given 2 years to implement time travel, chances are half way you will still not have understood what you are doing. You won't have understood the implementation. You won't have the desigs. The designs will keep on changing.

 -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="definitions-and-theories"><a class="header" href="#definitions-and-theories">Definitions and Theories</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-linker-1"><a class="header" href="#the-linker-1">The linker</a></h1>
<h4 id="what-is-a-linker"><a class="header" href="#what-is-a-linker">What is a linker?</a></h4>
<p>A linker is a program that links object files generated by a compiler into an executable or shared library. It resolves external symbols and relocations, and generates the final output file.</p>
<h4 id="what-are-external-symbols"><a class="header" href="#what-are-external-symbols">What are External symbols?</a></h4>
<p>External symbols are variables that get declared in one object file, but they also get used in another different object file. For example, I may declare a global variable 'X' in a header file. If I reference this header file in another file (eg main.rs) and use variable X... that means X has become an external symbol.</p>
<h4 id="what-does-resolving-exernal-symbols-mean"><a class="header" href="#what-does-resolving-exernal-symbols-mean">What does resolving Exernal symbols mean?</a></h4>
<p>In the context of the linker, resolving External symbols means finding the actual memory location or address of a symbol that is referenced by another module, and updating the reference in the module that uses the symbol to point to the correct memory location.</p>
<h4 id="what-is-relocation"><a class="header" href="#what-is-relocation">What is relocation?</a></h4>
<p>Relocation is the act of changing the memory address pointed to by a variable. In the case of a linker, after it has resolved all external symbols, it changes the memory addresses of those external symbols and makes them point to different memory addresses that were specified by the linking script.</p>
<p>So the relocation process adjusts the addresses of symbols in an object file to reflect their final location in memory. In this case, we mean virtual memory addresses... NOT Physical memory addresses.</p>
<h4 id="what-is-this-linking-script"><a class="header" href="#what-is-this-linking-script">What is this linking script?</a></h4>
<p>A linker script is a text file that provides additional instructions to the linker about how to link the input files. It can specify the layout of the output file or the order in which the input files should be linked.</p>
<p><a href="go_back?">crude</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="qemu"><a class="header" href="#qemu">Qemu</a></h1>
<p>QEMU is a generic and open source machine emulator and virtualizer.</p>
<p>A machine emulator is a software program that simulates the behaviour of another computer or another computing system. For example you may simulate the behavior of a quantum computer on a convetional computer.</p>
<p>A virtualizer is a program that abstracts an underlying system. The underlying system can be anything : Bare metal cpu, a hard disk, an operating system... anything.</p>
<p>QEMU can be used in several different ways. The most common is for System Emulation, where it provides a virtual model of an entire machine (CPU, memory and emulated devices) to run a guest OS. In this mode the CPU may be fully emulated, or it may work with a hypervisor such as KVM, Xen, Hax or Hypervisor.Framework to allow the guest to run directly on the host CPU.</p>
<p>The second supported way to use QEMU is User Mode Emulation, where QEMU can launch processes compiled for one CPU on another CPU. In this mode the CPU is always emulated.</p>
<p>In our project, we will use Qemu as a <a href="https://www.qemu.org/docs/master/system/target-riscv.html">Riscv System Emulator</a>. </p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><p>[undone]</p>
<div style="break-before: page; page-break-before: always;"></div><p>undone</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio"><a class="header" href="#virtio">VirtIO</a></h1>
<p><a href="https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html">Main reference paper</a>
<img src="./images/Virtio%20MMIO%20Register%20Layout%20part%201.png" alt="Virtio memory layout part 1" />
<img src="./images/Virtio%20MMIO%20register%20layout%20part%202.png" alt="Virtio memory layout part 2" />
<img src="./images/virtio%20device%20IDs.png" alt="Virtio device type IDs" />
<em><strong>terms</strong></em></p>
<p>This definitions are described under the context of OS development. They may mean entirely different things under other contexts such as web development or cloud development. Moreover, there is no way to categorize software perfectly using words... for example person A may call Linux a full-fledged OS while another may call it 'just an advanced kernel'. Potato-potato</p>
<ol>
<li><strong>A virtual machine</strong> - A virtual machine is a software program that emulates a computer system. A 'computer system' in this case means (OS + drivers + hardware)
For example when creating an insance of a VM, you will first just name the number of CPUs you want... or the number of network cards you want... and anstract instances of those devices will be created. 
Second after, you will need to install a real OS on top of that abstract hardware.</li>
</ol>
<p>A virtual machine typically runs on top of virtualized hardware. VMs can be made to run directly on top of bare-metal but this is undesirable.  For this reason, a VM must run on top of a virtualizer software such as a hypervisor.</p>
<p>Without the hypervisor or virtualization layer, a virtual machine would have direct access to the underlying physical hardware, just like any other application running on the bare metal. If you intend to run only one virtual machine then running on bare metal is possible. But if you intend to run multiple VMs then this would create a potential conflict for resource allocation and management, and could also create security risks, as different operating systems and applications could interfere with each other and potentially compromise the integrity of the entire system.</p>
<p><img src="./images/raw/virtual_machine.jpeg" alt="Virtual machine" /></p>
<ol>
<li><strong>A Hypervisor</strong> - a hypervisor is a piece of software that runs on top of hardware. A hypervisor's main goal is to abstract the underlying hardware.<br />
In the context of virtual machines : The hypervisor is a layer of software that sits between the virtual machine and the underlying hardware, and it provides a level of abstraction that allows multiple virtual machines to share and utilize the same physical hardware resources.</li>
</ol>
<p>So the hypervisor will abstract things like the CPU, network cards, Graphics, Harddisks etc.</p>
<h3 id="virtio-1"><a class="header" href="#virtio-1">VirtIO</a></h3>
<p>The VirtIO is a framework (standard set of communication protocols and interfaces).<br />
It is a framework that defines how the virtual machine interacts with the host environment. The Host environment can be a hypervisor, a virtualization software, another bare-metal OS or anything that virtalizes hardware.</p>
<p>It was created to address the need for fast and efficient I/O (input/output) between virtual machines and their hosts.<br />
Before Virtio came through, people used to abstract hardware in full (virtualize all components and all commands).<br />
For example they would virtualize the whole motherboard...even devices that are not in Use.<br />
They would replicate everything down to the registers of the CPU. But we are in a virtual environment... why bring all that detail to our simple virtual world? 
They would replicate the entire APIs to those abstracted devices. Meaning that the same unique low level protocols needed to communicate with those devices get brought to our simple peaceful virtual world.</p>
<p>As you can see, Before VirtIO, virtual machines used <strong>emulated</strong> devices to communicate with their hosts, which often led to slow and inefficient I/O operations. </p>
<p>But virtio allows you to only abstract necessary devices. 
Each device has a simplified interface that might not be identical to the interface defined by the physical hardware. Virtualize devices... not emulate them. 
This two qualities make it suitable for our peaceful virtual world</p>
<p>With Virtio being made standard, it made VMs to be platform agnostic.</p>
<p><em><strong>How does Virtio work?</strong></em></p>
<p>VirtIO defines a set of virtual devices that are <em>optimized for use in virtualized environments</em>. 
By optimized I mean that they have:</p>
<ul>
<li>
<p>Reduced overhead: VirtIO devices are implemented as software running in the host environment, so they can be designed to minimize overhead and reduce the amount of processing required in the virtual machine.</p>
</li>
<li>
<p>Simplified interfaces: VirtIO devices use standardized, simple interfaces that are designed to be easy to implement and use. This reduces the complexity of communication between the virtual machine and the host environment, which can improve performance.</p>
</li>
<li>
<p>Flexibility: VirtIO devices can be added or removed from a virtual machine as needed, without requiring any changes to the virtual machine itself. This provides flexibility and makes it easier to manage virtualized environments. </p>
</li>
</ul>
<p>These devices are designed to be lightweight and efficient, with a simplified interface that allows them to communicate directly with the hypervisor and other devices in the virtualized environment.</p>
<p>When a virtual machine is created, the hypervisor provides a set of these VirtIO devices to the virtual machine. These devices are presented to the virtual machine as if they were physical devices, but they are actually implemented as software running in the host environment.</p>
<p>The virtual machine communicates with the VirtIO devices through a set of standardized interfaces, which are implemented by the hypervisor. These interfaces are designed to be simple and efficient, allowing the virtual machine to communicate with the VirtIO devices without incurring significant overhead.</p>
<p>One of these virtio devices is the Block device.<br />
A block device is a storage device that returns data in blocks. It does not return data in a character stream</p>
<p><em><strong>Implementation</strong></em></p>
<p>The Virtio has 2 layers :</p>
<ol>
<li>The front-end layer  - deals with the communication between VM and virtio devices </li>
<li>The back-end layer   - deals with the communication between the virtio devices and physical hardware</li>
</ol>
<p>The VM communicates with the virtio device using MMIO programming. Under virtio bus, there are a bunch if registers</p>
<p>The device and guest share a segment of physcal memory. This segment is used for communication purposes. This is where the virtiqueue is found.<br />
To pass along a message, you store that message in the virt Queue and press the GO (write 1 to the Queue notifier register). The device reads the message, processes it and writes the responce in the virtiqueue.</p>
<p>The virt Queue is the main send/response structure.<br />
The Virtio virtual queue is a shared memory region used for communication between a Virtio device and its driver. It consists of three main components: the descriptor table, the available-ring, and the used-ring.</p>
<p>The descriptor table is an array of descriptors that describes the data buffers or commands that will be processed by the Virtio device. Each descriptor is a small data structure that contains information about a single data buffer or command. The descriptor typically includes a pointer to the data buffer, its length, and control flags.</p>
<p>The available-ring is another array of descriptors that are used by the driver to indicate which descriptors in the descriptor table are available for the Virtio device to process. Each entry in the available-ring corresponds to an entry in the descriptor table and contains a descriptor index and a set of flags.</p>
<p>When the driver wants to send data to the Virtio device, it updates one or more entries in the available-ring to indicate which descriptors are available for the device to process. The driver then notifies the device by writing to a special register in the Virtio device.</p>
<p>The Virtio device reads the available-ring to determine which descriptors are available for processing. It then processes the data buffers or commands described by the descriptors and writes completion descriptors to the used-ring.</p>
<p>The used-ring is the third component of the Virtio virtual queue. It is another array of descriptors that are used by the Virtio device to indicate which descriptors in the descriptor table it has completed processing. Each entry in the used-ring corresponds to an entry in the descriptor table and contains a descriptor index and a set of flags.</p>
<p>After the Virtio device has processed a data buffer or command, it writes a completion descriptor to the used-ring to indicate that the corresponding descriptor in the descriptor table has been processed. The driver reads the used-ring to determine which descriptors have been completed and can then free any associated resources.</p>
<p>Overall, the Virtio virtual queue provides a flexible and efficient way for a Virtio device and its driver to communicate with each other. By using a shared memory region, it avoids the overhead of copying data between different memory regions, as would be necessary if the device and driver communicated using I/O ports or other communication channels.</p>
<h6 id="the-queue-pfn-physical-frame-number"><a class="header" href="#the-queue-pfn-physical-frame-number">The Queue PFN (physical Frame Number)</a></h6>
<p>The Queue PFN stores the physical address of the virtual queue in memory.</p>
<p>The virtual queue itself is typically implemented as a data structure in the driver's memory, and the driver is responsible for mapping the virtual queue into physical memory so that the Virtio device can access it.<br />
When the driver initializes the Virtio device, it writes the physical address of the virtual queue into the QueuePFN field in the device's configuration space. This allows the Virtio device to access the virtual queue by reading and writing to the memory location specified by the QueuePFN field.</p>
<h6 id="the-queuenum-register"><a class="header" href="#the-queuenum-register">The QueueNum Register</a></h6>
<p>The QueueNum register is another field in the Virtio device's configuration space that is used to configure the Virtio virtual queue. It specifies the number of virtual queue pairs that the device and driver will use to communicate.</p>
<p>A virtual queue pair consists of an available-ring and a used-ring, as well as the associated descriptor table. Each virtual queue pair is used for a separate stream of communication between the Virtio device and its driver.</p>
<p>When the driver initializes the Virtio device, it writes the desired number of virtual queue pairs to the QueueNum register. The Virtio device then uses this value to determine the size of the descriptor table and the available-ring and used-ring arrays for each virtual queue pair.</p>
<h6 id="the-request-block"><a class="header" href="#the-request-block">The Request block</a></h6>
<p>The desriptors in the Vurtqueue point to the Request block.<br />
So the request Blocks are found in the Drivers's memory.<br />
They give information about the type of I/O operation the driver needs from the device.</p>
<p>The Request Block (also known as the &quot;virtqueue descriptor&quot; or simply &quot;descriptor&quot;) is a data structure used in Virtio block devices to describe a block I/O operation that the driver wants to perform on the device. It is typically implemented as a buffer in the driver's memory, and is used to pass information about the I/O operation from the driver to the Virtio block device.</p>
<p>The Request Block contains several fields that describe the I/O operation, including the operation type (e.g. read or write), the starting sector number, the length of the data to be transferred, and a set of flags that control various aspects of the I/O operation.</p>
<p>When the driver wants to perform a block I/O operation on the Virtio block device, it creates a Request Block in its memory and initializes the appropriate fields with the desired values. It then adds the Request Block to the appropriate Virtio virtual queue by appending it to the available-ring and notifying the device using the appropriate mechanism (e.g. an interrupt or memory-mapped I/O).</p>
<p>When the Virtio block device receives a Request Block from the driver, it reads the necessary fields from the Request Block to determine the details of the I/O operation being requested. It then performs the requested operation and updates the Request Block with the results of the operation (e.g. the number of bytes transferred). Finally, it appends the updated Request Block to the used-ring of the corresponding virtual queue pair to notify the driver that the operation has completed.</p>
<p>The request block is this struct  with  (request header), (Data Buffer) and (status buffer)
- request type (32bits) [IN(a read by driver), OUT(a write by driver), FLUSH, DISCARD, WRITE_ZEROES]
- reserved (32 bits)
- Request sector {
- (tells us from which block to begin to read from in memory. Sector is a fancy name for 'block'. Block size gets set durin initialization of the device)
- every sector has 512 bytes
- The databyte [sector][byte_number] ---&gt; The actual dat byte getting affected by this block I/O operation
- The Status ---&gt; This byte shows whether the virtio device succeeded or failed  or if the operation was unsupported.
- this byte gets written to by the virtio device (o == success, 1 == error, 2 == unsupported operation)
- at the beginning, make the driver write some arbitrary value such as x... that way we will know if the block device even did dare touch our block request </p>
<p>The request block will be represented by 3 descriptors.<br />
- Descriptor 1 (stored in the virtqueue descriptor array) - references the block request header. The &quot;next&quot; address points to the Descriptor 2 (that is stored in guest's memory instead of virtqueue)
- Descriptor 2 points to Descriptor 3 and references data Buffer
- Descriptor  3 references the status buffer of the block request</p>
<ul>
<li>All this in the name of modularity </li>
</ul>
<p><em><strong>In Qemu</strong></em>
For the QEMU emulator, it puts virtio devices (backwards) from 0x1000_1000 to 0x1000_8000. If we only have one device, it should be attached at 0x1000_8000</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="under-the-memory-management"><a class="header" href="#under-the-memory-management">Under the Memory Management</a></h3>
<ol>
<li>
<h4 id="error_m1-"><a class="header" href="#error_m1-">Error_M1 :</a></h4>
</li>
</ol>
<p>The allocation function was requested to allocate no Pages. This is like going to the shop and telling a shopkeeper to sell you nothing. There might be a problem with the calling function.</p>
<ol start="2">
<li>
<h4 id="error_m2"><a class="header" href="#error_m2">Error_M2</a></h4>
<ul>
<li>Reason for Error :The RAM has no contiguous free pages that are equal to the number of pages requested. </li>
<li>Possible causes  : 
<ul>
<li>The heap in the RAM has space... it is just that the available space is not contiguous. The available space is scatterd in fragmnets.</li>
<li>The RAM is fully occupied and there is no extra space.</li>
</ul>
</li>
<li>Possible solutions :
<ul>
<li>Defragment the heap</li>
<li>Add more RAM to the machine</li>
<li>Close other processes so that some space in the RAM can be freed</li>
<li></li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="error_m3"><a class="header" href="#error_m3">Error_M3</a></h4>
<ul>
<li>Reason for error : The program tried to de-allocate a null pointer. You cannot de-allocate nothing. </li>
<li>Possible causes  : The program written by the programmer tried to free an null pointer. Fix your code brah</li>
<li>Possible solutions : Fix your code... ha ha</li>
</ul>
<h4 id="error_m4"><a class="header" href="#error_m4">Error_M4</a></h4>
<ul>
<li>Error              : The program tried to access and deallocate an address that is not found within the heap section</li>
<li>Possible causes    : The address is not within the address ranges specified by the linker script</li>
<li>Posible solutions  : Find an address that is within the range : _heap_start and _heap_end. Moreover, the address should be within the segment of the heap where data_pages are stored.</li>
</ul>
<ol>
<li>Error_M5</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>You can check this out : <a href="https://docs.wasmtime.dev/examples-profiling.html">here</a> it is a way of measuring wasm app performance on Linux.<br />
This can act as a gate to other performance measurers</p>
<p>All the Best future James, I leave it in your capable hands</p>
<div style="break-before: page; page-break-before: always;"></div><p>One of the responsibilities of the linker is to resolve global symbols. What that means is that you can reference a global variable or global function that was used in a seperate crate or object file... and the linker will take care of validating and linking those references.</p>
<p>In our project, we had declared and initialized some memory positions in the linker script. We need to reference those memory position variables in our Rust code.</p>
<p>To do that, we make those variables global using the lds.s file... For example, to make _heap_size global, we do something like this :</p>
<pre><code class="language-asm">.global HEAP_SIZE
HEAP_SIZE: .dword _heap_size
</code></pre>
<p>Afterwards we import thos global variables into Rust using the <a href="https://doc.rust-lang.org/std/keyword.extern.html">'extern' keyword</a></p>
<p>Considering that Rust does not trust or know the implementations of variables and functions that have been borrowed from other languages, we are required to enclose the borrowed code in 'unsafe' blocks.</p>
<p>Unsafe variables are unreliable... so we enclose them in safe getter functions as shown below :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Constants recieved from the linker script
extern &quot;C&quot;
{
    static TEXT_START: usize;
    static TEXT_END: usize;
    static RODATA_START: usize;
    static RODATA_END: usize;
    static DATA_START: usize;
    static DATA_END: usize;
    static BSS_START: usize;
    static BSS_END: usize;
    static KERNEL_STACK_START: usize;
    static KERNEL_STACK_END: usize;
    static HEAP_START: usize;
    static HEAP_END: usize;
}

/// Get the text start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn text_start() -&gt; usize
{
	unsafe { TEXT_START }
}

/// Get the text end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn text_end() -&gt; usize
{
	unsafe { TEXT_END }
}

/// Get the rodata start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn rodata_start() -&gt; usize
{
	unsafe { RODATA_START }
}

/// Get the rodata end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn rodata_end() -&gt; usize
{
	unsafe { RODATA_END }
}

/// Get the data start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn data_start() -&gt; usize
{
	unsafe { DATA_START }
}

/// Get the data end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn data_end() -&gt; usize
{
	unsafe { DATA_END }
}

/// Get the bss start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn bss_start() -&gt; usize
{
	unsafe { BSS_START }
}

/// Get the bss end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn bss_end() -&gt; usize
{
	unsafe { BSS_END }
}

/// Get the stack start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn stack_start() -&gt; usize
{
	unsafe { KERNEL_STACK_START }
}

/// Get the stack end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn stack_end() -&gt; usize
{
	unsafe { KERNEL_STACK_END }
}

/// Get the heap start address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
pub fn heap_start() -&gt; usize
{
	unsafe { HEAP_START }
}

/// Get the heap end address as a usize
/// Safety: Because this value should have been read properly from the linker
/// script, this is safe
<span class="boring">pub fn heap_end() -&gt; usize
</span>{
	unsafe { HEAP_END }
<span class="boring">}
</span><span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><p>undone</p>
<div style="break-before: page; page-break-before: always;"></div><p>Software development is hard.<br />
When writing conmplex software that involves many modules interacting with each other... it becomes a good thing to create only one instance of a module that gets used by other modules.<br />
This reduces the complexity of having to manage many instances.</p>
<p>Two principles :</p>
<ol>
<li>Restrict the instantiation of a class to one object </li>
<li>Ensures that there is a global point of access to that object. </li>
</ol>
<p>One of the many ways to implement Principle 1 :</p>
<ul>
<li>implement the class as a module with a well defined API. Uder that API, the costructor should be private. Just provide an &quot;get_instance&quot; function that calls the constructor in the background. In the Singleton pattern, the class provides a static method (often called getInstance()) that returns a single instance of the class. The method checks if an instance of the class has already been created, and if so, it returns that instance. If not, it creates a new instance and returns it.</li>
</ul>
<p>Implementation In Rust :
preliminaries : </p>
<ul>
<li>understand the <a href="https://doc.rust-lang.org/beta/std/keyword.static.html">&quot;static&quot; keyword</a> in Rust</li>
<li>have some knowledge on <a href="https://doc.rust-lang.org/beta/std/sync/index.html">std::sync</a></li>
</ul>
<p>static keyword</p>
<ul>
<li>static variables are variables that last during the entire program. THey are defined in memory, meaning you can access them as global variables. </li>
<li>Now that static variable memory point can be accessed through out the program by any program code, it means that mutating that memory point may cause thread unsafety. So anytime we mutate that memory point we do it in an unsafe block.</li>
<li>Moreover, the value of the data made mutably static MUST implement the <a href="https://doc.rust-lang.org/beta/std/marker/trait.Sync.html">sync trait</a>(A trait to show that a type is safe to share references between threads).</li>
</ul>
<p>Under std::sync you can use the following objects... among many:<br />
Mutex: Mutual Exclusion mechanism, which ensures that at most one thread at a time is able to access some data.
Once: Used for a thread-safe, one-time global initialization routine
OnceLock: Used for thread-safe, one-time initialization of a global variable.</p>
<p>Example :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::{Mutex, Once};

pub struct UartDriver {
    // ... UART driver state goes here ...
}

impl UartDriver {
    // Private constructor to prevent other code from creating instances of UartDriver
    fn new() -&gt; UartDriver {
        UartDriver {
            // ... Initialize the UART driver state here ...
        }
    }

    // Static method that returns the singleton instance of UartDriver
    pub fn get_instance() -&gt; &amp;'static Mutex&lt;UartDriver&gt; {
        static mut INSTANCE: *const Mutex&lt;UartDriver&gt; = 0 as *const Mutex&lt;UartDriver&gt;;
        static ONCE: Once = Once::new();

        unsafe {
            ONCE.call_once(|| {
                // Create the singleton instance of UartDriver using a Mutex to allow safe concurrency
                let mutex = Mutex::new(UartDriver::new());
                INSTANCE = std::mem::transmute(Box::new(mutex));
            });

            &amp;*INSTANCE
        }
    }

    // ... Methods for interacting with the UART go here ...
}
<span class="boring">}</span></code></pre></pre>
<h5 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h5>
<p>Source : <a href="https://refactoring.guru/design-patterns/singleton">this blog</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multitasking"><a class="header" href="#multitasking">Multitasking</a></h1>
<p>main reference material : <a href="https://os.phil-opp.com/async-await/">Philip Opperman's blog</a></p>
<p>Programs share the CPU. There is an illusion that programs are executing in parallel. This illusion is achieved through multitasking.<br />
There are various multitasking techniques :
- Preemptive multitasking
- Time sliced mutltitasking
- Co-operative multitasking</p>
<p><strong>Preemptive multitasking</strong> - The kernel says : &quot;Program A, you have used the CPU enough... I am cutting you off from the CPU... I will make program B have access to the CPU&quot;.</p>
<p><strong>Time sliced Multitasking</strong> - Each process gets to use the CPU for a specified interval of time. The scheduler might to his priority sorting but fact remains that a process switch occurs after a specified time irregardless.</p>
<p><strong>Co-operative multitasking</strong> - The CPU and the kernel don't decide the amount of time a program gets to use the CPU. THe program itself decides when to stop using the CPU and share it with others. It is expected that the programs are not selfish and that they care about each other... they co-operate with each other. Sounds like a fantasy.</p>
<p>Cooperative multitasking was commonly used in early operating systems because it was simpler to implement and required less overhead than preemptive multitasking, which requires the kernel to forcibly take control of the CPU from running tasks. However, as computer systems became more complex and demanding, preemptive multitasking became the norm because it offers better control over system resources and can improve system responsiveness.</p>
<p>That being said, cooperative multitasking can still be useful in certain situations, such as when dealing with low-level embedded systems or real-time applications, where predictability and determinism are more important than system performance. In these cases, cooperative multitasking can provide a simpler and more reliable way to manage system resources.</p>
<p>The Past is the future.... haha</p>
<p>A <strong>coroutine</strong> is a special kind of function that can pause its execution, save its current state, and resume execution from where it left off later, without losing its context or stack.</p>
<p>Cooperative multitasking is often used at the language level, like in the form of coroutines or async/await. The idea is that either the programmer or the compiler inserts yield operations into the program, which give up control of the CPU and allow other tasks to run. For example, a yield could be inserted after each iteration of a complex loop.</p>
<p>So we can implement asynchronous operations using co-operative multitasking. When a process needs to call an I/O operation that may take some time... we make it yield control to the CPU after calling that expensive I/O operation.</p>
<p>Since the programs themselves define their pause points, they are the one that know which information is important to save. it is up to the program to save its state status. This results in better performance. For example, a task that just finished a complex computation might only need to backup the final result of the computation since it does not need the intermediate results anymore.</p>
<p>For example, the Rust Async/Wait struct automatically creates a struct that stores all variables that are still needed and the call stack values. Meaning you do not have to program the kernel to back up stack data. By backing up the relevant parts of the call stack before pausing, all tasks can share a single call stack, which results in much lower memory consumption per task.</p>
<p>The thing about preemptive multitasking is that each task has its own stack. But in co-operative multitasking, all tasks can share one stack, which results in much lower memory consumption per task. This makes it possible to create an almost arbitrary number of cooperative tasks without running out of memory.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bitmasking_and_bit_operations"><a class="header" href="#bitmasking_and_bit_operations">Bitmasking_and_bit_operations</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compressed-instructions"><a class="header" href="#compressed-instructions">Compressed Instructions</a></h1>
<div style="break-before: page; page-break-before: always;"></div><p>I don't understand what the ABI is. Future James, save us.</p>
<p>So right now I just got this from Chatgpt... but I don't understand it : 
&quot;
ABI stands for Application Binary Interface. It is a set of rules that define how software interacts with the operating system and hardware at the binary level. An ABI specifies things like data type sizes, calling conventions, register usage, and system call numbers, among other things. By providing a standard interface between software components, an ABI enables software written in different programming languages, compiled by different compilers, and running on different hardware and operating systems to interoperate.
&quot;</p>
<p>For example the RISCV ABI specifies how system calls should be made : all that ... load syscall into a7, store arguments into a0-a5.<br />
Now if your OS uses this RISCV syscall calling convention, you could say that your OS uses the RISCV ABI. You cannot directly use the RISCV ABI  on a x86 machine. The RISC-V ABI is specific to the RISC-V architecture and specifies things like register usage, stack layout, and calling conventions that are specific to that architecture. Probably x86 does not have mepc and mret instruction mighrt not work as the RISCV assembly instruction.</p>
<p>If you were to run an operating system or program that was compiled for the RISC-V architecture on an x86 CPU, you would need an emulator or virtual machine that can simulate the RISC-V instruction set and system calls. However, the resulting binary would not conform to the x86 ABI, which specifies a different set of rules for how programs interact with the system on x86 CPUs.</p>
<p>What is the Linux-ABI
Can we have a custom API that combines aspects of Linux ABI and RISCV  </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="elf-files"><a class="header" href="#elf-files">Elf Files</a></h1>
<p>Reference Sources :
- The ELF file Specifications : (from <a href="https://refspecs.linuxfoundation.org/elf/elf.pdf">Linux Foundation</a>) (From <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15213-f00/docs/elf.pdf">CMU.edu</a>)
- https://wiki.osdev.org/ELF
- <a href="https://linuxhint.com/understanding_elf_file_format/">Tools in inspecting Elf files</a></p>
<h3 id="the-elf-file-format"><a class="header" href="#the-elf-file-format">The ELF file format</a></h3>
<p>There are many file format standards. Humanity needed to agree on how certain important bytes get arranged. In this case, humaniy agreed on how to arrange bytes of an executable binary or a library object file or a core dump. So it was called the ELF file format. There are other formats.... but those are none of our concern, we are purists.</p>
<pre><code>A relocatable object file holds sections containing code and data. This file is suitable to be linked with other relocatable object files to create dynamic executable files, shared object files, or another relocatable object.

A dynamic executable file holds a program that is ready to execute. The file specifies how exec(2) creates a program's process image. This file is typically bound to shared object files at runtime to create a process image.

A shared object file holds code and data that is suitable for additional linking. The link-editor can process this file with other relocatable object files and shared object files to create other object files. The runtime linker combines this file with a dynamic executable file and other shared object files to create a process image.
</code></pre>
<p>ELF’s <em>design</em> is not limited to a specific processor, instruction set, or hardware architecture.</p>
<p>The reason we agreed on a standard is mainly for the sake of simplicity.<br />
You can write software that loads programs to the CPU without knowing which programs will finally run on that CPU. As long as those programs follow the ELF format.<br />
You can write software that links executables (a linker) or Debuggers (software that inspects executables) more easily... because you can count on the fact that executable programs will always follow a certain format.</p>
<p>Developers no longer have to read many documentations about how bytes in a file are organized... they just have to read one file format... the elf file. Life is good.</p>
<p>With the ELF format, programmers can just compile their source code for a specific architecture (eg Riscv) and expect that the program will have no problem running on any RISCV implementation.</p>
<p>The specification does not clarify the filename extension for ELF files. In use is a variety of letter combinations, such as .axf, .bin, .elf, .o, .prx, .puff, .ko, .so, and .mod, or none.  Some file systems don't give a hoot about the extension... a file is just a bunch of bytes... attaching meaning to a file is overengineering ... right?</p>
<h5 id="so-what-is-the-format"><a class="header" href="#so-what-is-the-format">So What is the format?</a></h5>
<p>You can read the specifications of the format in the official specifications : The ELF file Specifications : (from <a href="https://refspecs.linuxfoundation.org/elf/elf.pdf">Linux Foundation</a>) (From <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15213-f00/docs/elf.pdf">CMU.edu</a>)</p>
<p>You can get a summary of the specification by running the following command on a linux machine terminal : </p>
<pre><code class="language-bash">man elf
</code></pre>
<p>Yea that's right, linux is not an operating system... it is a development software.</p>
<p>An executable file using the ELF file format consists of an ELF header,
followed by a program header table or a section header table, or  both.
The  ELF  header  is  always  at  offset zero of the file.  The program
header table and the section header table's  offset  in  the  file  are
defined  in  the  ELF  header.  The two tables describe the rest of the
particularities of the file</p>
<h6 id="tools-used-to-handle-the-elf-file"><a class="header" href="#tools-used-to-handle-the-elf-file">Tools used to handle the ELF file.</a></h6>
<p>Now, the elf file is in binary, it is full of zeros and ones... quite unreadable.<br />
So people from the past came up with tools to inspect the ELF file in a slightly readable way.</p>
<p>***1. The Hexdump  ***
One way was to convert each byte in the binary file into their hexadecimal equivalent. A byte gets represented by 2 hex digits.  Printable Characters are then represented in their equivalent ascii values... but in hex.<br />
This tool is called a <strong>Hexdump program</strong>.  Linux comes with an inbuilt hexdump which you can access through the command line as follows :</p>
<pre><code class="language-bash">hexdump -C input_file.txt    // the file can be any extension
man hexdump                  // this command will help you get hexdumps documentation
</code></pre>
<p>The -C flag (capital C, not small c) means that the output should be in Canonical hex+ASCII display. This information does not matter now. just cram it.</p>
<p><em><strong>2. THe Readelf</strong></em><br />
But the Hexdump was not enough... who understands Hex... I know I don't.<br />
Ro readelf was made, with readelf you can get different sections in more human terms.</p>
<pre><code class="language-bash">readelf -h hello  // show Elf header
readelf -l hello  // show program header table and Segment-to-section mapping
</code></pre>
<h5 id="format"><a class="header" href="#format">Format</a></h5>
<p>The Elf file has 2 main parts :
- The ELF Header (description if the file data)
- The file data (data being described by elf header) </p>
<p>The File Data part contains :
- The program header table : describes zero or more <strong>segments</strong>
- The section header table : describe zero or more <em>sections</em>
- Data                     : contains the actual data referred to by the program header table and section header table.</p>
<p><em>What is the difference between <strong>segments</strong> and <strong>sections</strong>?</em><br />
A Section is an individual unit of either data or code. And it is given a unique name because it serves a unique purpose.<br />
For example, we have may Code section, data section, bss section, symbol table section.</p>
<pre><code>On the other hand segments are units that house sections. The segments describe the memory addresses that the runtime will use to load the sections within it into memory. So sections are found within segments.

To summarize, sections are the individual units of code and data within an ELF file, while segments are groups of sections that are combined together for the purpose of being loaded into memory at runtime.
</code></pre>
<p>Here is a diagram presentation of the ELF File Format :<br />
<img src="./images/not_raw/elf_format.webp" alt="" /></p>
<h6 id="the-elf-header"><a class="header" href="#the-elf-header">The Elf Header</a></h6>
<p>This part is 32 bytes long... ALWAYS.<br />
It starts at address 0. Its first 4 bytes (32 bits) contain the magic Number. The magic number is used to identify th file format. In our case the 4 bytes spell out 'ELF' ie 0x7F followed by 0x45, 0x4c, and 0x46.</p>
<p>Here is a sample elf header :</p>
<pre><code class="language-bash">readelf -h hello  // show Elf header
readelf -l hello  // show program header table and Segment-to-section mapping
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              DYN (Shared object file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  Entry point address:               0x87f0
  Start of program headers:          64 (bytes into file)
  Start of section headers:          4267608 (bytes into file)
  Flags:                             0x0
  Size of this header:               64 (bytes)
  Size of program headers:           56 (bytes)
  Number of program headers:         12
  Size of section headers:           64 (bytes)
  Number of section headers:         42
  Section header string table index: 41

</code></pre>
<p>Here is a more detailed Header Layout and Possible Values (extracted from : <a href="https://wiki.osdev.org/ELF">OSDev Wiki</a>)<br />
<img src="./images/elf/ELF%20Header%20Values.png" alt="" />
<img src="./images/elf/ELF%20header%20ISA%20values%20.png" alt="" /></p>
<h6 id="the-program-header-table"><a class="header" href="#the-program-header-table">The Program Header Table</a></h6>
<p>The Program Header Table, also known as the &quot;segment header table,&quot; is a section of an ELF (Executable and Linkable Format) file that provides information about the program's memory layout at runtime.</p>
<p>The Program Header Table defines the layout of the program in memory, by specifying the segments that make up the program and the attributes of each segment. A segment is a part of the program's memory layout that has a specific type and purpose.</p>
<p>The Program Header Table contains entries for each segment in the program, and each entry specifies the following information:</p>
<pre><code>- Segment type: describes the type and purpose of the segment, such as code, data, stack, or dynamic linking information.
- Segment offset: specifies the offset of the segment within the file.
- Virtual address: specifies the virtual address of the segment in memory.
- Physical address: specifies the physical address of the segment in memory (if applicable).
- Segment size: specifies the size of the segment in bytes.
- Flags: specifies various flags that control the behavior of the segment, such as read/write/execute permissions.
</code></pre>
<p>The information in the Program Header Table is used by the operating system to load the program into memory, by mapping the segments specified in the table to their corresponding memory locations. This allows the program to execute correctly, with the proper memory layout and permissions. </p>
<pre><code class="language-bash">readelf -l hello

Elf file type is DYN (Shared object file)
Entry point 0x87f0
There are 12 program headers, starting at offset 64

Program Headers:
  Type           Offset             VirtAddr           PhysAddr
                 FileSiz            MemSiz              Flags  Align
  PHDR           0x0000000000000040 0x0000000000000040 0x0000000000000040
                 0x00000000000002a0 0x00000000000002a0  R      0x8
  INTERP         0x00000000000002e0 0x00000000000002e0 0x00000000000002e0
                 0x000000000000001c 0x000000000000001c  R      0x1
      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]
  LOAD           0x0000000000000000 0x0000000000000000 0x0000000000000000
                 0x00000000000052b0 0x00000000000052b0  R      0x1000
  LOAD           0x0000000000006000 0x0000000000006000 0x0000000000006000
                 0x00000000000382a1 0x00000000000382a1  R E    0x1000
  LOAD           0x000000000003f000 0x000000000003f000 0x000000000003f000
                 0x000000000000cdd0 0x000000000000cdd0  R      0x1000
  LOAD           0x000000000004c270 0x000000000004d270 0x000000000004d270
                 0x0000000000002dc0 0x0000000000002ed0  RW     0x1000
  DYNAMIC        0x000000000004e6f8 0x000000000004f6f8 0x000000000004f6f8
                 0x0000000000000230 0x0000000000000230  RW     0x8
  NOTE           0x00000000000002fc 0x00000000000002fc 0x00000000000002fc
                 0x0000000000000044 0x0000000000000044  R      0x4
  TLS            0x000000000004c270 0x000000000004d270 0x000000000004d270
                 0x0000000000000028 0x0000000000000050  R      0x8
  GNU_EH_FRAME   0x0000000000043d9c 0x0000000000043d9c 0x0000000000043d9c
                 0x00000000000010c4 0x00000000000010c4  R      0x4
  GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000
                 0x0000000000000000 0x0000000000000000  RW     0x10
  GNU_RELRO      0x000000000004c270 0x000000000004d270 0x000000000004d270
                 0x0000000000002d90 0x0000000000002d90  R      0x1

 Section to Segment mapping:
  Segment Sections...
   00     
   01     .interp 
   02     .interp .note.gnu.build-id .note.ABI-tag .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt 
   03     .init .plt .plt.got .text .fini 
   04     .rodata .debug_gdb_scripts .eh_frame_hdr .eh_frame .gcc_except_table 
   05     .tdata .init_array .fini_array .data.rel.ro .dynamic .got .data .bss 
   06     .dynamic 
   07     .note.gnu.build-id .note.ABI-tag 
   08     .tdata .tbss 
   09     .eh_frame_hdr 
   10     
   11     .tdata .init_array .fini_array .data.rel.ro .dynamic .got 

</code></pre>
<p>The illustration above is an example. Below this sentence is a more general layout description of the Section Header Table :<br />
<img src="./images/elf/Program%20Header%20Table%20Layout%20and%20values.png" alt="" /></p>
<h6 id="the-section-header-table"><a class="header" href="#the-section-header-table">THe Section Header Table</a></h6>
<p>The Section Header Table, also known as the &quot;section header string table,&quot; is a section of an ELF (Executable and Linkable Format) file that provides information about the various sections within the file.</p>
<p>The Section Header Table contains entries for each section in the ELF file, and each entry specifies the following information:</p>
<pre><code>Section name: specifies the name of the section.
Section type: describes the type and purpose of the section, such as code, data, symbol table, or relocation information.
Section flags: specifies various attributes of the section, such as read/write/execute permissions, whether the section should be loaded into memory at runtime, and whether it contains relocation information.
Section virtual address: specifies the virtual address of the section in memory.
Section file offset: specifies the offset of the section within the file.
Section size: specifies the size of the section in bytes.
Link: specifies the section index of the associated section (if applicable).
Info: provides additional information about the section, depending on its type.
</code></pre>
<p>The Section Header Table is used by the linker to combine object files into a single executable or shared object file. The linker uses the information in the table to resolve symbol references between the different sections in the file, and to determine the appropriate memory layout of the sections at runtime.</p>
<p>At runtime, the operating system uses the information in the Section Header Table to map the sections of the program into memory, based on their virtual addresses and sizes. This allows the program to execute correctly, with the proper memory layout and permissions.</p>
<p>Common Sections include :</p>
<ul>
<li><strong>Code Section</strong> - this section contains the CPU instructions</li>
<li><strong>The Data section</strong> - This section contains global initialized variables.</li>
<li><strong>The BSS Section</strong> - This section contains global but uninitialized variables.</li>
<li><strong>The rodata Section</strong> - This section contains read only global data. These are constant values like string literals. They occupy literal memory space.</li>
<li><strong>LOAD sections</strong> - load sections are either code or data units that need to be loaded into memory at runtime. Not all elf sections get loaded into memory at runtime. Sections like debugging section and symbols section do not get loaded in memory at runtime.</li>
<li><strong>The INTERP section</strong> - the interp section contains a null-terminated string that specifies the path to the dynamic linker program that should be used to resolve shared library dependencies at runtime. The dynamic linker is typically located in a standard system location, such as /lib or /lib64, rather than being specified by the INTERP section.</li>
</ul>
<p>We need a dynamic linker to help link our executable with shared libraries during runtime. A shared library is a precompiled code module that is a dependency to multiple executables. for example lib.c module. If multiple modules depend on lib.c, we could have included lib.c code inside the elf file of hello_world... but this would bloat hello_world...it would consume too much space. So people came up with shared libraries... shared libraries get loaded in memory once. And executables that depend on it can just reference it instead of redefining it in their code.</p>
<p>This preserves space.</p>
<h3 id="implementing-user-processes"><a class="header" href="#implementing-user-processes">Implementing User Processes</a></h3>
<ul>
<li>The ELF header contains a field called &quot;entry_point&quot;. This is a virtual address that indicates the first appropriate instruction for the whole executable. This is where the Program counter should point to at the beginning. The Entry_point is where the _start fuction is found.</li>
<li>The Design of a very simple userprogram is as follows :  _start calls main and when main retuns it calls the exit system call. It is up to us to code the exit system call. The RISCV hardware can only take us as far us providing the RISCV System call Convention. We place the exit syscall ID_code in the a7 register... and hopefully we should have defined the exit call function in the kernel code(in some fancy table or Match statement).</li>
<li>The exit system call removes the process from the schedule list and then it frees all of the resources related to the ending process</li>
</ul>
<p>Just like how all problems in Computer Science get solved, we first abstract the ELF file format using code... specifically the ELF header for now :</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// this is a 64byte struct
#[repr(C)]
pub struct Header {
    pub magic: u32,   // a 32 bit identifier. The value inside this spells 'ELF' in ASCII. It confirms that this file follows the ELF file Format  : Value (0x7f454c46)
    pub bitsize: u8,  // this byte specifies the size of the target platform data model : 1 == 32bit machine, 2 == 64bit machine
    pub endian: u8,   // this byte indicates whether the target platform byte-order uses little endian or Big-endian ... shows if 1st complement or 2nd complement: (1 == little_endian, 2 == big_endian)
    pub ident_abi_version: u8,  // this is the ELF header ABI version of the ELF file format.... down below we have the ELF version. THis just specifies the ELF header version 
    pub target_platform: u8,    // (OS/ABI) This specifies the target platform for the elf file ie. OS and ABI
    pub abi_version: u8,        // Now that you have stated the ABI above...you need to specify the version of that ABI 
    pub padding: [u8; 7],       //  A 7-byte field that is reserved for future use and is typically set to all zeroes
    pub obj_type: u16,          // A 16-bit field that identifies the type of object file (e.g., executable, shared object, object file). Values : relocatable = 1, executable = 2, shared = 3, Core = 4
    pub machine: u16,           // This is the underlying machine ISA --&gt; 0xf3 for RISC-V 
    pub version: u32,       // Version of the ELF file  
    pub entry_addr: usize,  // Virtual memory address of the  _start function
    pub phoff: usize,   // program header table offset
    pub shoff: usize,   // Section header table offset
    pub flags: u32,     // : A 32-bit field that contains processor-specific flags associated with the file. for example ;
                        //     EF_RISCV_RVC: This flag indicates that the file contains RISC-V compressed instructions.
                        // EF_RISCV_FLOAT_ABI: This flag specifies the floating-point ABI used by the file. It can have one of the following values:
                        //     0x0: No floating-point instructions used.
                        //     0x1: The file uses the single-precision floating-point ABI.
                        //     0x2: The file uses the double-precision floating-point ABI.
                        //     0x3: The file uses the quad-precision floating-point ABI.
                        // EF_RISCV_GNU_PROPERTY: This flag indicates that the file uses GNU-specific properties.
    pub ehsize: u16,    // Elf Header Size (64 bytes?)
    pub phentsize: u16, // Program header Table entry size - Shows the size of a single entry in the Program header Table
    pub phnum: u16,     // Specifies the number of entries in the Program Header Table
    pub shentsize: u16, // Section Header Table entry Size - Specifies the size of a single entry in the section table entry
    pub shnum: u16,     // Specifies the number of entries in the Section Header Table
    pub shstrndx: u16,  // specifies the index of the section header table entry that contains the name of the section string table. The section string table contains the names of all sections in the elf file, and its index is specified in the e_shstrndx field of the ELF header.

                        // By storing the index of the section header table entry that contains the section string table name,
                        // the loader can easily locate the section string table and use it to find the names of other sections in the file. 
                        // This information is important for loading and linking the program, as it helps the loader to
                        // understand the structure of the file and where to find different sections in memory.
                        
}



// The program headers have the following structure in Rust
// This is for the 64 bit elf file. NOT the 32 bit elf file
#[repr(C)]
pub struct ProgramHeader {
    pub seg_type: u32,  // Show the type of segment. It can be (Null --&gt; 0, Load --&gt; 1, Dynamic --&gt; 2, Interp --&gt; 3, Note --&gt; 4)
    pub flags: u32,     // Tell us the access permisions of the segment (Execute = 1, Read = 2,  Write = 4)
    pub off: usize,     // the offset in the file that the data found under this segment can be found
    pub vaddr: usize,   // The virtual memory address start of the segment 
    pub paddr: usize,   
    pub filesz: usize,  // memory size of the segment in the raw elf file (data + padding)
    pub memsz: usize,   // memory size that the segment occupies when loaded into RAM.  MemSize != fileSize . Explanations below
    pub align: usize,
}

<span class="boring">}</span></code></pre></pre>
<p>Explanations for including filesize and memorysize (Extracted from Chatgpt... hopefully future James has wrapped his head around this... but for now I don't get it)  : 
&quot;The &quot;size of segment in file&quot; and &quot;size of segment in memory&quot; fields in the program header of an ELF file specify the size of a segment in the file and in memory, respectively.</p>
<p>The &quot;size of segment in file&quot; field specifies the size of the segment in the ELF file. This is the amount of disk space that the segment occupies in the file, and it includes any padding or alignment that may be added to ensure that the segment is loaded into memory at a properly aligned address.</p>
<p>The &quot;size of segment in memory&quot; field specifies the size of the segment in memory when it is loaded into RAM. This is the amount of memory that the segment will occupy when it is loaded into memory, and it may be larger than the size of the segment in the file if the segment contains uninitialized data that is initialized to zero at runtime.</p>
<p>The reason for including both of these fields is that the segment may be loaded into memory at a different address than it occupies in the file. This can happen if the program requires relocation to a different address, or if the operating system uses address space layout randomization (ASLR) to randomize the memory addresses of loaded segments for security reasons.</p>
<p>By specifying both the &quot;size of segment in file&quot; and &quot;size of segment in memory&quot;, the program loader knows how much disk space to allocate for the segment in the file and how much memory to allocate for the segment in RAM. This information is important for ensuring that the program can be loaded and executed correctly, even if the memory addresses used at runtime are different from those used in the file.&quot;</p>
<h5 id="loading-an-elf-file-into-memory"><a class="header" href="#loading-an-elf-file-into-memory">Loading An Elf file into memory</a></h5>
<p>References sources: </p>
<ul>
<li>Section 2-7 of the Official ELF specification</li>
<li></li>
</ul>
<p>Loading the Elf file means ... reading the elf file contents in the hard disk and storing a copy of the different sections in the RAM. That way, the CPU program Counter can point to the code of the program. It is up to the loading process to map the sections in the RAM just as how the virtual addresses found in the ELF file specified.</p>
<p>So high_level tasks in loading become : 
- Read and understand the layout of the elf file. (read the header, program_header_table and section_header_table)
- Create memory space in the RAM (the process memory).
- Read the contents of the ELF file and store them in some buffer.
- Arrange the elf data appropriately as specified in the elf layout
- Execute the program by making the program counter of the CPU to point to the entrypoint address.</p>
<p>A more precise guide for loading the ELF file from the OSDev Wiki : </p>
<ol>
<li>Verify that the file starts with the ELF magic number (4 bytes)</li>
<li>Read the ELF Header. The ELF header is always located at the very beginning of an ELF file. The ELF header contains information about how the rest of the file is laid out. An executable loader is only concerned with the program headers. Not the Section header table or the sections or data.</li>
<li>Parse the Program header Table entries. Only headers with a type of PT_LOAD describe a loadable segment. </li>
<li>Load each of the loadable segments. This is performed as follows: 
<ul>
<li>Allocate memory for each loadable segemnt ; the starting address for each segment being at ProgramHeader::vaddr and the size of the segment being ProgramHeader::memsize.</li>
<li>Copy the segment data from the file offset specified by the p_offset member to the virtual memory address specified by the p_vaddr member. The size of the segment in the file is contained in the p_filesz member. This can be zero. </li>
<li>If the segment filesize is smaller than segemnt memory size. Then you should padd the memory remaining (memsz - filesize) with zeroes.</li>
</ul>
</li>
<li>Read the executable's entry point from the elf header.</li>
<li>Make the CPU's Program counter point to the address of the executable's entry point</li>
<li>Set up process structure... stack and all</li>
<li>Add the process to the scheduler's radar</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="riscv_registers"><a class="header" href="#riscv_registers">Riscv_registers</a></h1>
<p>References :</p>
<ul>
<li><a href="https://shakti.org.in/docs/risc-v-asm-manual.pdf">Riscv manual - by SHakti deelopment Team</a></li>
</ul>
<p>Types of registers : 
- control and status registers
- General purpose registers</p>
<p>extract : &quot;RISC-V is divided into different categories based
on the maximum width of registers the architecture can support, for example, RV32 (RISC-V 32)
provides registers whose maximum width is 32-bits and RV64 (RISC-V 64) provides registers whose
maximum width is 64-bits. Processors with larger register widths can support instructions and data
of smaller widths. So an RV64 platform supports both RV32 and RV64&quot;</p>
<p>RISCV has 3 privilege levels : User, Machine and Supervisor level.<br />
Each level has its own set of control status registers for manipulation and observation of the CPU, meaning each level has its own way of controlling the CPU.</p>
<p>RISCV provides 31 read-write general purpose registers. (x1 upto x31)<br />
x0 is a read-only register that has the value zero hardwired to it. You can use it to set other registers to zero.</p>
<p>However, there is a RISCV ABI convention. Even if the registers are general purpose, they are typicallly used as follows ;</p>
<p>x1 == the return address</p>
<p><em>x2</em> - used as a stack pointer; it holds the base address of the stack. In addition, stack base address must aligned to 4 bytes. Failing which, a load/store alignment fault may arise. 
The x2 register can hold an operand in the following ways:</p>
<ol>
<li>As a base register for load and store instruction. In this case, the load/store address must be 4 byte aligned.</li>
<li>As a source or destination register for arithmetic/logical/csr instructions.</li>
</ol>
<p><em>x3</em> - is the register that will hold the global pointer value (the base address of the global data section)</p>
<p><em>x4</em> = THe Thread Pointer Register : Each thread has its own variables. These variables get stored contiguously in memory. This register points to the base register of this contiguous memory.</p>
<p><em>x10 to x17</em> - these are argument registers, when a function is called, the arguments to the functionare copied to these 8 registers. The stack is used in case the number exceeds 8.</p>
<p>General Registers and their RISCV ABI usage :<br />
<img src="./images/RISCV/General_registers_when_using_ABI.png" alt="" /></p>
<p>Control Status Registers while in Machine Mode
<img src="./images/RISCV/machine_mode_CSRs.png" alt="" /></p>
<h3 id="csrs"><a class="header" href="#csrs">CSRs</a></h3>
<p>Each CSR has a special name and is assigned a unique function.<br />
Reading and/or writing to a CSR will affect processor operation.<br />
The CSR cannot be read/written the way a general register can. A special set of instructions called csr instructions are used to facilitate this process.</p>
<p>An attempt to access a CSR that is not visible in the current mode of operation results in privilege violation. Similarly, in the current mode of operation, a privilege violation occurs when an attempt is made to write to a “read-only” labeled CSR. This attempt results in an illegal instruction exception. In addition to restrictions on how a CSR register is accessed, fields within some registers come with their own restrictions which are as listed as follows.</p>
<pre><code>- *Reserved Writes Ignored, Reads Ignore Values (WIRI)* --&gt; a read or write to this field will be ignored. In case the entire CSR is a read-only register, an attempt to write to the WIRI field will raise an illegal instruction exception.
- Write/Read Only Legal Values (WLRL)  ---&gt; You can only write legal values, you can only read legal values : Some fields only accept specific values, they are like an enum.
- Reserved Writes Preserve Values, Reads Ignore Values (WPRI) - some fields are not available for now, they are preserved for the future. A write to such a field does nothing to the vqalue that was originally there. A read of the field is ignored
- Write Any Values, Reads Legal Values (WARL) - You can write any value to this field, but you can only read kegal values. If there is an illegal value inside, the read will return a legal value that symbolizes that there was an illegal dat in the field
</code></pre>
<h4 id="csr-instructions"><a class="header" href="#csr-instructions">CSR instructions</a></h4>
<ul>
<li>CSR Read and Clear Bits (CSRRC) is used to clear a CSR.</li>
</ul>
<p>undone</p>
<h4 id="machine-register-fields"><a class="header" href="#machine-register-fields">Machine Register fields</a></h4>
<p>undone</p>
<p>PMP protection
mideleg ?
sfence</p>
<p>10.3 Machine mode exception handling</p>
<div style="break-before: page; page-break-before: always;"></div><p>/*</p>
<ul>
<li>QEMU RISC-V VirtIO machine interface</li>
<li></li>
<li>Copyright (c) 2017 SiFive, Inc.</li>
<li></li>
<li>This program is free software; you can redistribute it and/or modify it</li>
<li>under the terms and conditions of the GNU General Public License,</li>
<li>version 2 or later, as published by the Free Software Foundation.</li>
<li></li>
<li>This program is distributed in the hope it will be useful, but WITHOUT</li>
<li>ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or</li>
<li>FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for</li>
<li>more details.</li>
<li></li>
<li>You should have received a copy of the GNU General Public License along with</li>
<li>this program.  If not, see <a href="http://www.gnu.org/licenses/">http://www.gnu.org/licenses/</a>.
*/</li>
</ul>
<p>// Source : https://github.com/qemu/qemu/blob/master/include/hw/riscv/virt.h</p>
<p>#ifndef HW_RISCV_VIRT_H
#define HW_RISCV_VIRT_H</p>
<p>#include &quot;hw/boards.h&quot;
#include &quot;hw/riscv/riscv_hart.h&quot;
#include &quot;hw/sysbus.h&quot;
#include &quot;hw/block/flash.h&quot;</p>
<p>#define VIRT_CPUS_MAX_BITS             9
#define VIRT_CPUS_MAX                  (1 &lt;&lt; VIRT_CPUS_MAX_BITS)
#define VIRT_SOCKETS_MAX_BITS          2
#define VIRT_SOCKETS_MAX               (1 &lt;&lt; VIRT_SOCKETS_MAX_BITS)</p>
<p>#define TYPE_RISCV_VIRT_MACHINE MACHINE_TYPE_NAME(&quot;virt&quot;)
typedef struct RISCVVirtState RISCVVirtState;
DECLARE_INSTANCE_CHECKER(RISCVVirtState, RISCV_VIRT_MACHINE,
TYPE_RISCV_VIRT_MACHINE)</p>
<p>typedef enum RISCVVirtAIAType {
VIRT_AIA_TYPE_NONE = 0,
VIRT_AIA_TYPE_APLIC,
VIRT_AIA_TYPE_APLIC_IMSIC,
} RISCVVirtAIAType;</p>
<p>struct RISCVVirtState {
/<em>&lt; private &gt;</em>/
MachineState parent;</p>
<pre><code>/*&lt; public &gt;*/
Notifier machine_done;
DeviceState *platform_bus_dev;
RISCVHartArrayState soc[VIRT_SOCKETS_MAX];
DeviceState *irqchip[VIRT_SOCKETS_MAX];
PFlashCFI01 *flash[2];
FWCfgState *fw_cfg;

int fdt_size;
bool have_aclint;
RISCVVirtAIAType aia_type;
int aia_guests;
char *oem_id;
char *oem_table_id;
OnOffAuto acpi;
const MemMapEntry *memmap;
</code></pre>
<p>};</p>
<p>enum {
VIRT_DEBUG,
VIRT_MROM,
VIRT_TEST,
VIRT_RTC,
VIRT_CLINT,
VIRT_ACLINT_SSWI,
VIRT_PLIC,
VIRT_APLIC_M,
VIRT_APLIC_S,
VIRT_UART0,
VIRT_VIRTIO,
VIRT_FW_CFG,
VIRT_IMSIC_M,
VIRT_IMSIC_S,
VIRT_FLASH,
VIRT_DRAM,
VIRT_PCIE_MMIO,
VIRT_PCIE_PIO,
VIRT_PLATFORM_BUS,
VIRT_PCIE_ECAM
};</p>
<p>enum {
UART0_IRQ = 10,
RTC_IRQ = 11,
VIRTIO_IRQ = 1, /* 1 to 8 <em>/
VIRTIO_COUNT = 8,
PCIE_IRQ = 0x20, /</em> 32 to 35 <em>/
VIRT_PLATFORM_BUS_IRQ = 64, /</em> 64 to 95 */
};</p>
<p>#define VIRT_PLATFORM_BUS_NUM_IRQS 32</p>
<p>#define VIRT_IRQCHIP_NUM_MSIS 255
#define VIRT_IRQCHIP_NUM_SOURCES 96
#define VIRT_IRQCHIP_NUM_PRIO_BITS 3
#define VIRT_IRQCHIP_MAX_GUESTS_BITS 3
#define VIRT_IRQCHIP_MAX_GUESTS ((1U &lt;&lt; VIRT_IRQCHIP_MAX_GUESTS_BITS) - 1U)</p>
<p>#define VIRT_PLIC_PRIORITY_BASE 0x00
#define VIRT_PLIC_PENDING_BASE 0x1000
#define VIRT_PLIC_ENABLE_BASE 0x2000
#define VIRT_PLIC_ENABLE_STRIDE 0x80
#define VIRT_PLIC_CONTEXT_BASE 0x200000
#define VIRT_PLIC_CONTEXT_STRIDE 0x1000
#define VIRT_PLIC_SIZE(__num_context) <br />
(VIRT_PLIC_CONTEXT_BASE + (__num_context) * VIRT_PLIC_CONTEXT_STRIDE)</p>
<p>#define FDT_PCI_ADDR_CELLS    3
#define FDT_PCI_INT_CELLS     1
#define FDT_PLIC_ADDR_CELLS   0
#define FDT_PLIC_INT_CELLS    1
#define FDT_APLIC_INT_CELLS   2
#define FDT_IMSIC_INT_CELLS   0
#define FDT_MAX_INT_CELLS     2
#define FDT_MAX_INT_MAP_WIDTH (FDT_PCI_ADDR_CELLS + FDT_PCI_INT_CELLS + <br />
1 + FDT_MAX_INT_CELLS)
#define FDT_PLIC_INT_MAP_WIDTH  (FDT_PCI_ADDR_CELLS + FDT_PCI_INT_CELLS + <br />
1 + FDT_PLIC_INT_CELLS)
#define FDT_APLIC_INT_MAP_WIDTH (FDT_PCI_ADDR_CELLS + FDT_PCI_INT_CELLS + <br />
1 + FDT_APLIC_INT_CELLS)</p>
<p>bool virt_is_acpi_enabled(RISCVVirtState *s);
void virt_acpi_setup(RISCVVirtState *vms);
#endif</p>
<div style="break-before: page; page-break-before: always;"></div><p>PCIe devices are attached to pin 32 upto 35</p>
<p>PCI Express (PCIe) is a high-speed serial computer expansion bus standard used for connecting various components in a computer system. PCIe devices are devices that can be plugged into the PCIe slots on a motherboard to add functionality to a computer system. Here are some examples of PCIe devices:</p>
<pre><code>Graphics cards: PCIe is commonly used to connect graphics cards to a motherboard, allowing for high-speed data transfer between the graphics card and the CPU.

Network interface cards: PCIe can also be used to connect network interface cards, providing high-speed network connectivity to a computer system.

Sound cards: PCIe can be used to add high-quality audio processing capabilities to a computer system.

Solid-state drives: PCIe-based solid-state drives (SSDs) offer much faster data transfer speeds compared to traditional SATA-based SSDs.

USB expansion cards: PCIe can be used to add additional USB ports to a computer system, providing more connectivity options for peripherals.

RAID controllers: PCIe-based RAID controllers can be used to add additional storage capacity to a computer system, allowing for data redundancy and improved data access speeds.
</code></pre>
<p>Overall, PCIe is a versatile and widely used standard for connecting various devices to a computer system, offering high-speed data transfer and low latency.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-devices"><a class="header" href="#virtio-devices">VIRTIO devices</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-global-allocator"><a class="header" href="#the-global-allocator">The Global Allocator</a></h1>
<p>Each operating system has an allocation system. An allocation system goes through the heap memory and returns the free memory that is was requested. At times, it is also used to deallocate memory fro the heap.</p>
<p>Each operating system has its own way of dealing with the heap. Meaning that Oses have different allocator implementations.<br />
Data structures like vectors and linked lists are built on the heap. This means that for you to create a linked list, you have to have a heap allocator... and heap allocators are OS_dependent.<br />
For this reason data structures that depend on the heap are not available by default in a NO_STD environment. We need a way to make the compiler know that we have writen our own heap allocator and that it can use this heap allocator to implement data structures like the heap.</p>
<p>References :</p>
<ul>
<li><a href="https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/unstable-book/language-features/global-allocator.html">Readable documentation</a></li>
<li><a href="https://doc.rust-lang.org/std/alloc/trait.GlobalAlloc.html">Official crate documentation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="buffer_overflow_attacks"><a class="header" href="#buffer_overflow_attacks">buffer_overflow_attacks</a></h1>
<p>References : 
- <a href="https://samsclass.info/127/proj/p3-lbuf1.htm">How to conduct an Buffer_overflow attack</a></p>
<p>Buffer overflow is a type of vulnerability that occurs when a program writes data beyond the end of a buffer or allocated memory region, potentially overwriting other data or code. This type of vulnerability can be exploited by attackers to execute arbitrary code or modify program behavior in unintended ways. Buffer overflow attacks are a common type of security vulnerability, and have been used in many high-profile attacks over the years.</p>
<p>The basic idea behind a buffer overflow attack is to supply more data than a buffer can hold, causing the excess data to overwrite adjacent memory locations. For example, consider a program that reads user input into a fixed-size buffer:</p>
<pre><code class="language-C">char buffer[10];
gets(buffer);
</code></pre>
<p>If a user supplies more than 10 characters of input, the excess characters will be written beyond the end of the buffer, potentially overwriting other memory locations. An attacker can use this vulnerability to overwrite critical program data or code with their own malicious code, which the program will then execute.</p>
<p>There are several techniques that attackers can use to exploit buffer overflow vulnerabilities, including:</p>
<pre><code>Code injection: An attacker can inject their own code into the program's memory space by overwriting a function pointer or return address. When the program returns to the overwritten code, it will execute the attacker's code instead of the original code.

Denial of service: An attacker can cause a program to crash or hang by overwriting critical data structures or interrupt handlers. This can be used to disrupt the normal operation of a system or to prevent the program from detecting other attacks.

Privilege escalation: An attacker can use a buffer overflow vulnerability to gain elevated privileges on a system by overwriting a memory location that controls access control or other security-related features.
</code></pre>
<p>To prevent buffer overflow attacks, programmers must be careful to properly manage buffer sizes and ensure that all memory writes are properly bounds-checked. Many programming languages and frameworks provide built-in protections against buffer overflow attacks, such as array bounds checking, stack canaries, and address space layout randomization (ASLR). Additionally, secure coding practices, such as input validation and output encoding, can help prevent buffer overflow attacks by limiting the amount and type of data that a program accepts from external sources.</p>
<p>Practical Example : </p>
<ul>
<li>use unsafe language to write program</li>
<li>use an intentional hack () </li>
</ul>
<h3 id="me"><a class="header" href="#me">ME</a></h3>
<p>Buffer overflow attacks are possible because the attacker knows the layout of the program in memory. This enables them to know where to ovewrite. As a solution, operating systems implement something called &quot;Address space layout randomization&quot;.</p>
<p>Under Address space layout randomization, the operating system randomizes the memory positions of key units in the layout of the memory of the process. For example the stack, heap, text section.</p>
<p>The problem now becomes that randomizing the memory positions will cause the program to crash. This is because the program counter will stil point to the memory addresses that it was meant to point to before randomization happened. </p>
<p>To solve this Program counter problem. The ASLR gets executed in a predictable manner, such that the PC can be updated by adding an offset. Since the ASLR is predictable... it means hackers only neet to predict the offset that the PC is being fed. A security measure need not be predictable.</p>
<p>A solution to this 'predictability' problem of the ASLR gets partially solved by : &quot;running the ASLR algorithm each time the program is executed&quot; - This means that the attacker can only start predicting the memory layout after a program gets executed. This does not mean the program is safe. Once the attacker figures out the ASLR pattern... it's game over.</p>
<p>The ASLR partially solves the Buffer overflow problem. It just makes it hard for the attacker to understand the memory layout of a program. It makes it hard for someone to find an opprtunity to execute an attack. But it does not make the program safe from the actual attack.</p>
<p>The ASLR also does not typically randomize kernel memory regions. (I have no proof, I read from a blog ___ Look_into_this). THis leaves the kernel vulnerable</p>
<p>ASLR in Linux typically does not randomize kernel memory regions. This leaves the kernel vulnerable to certain types of attacks, such as kernel buffer overflow exploits, which can be used to gain unauthorized access to the system.</p>
<p>The reason for this is that the kernel is responsible for managing the memory of the entire system, and it needs to have a predictable layout of kernel memory regions in order to function properly. Randomizing the location of kernel memory regions could potentially break system functionality and cause stability issues.</p>
<p>Instead of relying solely on ASLR, the Linux kernel employs a variety of other security mechanisms to protect against attacks on kernel memory, such as kernel address space layout randomization (KASLR), which randomizes the location of the kernel image in memory. Other security mechanisms include memory protection and access controls, input validation, and privilege separation, among others</p>
<h3 id="experiment"><a class="header" href="#experiment">Experiment</a></h3>
<p>The call stack is a region of memory that is used to manage the execution of function calls in a program.<br />
Each program has a call stack.<br />
When a function is called, a new stack frame is created and pushed onto the call stack, and when the function returns, its stack frame is popped off the stack.</p>
<p>The Extended Stack Pointer (ESP) is a register in the x86 computer architecture that is used to keep track of the memory location of the current stack frame.<br />
So if you read the ESP, you will be getting a value within the call stack of the current executing thread in CPU.</p>
<p>As easlier discussed, Linux uses ASLR to protect code and data found in the User Address Space. 
The &quot;randomize_va_space&quot; file in the &quot;/proc/sys/kernel&quot; directory is used to control the behavior of ASLR in the Linux OS.<br />
You can configure the ASLR mode using one of the following commands :</p>
<pre><code class="language-bash">echo 0 &gt; /proc/sys/kernel/randomize_va_space   // Disable ASLR completely
echo 1 &gt; /proc/sys/kernel/randomize_va_space   // affect callstack only 
echo 2 &gt; /proc/sys/kernel/randomize_va_space   // affect all memory regions
</code></pre>
<p>The &quot;proc&quot; file system allows users and applications to retrieve information about system configuration, hardware devices, running processes, and other system-level data. For example, the /proc/cpuinfo file provides detailed information about the processor(s) installed in the system, such as their model, speed, cache size, and features.</p>
<p>Setting the config value to 1 enables ASLR capabilities on the user_program stack and mmap-based allocations only. It disables ASLR on those other types of memory such as heap, shared libraries.</p>
<p>Setting the config value to 2 enables ASLR capabilities on all the user_program memory sections. This is full ASLR.  The config value 2 is usually the default.</p>
<p>To execute our BF attak, let as assume that we have cracked the ASLR. So let's dissable the ASLR by stiing the value in randomize_va_space file to 0.</p>
<p>Note to self :
Under C we usually have stuff like this : &quot;void main(int argc, char *argv[]) &quot;</p>
<pre><code>The &quot;argc&quot; parameter is an integer that represents the number of command-line arguments passed to the program, including the program name itself. For example, if you execute a program named &quot;myprog&quot; with the command &quot;myprog arg1 arg2 arg3&quot;, argc would be 4.

The &quot;argv&quot; parameter is an array of pointers to strings, where each string represents a command-line argument passed to the program. The first element of the &quot;argv&quot; array is a pointer to the program name, and the remaining elements are pointers to the strings representing the other command-line arguments.
</code></pre>
<p>So we are going to do the attack in two fronts :
- Cause a Denial of Service by crashing a program.
- Execute bad code </p>
<p>So we write a program that overloads the buffer of an array. So it causes a segmentation Fault. A segmentation fault, often called a segfault, is a common error that occurs when a program attempts to access memory that &quot;does not belong to it.&quot; This typically happens when a program tries to read or write to memory that it doesn't have permission to access, or when the program tries to access memory that simply does not exist.</p>
<p>When a program causes a segmentation fault, it is usually terminated by the operating system to prevent further damage to the system. The error message for a segmentation fault often includes the phrase &quot;core dumped,&quot; which means that a snapshot of the program's memory has been saved to disk for debugging purposes.</p>
<p>Now, we have managed to crush a user program... can we crush the OS ?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="web-assembly"><a class="header" href="#web-assembly">Web Assembly</a></h1>
<p>why?</p>
<ul>
<li>sandbox security</li>
<li>portability
<ul>
<li>universal APIs </li>
<li>anything that has the runtime</li>
<li>any language as source</li>
<li>Glueing together languages (in development modules + package system)</li>
<li>cloud updates for IoT (you don't have to worry about compilation )</li>
<li>Keeping up with ubiquitous computing (heterogenous devices)</li>
</ul>
</li>
<li>Standard application inspection for security (3rd party)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="literature-review-papers"><a class="header" href="#literature-review-papers">Literature review papers</a></h1>
<p>Here is a list of references that will guide the following sub_chapters :</p>
<ol>
<li>Bringing WebAssembly Up to Speed with Dynamic Linking - <a href="https://helda.helsinki.fi/bitstream/handle/10138/335259/WAsDE_SAC2021.pdf?sequence=1#cite.taivalsaari2018development">source</a></li>
<li>Architecting the Web of Things for the fog computing era - <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-sen.2017.0350">source</a></li>
<li>On the Development of IoT Systems - <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiw6vuuxZj_AhX_m_0HHW_9CD8QFnoECAoQAQ&amp;url=https%3A%2F%2Fresearchportal.helsinki.fi%2Ffiles%2F151313168%2Ffour_corners_iot_1.pdf&amp;usg=AOvVaw0R8kUvpQkK2rporht2l2F0">Source</a></li>
<li>Isomorphic IoT Architectures with Web Technologies - <a href="https://helda.helsinki.fi/bitstream/handle/10138/336610/Isomorphic.pdf?sequence=1">source</a></li>
</ol>
<h4 id="from-1"><a class="header" href="#from-1">From [1]</a></h4>
<p>We need to accomodate writing software for heterogenous computing. (ubiquitous computing)<br />
IoT devices require both local support systems and cloud systems. (fog computing and edge computing)
Edge computing insists that computational tasks get done by the end bots only, while fog computing allows computation tasks to be done by bots, near_nodes and cloud.<br />
But in todays world, fog computing and edge computing can be used together. No_one cares. It is very hard to build anything as pure edge. There is no need to put names on things</p>
<h4 id="from-2"><a class="header" href="#from-2">From [2]</a></h4>
<p>Fog computing involves creating an architecture where data processing is distributed across devices located closer to the point of data collection, instead of relying solely on a centralized cloud infrastructure.</p>
<p>It is not enough to depend on a central server for all the tasks.<br />
You need to choose wisely which tasks get doe centrally and which tasks get done &quot;almost locally in a distributed manner&quot;.</p>
<p>Now we are going in an error of programmable matter. We cannot depend on the traditional cloud-server architecture because it is slow. The network transfer time is very expensive. Programmable Matter co-ordinations need to be real time fast.</p>
<p>Example : Your security system at home uses the camera, thermostat and automated machine gun. These devices need to co-ordinate fast. It is unimaginable that for them to communicate they have to first communicate to a far remote server. That is very slow. More time is spent on message transfer than actual data processing.</p>
<p>The solution is fog computing, where you have a local server nearby. Or one of the devices(eg camera) takes up the role of a server.</p>
<h3 id="considering-webassembly-containers-on-iot-devices-as-edge-solution"><a class="header" href="#considering-webassembly-containers-on-iot-devices-as-edge-solution">Considering WebAssembly Containers on IoT Devices as Edge Solution</a></h3>
<p><a href="https://liu.diva-portal.org/smash/get/diva2:1575228/FULLTEXT01.pdf">source</a></p>
<ul>
<li>
<p>we need to cover the :</p>
<ul>
<li>wasm containers</li>
<li>wasm container runtimes</li>
<li>wasm ecosystem (management of both the runtimes and containers)</li>
</ul>
</li>
<li>
<p>Under wasm runtimes, we will cover their :</p>
<ul>
<li>speed of execution</li>
<li>memory footprint</li>
<li>functionalities provided and how they can be extended</li>
<li>security (internal security + external security)</li>
<li>future</li>
</ul>
</li>
<li>
<p>used wasmer and wasmtime as the test subjects (both written in Rust) BUT wasmtime uses the CraneLift compiler while wasmer offers the option to use Singlepass(AOT), CraneLift(rust JIT), and LLVM(AOT/JIT). Wasmtime is made by the Bytecode Alliance while wasmer is made by the Wasmer private organization(I don't know their name)</p>
</li>
<li>
<p>WebAssembly Runtime is essentially an environment where Wasm can be executed safely, it provides an abstraction layer above the OS, containerizing the execution.</p>
</li>
<li>
<p>Why not Docker?</p>
<ul>
<li>slower startup time. Meaning it might be wasteful for an event_driven IoT device</li>
<li>memory heavy in comparison to (wasm container + runtime)</li>
</ul>
</li>
<li>
<p>Why Docker?</p>
<ul>
<li>known, docs, tutorials</li>
</ul>
</li>
<li>
<p>Why not native?</p>
<ul>
<li>not capability_based security by default</li>
<li>Not as secure as with wasmRT (why?)</li>
<li>portability when dealing with heterogenous IoT devices. We need to migrate code + libraries more easily.</li>
</ul>
</li>
<li>
<p>Why native?</p>
<ul>
<li>fast execution because there is no boundary between code and OS. We are taking into account AOT runtimes. JIT and interpreters are out of the picture.</li>
<li>optimization of native machine code is not an excuse because the AOT compiler insude the WasmRT also does optimization of the native machine code</li>
</ul>
</li>
</ul>
<p>What are our benchmarks when determining when to use? C vs wasm_binary vs Docker_solution</p>
<ul>
<li>Runtime memory usage</li>
<li>Runtime execution time </li>
<li>container image/executable image memory usage</li>
<li>container image/executable execution time</li>
<li>container image/executable startup time (important becuse of event driven IoT apps?)</li>
</ul>
<p>Limitations of paper :</p>
<ul>
<li>used raspberry pi model 3 only (it is generic iot platform)</li>
<li>tested wasmer and wasmtime only</li>
<li>did not benchmark qualitative aspects of development of programs with Wasm eg ease of development and maintainability or debugging (a major challege in IoT development)</li>
</ul>
<p>The paper Uses PolyBench/C - is a suite of 30 different numerical computations, ranging all the way from linear algebra to image processing and physics simulations, written in C</p>
<p>PolyBench/C has both shorter, burst like jobs, as well as longer; it also has workloads that uses less memory, and workloads that uses substantially more. This shows both the strengths and weaknesses of Docker and Wasm, as they can show how well they perform in the different subsets of these workloads.</p>
<p>The bias of using PolyBench/C is that it tests many kinds of algorithms. It is better to test for your specific kind of algorithms instead of general algorithms.</p>
<p>Test : 
- run each PolyBench Algorithm ten times. Find the average. call it x
- plot a graph that shows each test runtime or memory usage.</p>
<p>So when you use wasm runtimes in userspace you get 2 levels of security :</p>
<ul>
<li>Security provided by runtime</li>
<li>security provided by OS kernel eg (file systems, MMU, memory randomization, process isolation)</li>
</ul>
<p>One layer of security is enough</p>
<p>Why wasm ?</p>
<ul>
<li>better security than native</li>
<li>code portability for heterogenous devices (hardware, capabilities, software and configs) in the rig</li>
<li>smaller memory footprint than docker containers</li>
<li>faster startup time that Docker when using a wasmRT that uses JIT/ interpretation technology</li>
<li>faster execution time than docker</li>
<li>Developers can write in languages that they are profecient in</li>
<li>
<h2 id="wasm-container-size-is-smaller-than-native-executables-by-averagely-147-andreas-rossberg-ben-l-titzer-andreas-haas-derek-l-schuff-dan-gohman-luke-wagner-alon-zakai-j-f-bastien-and-michael-holman-2018-bringing-the-web-up-to-speed-with-webassembly"><a class="header" href="#wasm-container-size-is-smaller-than-native-executables-by-averagely-147-andreas-rossberg-ben-l-titzer-andreas-haas-derek-l-schuff-dan-gohman-luke-wagner-alon-zakai-j-f-bastien-and-michael-holman-2018-bringing-the-web-up-to-speed-with-webassembly">Wasm container size is smaller than native executables by averagely 14.7% (Andreas Rossberg, Ben L. Titzer, Andreas Haas, Derek L. Schuff, Dan Gohman, Luke Wagner, Alon Zakai, J. F. Bastien, and Michael Holman. 2018. Bringing the Web up to Speed with WebAssembly.)</a></h2>
</li>
</ul>
<p>So wasm brings portability and security at the cost of performance. </p>
<p>Why is native code execution faster than wasm code execution(AoT execution in this case)?</p>
<ul>
<li>JIT compilation</li>
<li>AOT compilation of wasm binaries to enhance speed</li>
<li>Interpretation</li>
</ul>
<p>What is the work of a runtime? Security? dependency sorting? What about AOT compiled wasm modules</p>
<p>Does that mean that the kernel should provide the option of AOT compilation for certain tasks and JIT compilation.<br />
But why do we need JIT compilation in Embedded systems? In embedded systems performance and efficiency is our first priority. So we are not considering intepreters or JIT compilers</p>
<p>So we need the runtime so as to execute the native code and further enforce sandboxing features. Not for JIT compilation or Intepretation.
The runtime interacts with the underlying OS during the execution process. It acts as a manager for the compiled wasm modules.</p>
<p>A runtime does the following functions :</p>
<ul>
<li>Load the wasm modules from storage</li>
<li>Validate the format of the wasm module</li>
<li>Compile the wasm module to target machine code. (using AOT or JiT or pure interpretation)</li>
<li>Interact with the OS system interface in order to initiate the execution of the compiled code . (eg loading code into ram, making syscalls, allocate heap memory)</li>
<li>Implement WASM sandboxing features</li>
</ul>
<p>So when you use wasm runtimes in userspace you get 2 levels of security :</p>
<ul>
<li>Security provided by runtime</li>
<li>security provided by OS kernel eg (file systems, MMU, memory randomization, process isolation)</li>
</ul>
<p>so the runtime is a sandbox environment : 
We use it because it offers the following security implementations :<br />
Memory Isolation: Wasm runtimes employ memory isolation techniques to prevent unauthorized access or modification of memory. They use memory sandboxing mechanisms such as linear memory boundaries, memory segmentation, or address space layout randomization (ASLR) to isolate Wasm module's memory from the rest of the system.</p>
<pre><code>Access Control: Runtimes enforce access control policies to restrict the Wasm module's ability to interact with the host environment and sensitive resources. They provide mechanisms to control and limit access to system APIs, file systems, network interfaces, and other potentially dangerous operations.

Sandboxing: Wasm runtimes leverage sandboxing techniques to isolate the execution of Wasm modules from the underlying system. They create a restricted environment where modules operate within predefined boundaries, preventing them from performing malicious or unauthorized actions. Sandboxing techniques may include process-level isolation, operating system-level sandboxes, or virtualization technologies.

Code Verification: Wasm runtimes validate and verify the integrity and safety of Wasm modules before execution. They perform checks on the Wasm module's binary format, ensuring it conforms to the specification and does not contain malicious or malformed code. Verification helps prevent code injection, stack overflows, or other common vulnerabilities.

Controlled Resource Access: Runtimes enforce restrictions on the Wasm module's access to system resources, such as limiting file I/O operations, network requests, or access to hardware devices. They provide APIs or interfaces that allow controlled and supervised interaction between the module and the host environment.

Runtime Sandboxing: Some runtimes implement additional sandboxing techniques at the runtime level. They establish secure execution environments with limited privileges, where Wasm modules can execute without compromising the security of the hosting system. This includes isolating the module's execution from the runtime itself and other concurrently executing modules.

Capability-Based Security: Wasm runtimes may adopt capability-based security models, where explicit permissions and capabilities are granted to Wasm modules. Modules can only access resources or perform privileged operations if they possess the necessary capabilities, effectively limiting their privileges and reducing the risk of unauthorized actions.

Security Auditing: Wasm runtimes undergo rigorous security audits and testing to identify vulnerabilities and mitigate potential risks. Regular security updates and patches are released to address discovered vulnerabilities and ensure the runtime remains secure and resistant to potential attacks.
</code></pre>
<h4 id="webassembly-modules-as-lightweight-containers-for-liquid-iot-applications"><a class="header" href="#webassembly-modules-as-lightweight-containers-for-liquid-iot-applications">WebAssembly Modules as Lightweight Containers for Liquid IoT Applications</a></h4>
<p>Even if we implement the wasmOS, we need to meet the following requirements in order to fully replace docker :</p>
<ul>
<li>application state synchronization (make it more mature)</li>
<li>dynamic orchestration of the apps : spawning of containers, deleting containers across different nodes. (kurbenetes supports wasm containers : This project, called Krustlet [22], allows Wasm containers to be run as Kubelets in Kubernetes.)</li>
<li>Fine grained security model </li>
</ul>
<h4 id="from-wasmachine"><a class="header" href="#from-wasmachine">From Wasmachine</a></h4>
<ul>
<li>
<ul>
<li>I do not know the security features of wasm runtime compilers : how do they ensure integrity in the control flow? For wasm machine, the AOT compiler ensures cotrol flow integrity by :</li>
</ul>
<ol>
<li>makes code segment immutable to avoid direct code injection </li>
<li>the compiler adds code that verifies the signature of indirect functions. eg virtual functions</li>
</ol>
</li>
<li>
<p>WASI is an API. It has different modules inside.</p>
</li>
<li>
<p>An API defines functions and data structures for interaction between 2 systems.</p>
</li>
<li>
<p>Wasi has many modules. For example we have the Core Module that describes an API that is common to many Operating systems eg File_functions, networking functions. We have other non_core modules such as Crypto.</p>
</li>
<li>
<p>A webassembly module without function imports can only do math... keeping the CPU warm.</p>
</li>
<li>
<p>A wasm module can import a host function. This host function can do other things like file manipulation or networking.</p>
</li>
<li>
<p>Now you still have to specify the specific host functions your module is using (capability based search).</p>
</li>
<li></li>
<li>
<p>WebAssembly modules interact with the outside world exclusively through APIs. </p>
</li>
<li>
<p>WASI file system rules _ sandboxing feature <a href="https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-capabilities.md">here</a></p>
</li>
<li>
<p>Function normalization happens during compilation. So if we use an AOT compiler, the normalization latency will not affect us at runtime. If we use a JIT or an interpreter, the normalization latency will affect us at runtime.</p>
</li>
<li>
<p>Here is an example of normalization : Normalization happens because there is a difference in function prototypes between the WASI API syscalls and the Native Syscalls.</p>
</li>
<li>
<p>We can eliminate the latency caused by normalization by .... (DRUM rolls...) eliminating the need for normalization. We will do this by making the underlying system to export its system calls using the WASI function prototypes.</p>
</li>
<li>
<p>Now the remaining latencies are the latencies experienced during : </p>
<ul>
<li>interception of syscalls by the runtime. Interception means inspecting the syscalls and making sure they follow runtime security rules...</li>
<li>Context switching from user process to kernel process.</li>
</ul>
</li>
</ul>
<p>The only way to remove the inspection latency is to remove the inspection stage. We cannot afford to remove the inspection stage. The inspection stage enforces capability based security.</p>
<p>To get rid of the context switching expense, we eliminate the need for the OS to do context switching from user_process to kernel_process. THe only way to completely avoid a context switch is to make the wasm_process part of the kernel code. Such that the wasm_process is just a function within the kernel. But this would mean that we have to compile the app together with the kernel. Such that if we want to update the wasm app, we hae to recompile the entire kernel and then re-install it.   But if performance was more important than maintainability, then this is the way to go.</p>
<p>If your wasm App needs no future updates regularly you can use the above method. WASMACHINE uses this method. The only difference is that they execute these &quot;wasm app functions&quot; as Threads (this is both good and bad) Critic this (hard to implement threading in bare, no updates for both apps and OS, hard for installed apps to communicate easily)
(simple to implement, fast, optimized during compile time)</p>
<p>For our method, we wil not get rid of context switching completely. This is because we want to implement the wasm app and the kernel as seperate processes. So we will just reduce the expense of context switching. </p>
<p>Context switching from a user_level process to a kernel_level process is more expensive that context switching from a kernel_level process to another kernel_level process. This is because switching from user_level process to a kernel_level process requires more overhead : You have to switch modes, re-map the page tables and switch the execution stack. On the other hand, you don't have to switch modes or re-map the page tables</p>
<p>sandboxing nature so far :</p>
<ul>
<li>only interact with outside resources via host function imports only... other than that, you are just keeping the CPU warm.</li>
<li>specify the specific host functions that a module uses. This will help in scanning which module is using host functions that are not meant to be used by it.</li>
<li>The runtime configurations ensure that the modules affect resources that it has been granted. For example, the config --dir=. says that </li>
</ul>
<ol>
<li>the wasm modules being executed by the runtime can exercise the file host functions that they have imported. ie '--dir'</li>
<li>The wasm modules can only affect the current file directory ONLY. Not the entire file system like in UNIX.</li>
</ol>
<ul>
<li>wasm modules use a linear memory that is accessed through base + offset. The wasm module can only access this linear memory. Any out-of-bound access will be detected at compile time.</li>
</ul>
<p>The case for wasmachine :</p>
<ul>
<li>completely eliminates context switches therefore fast : . The results show that WebAssembly applications in Wasmachine is up to 11% faster than their native opponents in Linux. </li>
<li>It uses a runtime that has AOT capabilities, meaning that the wasm apps are compiled to native... the only performance bottleneck becomes the systemcall inspections. can you imagine that... inspections... a bunch of majorly read operations</li>
<li>The final extended runtime does not include the compiler (be it JIT or AOT), it just contains the runtime manager. THis is because the compiler is a different </li>
<li>The wasmachine </li>
</ul>
<p>Why no JIT or intepreter</p>
<ul>
<li>Too Compute intensive for embedded environment (constant loops of compilation) - bad for battery and processing power sharing</li>
<li>Time inefficient (no optimaization like in AOT)</li>
<li>unpredictable performance in JIT due to dynamic optimization</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software_deployment"><a class="header" href="#software_deployment">Software_deployment</a></h1>
<p>Software deployment is the process of making software available for the end user. This is the process that comes after you have already finished coding the project and now what you are left with is just... packaging it for different running environments.<br />
A running environment is any system (software or hardware) that the code runs on top of. For example; the running environment for the BIOS firmware is the bare metal motherboard. The running environment for the kernel is the bare metal motherboard. The running environment for Skyrim is a Play Station console. The Running environment for your hello world program is the target_system you have compiled for.</p>
<p>Software_deployment generally involves the following steps:</p>
<ol>
<li>Providing and Setting up the run environment. eg installing a Windows 10 operating system. </li>
<li>Installing the software and its dependencies ontop of the run environment. eg installing Skyrim and the dependent DirectX graphics libraries.</li>
<li>Configuring the installed softwware and dependencies to suit the run environment (optional) - reducing the graphics quality of the game because your PC is whack.</li>
<li>Testing if the software runs fine on the run_environment with the set configurations.</li>
<li>Rolling out the plug n play button : this may involve actions such as Distributing a link to the runing app, updating an interaction button in a website. In our Skyrim case, it may involve creating a shortcut icon for opening the game</li>
</ol>
<p>Let us imagine that we are building an army of nanorobots that can terraform a planet using swarm technology.<br />
This bots are just cold carbon, pure metal.<br />
The software that we are trying to run is an AGI program that has specialized in physical engineering.</p>
<p>Setting up the environment will be about installing the kernel on the lifeless bodies of those bots.<br />
We would then install all the driver dependencies that the AGI software needs.</p>
<p>With the environment set up, we would install the AGI on top of the kernel.<br />
We would then configure the kernel to prioritize orders from the AGI only.</p>
<p>We would then run a couple of tests to see whether the kernel, AGI and bot_metal have synched.<br />
Rolling out in this case would be initiating a remote control system here on earth that can be used to remotely ontrol those terraforming bots. The end users would be humans who are overseeing the terraforming process.</p>
<div style="break-before: page; page-break-before: always;"></div><h4 id="random"><a class="header" href="#random">Random</a></h4>
<p>Random :</p>
<ul>
<li>wasm is an Instruction set because it specifies a set of instructions that can be understood by a virtual machine. The same way Riscv assembly instructions can be understood by a Riscv CPU. Riscv is an instruction set too.</li>
<li>In computer science abtraction solves everything. When you want to standardize somthing... just virtualize accurately it and leave individual implementations to other people.</li>
<li>Now to enable code to run on any machine... wasm group created a binary language/ an assembly language for a virtual machine. There are different implementations of this virtual machine. Those different implementations are called web assembly runtimes </li>
</ul>
<p>From Chat Gpt :
Web Assembly (WASM) was designed to enable code to run on any machine, regardless of the underlying hardware or operating system. To achieve this, the WASM group created a binary format that can be executed by a virtual machine. This binary format serves as an assembly language for the virtual machine and is designed to be compact and efficient.</p>
<pre><code>There are different implementations of the Web Assembly virtual machine, each designed to run on a specific platform or operating system. These implementations are sometimes referred to as &quot;Web Assembly runtimes&quot; or &quot;WASM runtimes.&quot; Examples of popular Web Assembly runtimes include the V8 engine used in Google Chrome and Node.js, the SpiderMonkey engine used in Firefox, and the Wasmtime runtime developed by the Bytecode Alliance.
</code></pre>
<p>These web assembly runtimes run on top of other systems. Eg on top of bare metal hardware, or in the browser, on top of operating systems.</p>
<h3 id="questions"><a class="header" href="#questions">Questions</a></h3>
<p>Which wasm runtimes can Run on bare metal?
- wasmer
- wasmi
- wasmtime
- wamr</p>
<p>Which one are we choosing?<br />
we will work with wasmtime or wasmer. They have very good documentation. They are up to date with WASM post-mvp features. They are maintained by core members of the Bytecode alliance. They are majorly written in Rust Programming Language. They have a mature Rust integration too.</p>
<p>Why not wasmi?</p>
<ul>
<li>wasmi does not have a comprehensive documentation in comparison to wasmer or wasmtime.</li>
<li>wasmi is implemented as an interpreter. An intepreter in the embedded space is inappropriate.</li>
</ul>
<p>Why not wamr?</p>
<ul>
<li>It is written in C.</li>
</ul>
<h4 id="on-wasmtime"><a class="header" href="#on-wasmtime">On wasmtime</a></h4>
<ul>
<li>
<p>Wasmtime can be used both as a crate or a cmd-utility.</p>
</li>
<li>
<p>we will use it as a crate. We will embed it in core Rust code.</p>
</li>
<li>
<p>Someone should be able to take a .wasm file that is wasi compliant, load it in the OS as a user program and wait for output.</p>
</li>
<li>
<p>You will not be able to install wasmtime itself as a user program in our OS... unless you install it as a .wasm fileS</p>
</li>
<li>
<p>Our focus will not be on the commandline utility aspect.</p>
</li>
</ul>
<h5 id="creating-wasm-files-from-highlevel-languagesrust-and-running-them"><a class="header" href="#creating-wasm-files-from-highlevel-languagesrust-and-running-them">Creating wasm files from highlevel languages(Rust) and running them.</a></h5>
<ul>
<li>Install wasm32-wasi target. Install wasmtime-CLI. run .wasm files using wasmtime CLI commands</li>
<li>But you can also go through the embedded way</li>
</ul>
<h5 id="the-embedded-wayour-ticket-to-heaven"><a class="header" href="#the-embedded-wayour-ticket-to-heaven">The Embedded Way...our ticket to heaven</a></h5>
<p>You can instantiate a wasm module inside Rust code using the crate ; <a href="https://docs.rs/wasmtime/8.0.0/wasmtime/index.html">wasmtime</a>.<br />
An instantiated module is a webassembly file that has been compiled and has been stored in memory... ready to be executed.<br />
Now let us breakdown that statement: 
- Now the Engine has an internal compiler that takes in a raw file and outputs an optimized target-specific binary.(eg x86, Riscv)
- We use this engine to first compile our .wat file into a .wasm file
- You can use the default compiler configurations or you can pass a Config struct to adjust the compiler's behaviour. Soe of the things that you can adjust include : (a full list is <a href="https://docs.rs/wasmtime/8.0.0/wasmtime/struct.Config.html">here</a>)
- Target architecture
- Enable or disable outputting certain Performance Profiling metadata
- Enable or disable outputting certain Debugging metadata that might be used by 3rd party debugging tools.
- Disable code optimization or adjust the level of code optimization done by the compiler.
- Enabling or disabling specific WebAssembly features. For example, the bulk_memory feature allows using bulk memory operations, while the simd feature allows using SIMD instructions 
- Now 'storing in memory' means  that the webassembly runtime understood the memory requirements of the module and allocated the appropriate data structures and space within the memory that was asigned to the host application in the RAM.
- Now from this definitions, an instance means 'a process' and the wasmtime runtime acts as the CPU.
- So just like a normal process, the wasm instances are self contained and isolated. Self contained means that they have their own memory and all the imports were satisfied. Isolated means that the host application only communicates through imports and exports.
- The host application code gets executed by the real CPU while the wasm instantiated module gets executed by the wasm runtime. *The WebAssembly runtime translates the WebAssembly code into machine code that can be executed by the CPU of the host machine.  so even though both codes get executed by the host CPU... they take different paths before they get there.</p>
<p>A Store is a collection of WebAssembly instances and host-defined state.<br />
The store is responsible for managing the state of instantiated modules. This state includes things like the module's memory, global variables, and table. The state of the module can be modified and accessed by calling its exported functions, but it is not directly accessible from outside the module or from other instantiated modules</p>
<p>We need to :</p>
<ol>
<li>import the wasmtime crate</li>
<li>Compile the .wat module
<ul>
<li>Instantiate an Engine : &quot;a compiler + a manager of instantiated modules under it&quot;</li>
<li>configure the compiler found within the Engine</li>
<li>Compilation is done using something called an engine's compiler. It returns a compiled module called wasmtime::Module</li>
</ul>
</li>
<li>Define a store</li>
<li>Define all the host functions that are required by the wasm  module</li>
<li>Function Wrap those host-defined functions</li>
</ol>
<p>The WebAssembly page size, currently 64 kilobytes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="webassembly_challenges"><a class="header" href="#webassembly_challenges">webassembly_challenges</a></h1>
<h4 id="learning"><a class="header" href="#learning">learning</a></h4>
<p>Positives :</p>
<ul>
<li>new, so many possibilities... (we can basically be pioneers and steer the direction of the technology)</li>
<li>many research papers</li>
<li>active development on_going, so there are discussions happening</li>
<li></li>
</ul>
<p>negatives : </p>
<ul>
<li>a lot of tools are browser_based. My interest is on non_web uses of webassembly</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>These are notes made from the book : Programming Web Assembly with Rust by Kevin Hoffman.</p>
<h2 id="intro"><a class="header" href="#intro">Intro</a></h2>
<ul>
<li>
<p>Tech comes and goes; client-server, thinclient, fatclient, full_blown_cloud native.</p>
</li>
<li>
<p>The web can be seen as a global computer.</p>
</li>
<li>
<p>How will Wasm change the way developers build applications/ software? How is it revolutionary?</p>
<ul>
<li>I thought it just changes the way you compile applications?</li>
<li>I thought it was just another bytecode? Or another universal elf file?</li>
</ul>
</li>
<li>
<p>Kevin says that wasm will change :</p>
<ul>
<li>how consumers interact with apps</li>
<li>how developers build applications</li>
<li>the kind of applications a developer can build (WHAAAAAAT???? I thought every language was turing complete?)</li>
<li>It could change the definition of the word appliation</li>
</ul>
</li>
</ul>
<h3 id="todays-web-technology"><a class="header" href="#todays-web-technology">Todays Web Technology</a></h3>
<ul>
<li>
<p>Web tech has improved : faster transfer speeds of data for any app, better browser engines for clients to run web apps, Better local machines to run those browser engines.</p>
</li>
<li>
<p>In other words, The global computer (the web) has become faster, more efficient and has much better processing units(RAM and CPU and HDD)</p>
</li>
<li>
<p>Wasm is still maturing, it lacks tools. But which tools?</p>
</li>
<li>
<p>It has pain points, But which are these pain points?</p>
</li>
<li>
<p>We need tools for : </p>
<ul>
<li>How to interface host systems with wasm modules eg Browser virtual machines, OS virtual machines</li>
<li>How to orchestrate wasm containers</li>
<li>How to do security scans and management on wasm containers</li>
<li>How to reconvert wasm code to readable high level code (so that people who speak different programming languages can understand each other.)</li>
<li>Handling wasm modules in a language independent manner</li>
</ul>
</li>
</ul>
<h2 id="fundamentals"><a class="header" href="#fundamentals">Fundamentals</a></h2>
<p>WebAssembly can be viewed at two different levels—the raw, foundational level and at the higher level of other programming .languages using WebAssembly as a target.
To understand fundamentals means that we need to Understand what web assembly can do, what it can't, and how to use language independent tools to deal with wasm modules</p>
<p>This way, you can understand how higher level languages produce wasm modules as a compilation product.</p>
<p>[promise]
<strong>One problem solved using wasm is that people can stick to the languages, tools and development styles they like. Regardless of whether they are in backend, frontend, embedded space</strong> - There are paradigm shifts between this spaces. And grass isn't always greener on the other side.  What if you did not have to change your language when switching between this domains?</p>
<p>You are no longer tied to JS or python if you want to go ubiquitous. You can use any library written in any language, al you have to do is to compile that library to wasm. </p>
<p>Wasm brings the message : You can code in the language you like, for any domain. No need to learn new languages. You can choose your own language. Be assured will run on anything that can habitate a wasm runtime. Be it a browser, desktop or a satellite. You concentrate on writing the business logic</p>
<p>You are no longer tied to Java in order to run things everywhere. Or The C# frameworks for making it okay to write for frontends and backends. Or Javascript to develop web applications. Write software in a language you want, it will run</p>
<p>Let us end this disease of tooling so that developers can concentrate on implementing business processes.</p>
<p>Wasm is different from Java because it :
- allows the use of other high level languages by default
- uses a capability based security architecture.</p>
<h3 id="wasm"><a class="header" href="#wasm">Wasm</a></h3>
<ul>
<li>
<p><strong>Portable binary code</strong> - has a bytecode that can be translated to existing assembly languages for different CPUs architectures. A generic bytecode. </p>
</li>
<li>
<p><strong>for a stack based machine</strong> - the bytecode instructions are designed for a stack execution. This has its advantages and disadvantages that affect its code size, code sped and code</p>
<p>advantages :</p>
<ul>
<li>smaller binary size</li>
<li>ease of portability : unlike in registers where some registers have predefined responsibilities mentioned in the ISA Convention. The stack does not have specifics about its stacks.</li>
<li>I do not understand the rest yet.</li>
</ul>
<p>Disadvantages :</p>
<ul>
<li>limited parallelism support. The stack is designed to be unwound sequentially. It is cumbersome to implement parallelism. It requires additional stack management activities.</li>
<li>Unintuitive for most programmers making it hard to debug</li>
</ul>
</li>
</ul>
<p>Why is Wasm not a JS killer? 
- Because most browsers have a JS virtual engine. This is a big project. Turning it to be fully wasm-only will be hard.
- JS has many default features and tools purposefully made to suit web development eg event handling. A language like rust is turing complete yes... but you have to start building new tools around it. Like Yew.
- Many developers building for the web use JS or typescript. Why change? </p>
<p>For Me, wasm matters because I am not delving into web development.</p>
<p>A wasm module on its own cannot do anything. It needs to run within a runtime system that can interact with the host. THe hos in this case can be the OS or an ethereum API.</p>
<h4 id="datatypes-and-control-flow"><a class="header" href="#datatypes-and-control-flow">Datatypes and control flow</a></h4>
<ul>
<li>it only has 4 data types : i32, i64, f32, f64.</li>
<li>Whether they are signed or unsigned can be specified during operations eg i32.add  or i32.add_u</li>
</ul>
<h4 id="linear-memory"><a class="header" href="#linear-memory">Linear memory</a></h4>
<p>Linear memory : a static set of cotiguous memory. You can grow this linear memory in pages (64KB each). You do this statically. The runtime will not do this for you.</p>
<p>The linear memory that a module uses can be :
- decalred by the code inside the module
- exported by the code inside the module
- imported from the host using the code inside the module.</p>
<p><strong>Why linear memory?</strong></p>
<ul>
<li>direct memory accesses are mad fast. Causing efficiency. You index the linear memory like an array. Using offsets as the index.</li>
<li>The wasm module can only read and write from this linear memory. The module cannot read and write to the hosts memory or any other memory that is not its linear memory [security]</li>
</ul>
<p><strong>But the host can access the wasm linear memory? Isn't this a risk?</strong>*[security]
Yes this is a risk. But this risk has been nullified by requiring the host to cause change to the linear memory by using the functions exported from the module. In short the module says to the host : &quot;I will let you control my spaceship, but you will only show you the buttons that I want you to press... and I will define what those buttons actually do in the background&quot; </p>
<p><strong>in the end, the runtime is the first place to attack</strong>*<br />
this is because the runtime is the one that enforces these interaction rules between the host and the wasm module.<br />
The choice of runtime becomes important</p>
<h2 id="language-independent-tooling"><a class="header" href="#language-independent-tooling">Language Independent Tooling</a></h2>
<ul>
<li>we will use the WABT - web Assembly Binary Tools</li>
<li>This wabt is a set of tools ; but we will only need wat2wasm and wasm-objdump</li>
<li>wat2wasm converts a .wat file to a .wasm file.</li>
<li>wasm-objdump helps you analyze :
<ul>
<li>see which memories were exported, or imported, or created -- and by which names</li>
<li>see which function signatures exist</li>
<li>see which functions exist (by name) and which signatures they use</li>
<li>see which functions exist (by name), which ones were imported (and their import name)</li>
<li>which are the global variables</li>
<li>what are the dependencies of the module?</li>
<li>other things that I currenntly don't need to know yet</li>
<li>You can disassemble the machine code and see the assembly code with memory locations of the instructions outlined</li>
<li>You can see the memory mapping of each header</li>
</ul>
</li>
</ul>
<p>commands : </p>
<pre><code class="language-bash">sudo apt install wabt             // installing WABT
wat2wasm test.wat -o test.wasm    // using wat2wasm
wasm-objdump test.wasm
</code></pre>
<p>You can embed wabt in rust code.<br />
You can embed wasmi and other runtimes in wasm code.</p>
<h1 id="outside-the-browser"><a class="header" href="#outside-the-browser">Outside the Browser</a></h1>
<p>Tasks you need to know:
- load a wasm module
- interpret/compile
- execute the interpreted/compiled code
- How to avail host functions
- How to use wasm exported functions
- How to use memory exported from a wasm module
- How to import memory from the host</p>
<h3 id="how-to-be-a-good-runtime-the-functions-of-a-runtime"><a class="header" href="#how-to-be-a-good-runtime-the-functions-of-a-runtime">How to be a good Runtime. The functions of a Runtime</a></h3>
<p>The WASI interface specifies the contract of interaction between host and wasm module:
- The syscalls
- The data formats, communication protocols that both Host and Module must follow for interoperability.</p>
<p>What does it mean ... &quot;run in the runtime sandbox&quot;?<br />
The runtime is just a program. It also gets packaged as an elf file filled with machine code.<br />
The wasm file gets turned to machine code too.<br />
Running as a runtime means that the wasm_machine_code gets embedded into the elf file of the runtime program.<br />
The runtime adds management instructions to the wasm_machine_code.<br />
For example ; wasm_code -&gt; check_syscall_permissions -&gt; validate_syscall_execution -&gt; wasm_code(that makes syscall) -&gt; </p>
<pre><code>The runtime also adds glue code :
    - links the WASI calls to the native Syscalls
    - links host functions to the orrect exported wasm functions
</code></pre>
<ul>
<li>draw a diagram here kid</li>
</ul>
<p>The functions of the host include :
- loading the wasm module : reading the bytes and storing them in a buffer</p>
<pre><code>- validating the wasm module : check for wasm syntatic, semantic and security specifications eg:
  - check type safety : eg in function signatures and return values. Or operations that act on wrong types
  - validate cotrol flow integrity
  - ensure that the memory references are within linear memory
  - ensure other run_time specific specs eg : 
    - checking for access rights 
    - check if imported functions are available

- Expose the wasm module exports to the Host
    A wasm module without an export just keeps the CPU warm.    
    A wasm mobule provides a way for the host to use its functionality by exporting functions.  
    The Runtime reads the exports and provides a unified working and API to the Host
    The runtime compiled elf file will contain code that links host functions to wasm exported functions

- Satisfy imports needed by the Wasm Module from the Host
    The wasm module may import 
        - host functions 
        - linear memory.
        - Global variables (for communication or config purposes)
        - Tables (This is an array of function references from the host or other wasm modules)
        - Host custom data structures (eg a driver struct)
        - A high level API (almost same as a custom data structure or a Table)
    
    The runtime needs to make sure that the imports are satisfied. And if they are not satisfied, it should take the appropriate response... depending on the issue.    
    eg. If the linear memory is not available, then it should return the error : &quot;Host does not hae enough free memory space&quot;   

- compilation or interpretation
- Execute 
- Module Isolation from other modules (manage exported and unexported items accordingly) - This can be achieved by executing the runtime elf as seperate threads for each independent module
</code></pre>
<h2 id="building-a-runtime-in-rust-but-not-from-scratch"><a class="header" href="#building-a-runtime-in-rust-but-not-from-scratch">Building a Runtime in Rust (but not from scratch)</a></h2>
<h3 id="wasmi-specifics"><a class="header" href="#wasmi-specifics">Wasmi specifics</a></h3>
<p>In order to execute code from a wasm module, it must be instantiated. Instantiation includes the following steps:</p>
<p>steps :
1. Loading
2. Validating 
3. Creating a module instance with no imports satisfied and no instant bootstrapping
4. Resolving the definition instances for each declared import in the module
5. Instantiating definitions declared in the module (e.g. allocate global variables, allocate linear memory, etc.).
6. Initializing memory and table contents by copying segments into them.
7. Executing the start function, if any
8. After these steps, the module instance is ready to execute functions</p>
<h4 id="1-loading"><a class="header" href="#1-loading">1. Loading</a></h4>
<ul>
<li>create an empty character buffer, preferrably a vector because of unknown size.</li>
<li>copy the contents of the .wasm file to the buffer.</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut buffer = Vec::new();
{
let mut f = File::open(&quot;../fundamentals/add.wasm&quot;)?;
f.read_to_end(&amp;mut buffer)?;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="2-validating-wasm-code"><a class="header" href="#2-validating-wasm-code">2. Validating wasm code</a></h4>
<ul>
<li>convert the wasm code into a format that wasmi can validate, and validate it.</li>
<li>All this can be done using the &quot;let module = wasmi::Module::from_buffer(buffer)&quot; command</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let module = wasmi::Module::from_buffer(buffer)?;
<span class="boring">}</span></code></pre></pre>
<h4 id="3-creating-a-module-instance-with-no-imports-satisfied-and-no-instant-bootstrapping"><a class="header" href="#3-creating-a-module-instance-with-no-imports-satisfied-and-no-instant-bootstrapping">3. Creating a module instance with no imports satisfied and no instant bootstrapping</a></h4>
<p>The runtime needs to resolve imports demanded by the validated wasm module.<br />
The following elements can be imported : linear memory, global variables, functions, tables, 
So we use some structs called ImportResolver to resolve each of the elements.<br />
An ImportBuilder calls a bunch of ImportResolvers.<br />
An ImportBuilder::default() does not call any resolvers.</p>
<p>The import builder outlines the host's element signatures and unique index . </p>
<p>A ModuleInstance .assert_no_start(); function panics if the wasm code contains a start function. It prevents automatic bootstraping of modules... for security purposes.</p>
<p>If we wanted a module with a Start, we could have called ModuleInstance .run_start()</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let add_import_builder = ImportBuilder::default();
let instance = ModuleInstance::new(&amp;module, &amp;add_import_builder).expect(&quot;failed to instatiate module&quot;).assert_no_start();
<span class="boring">}</span></code></pre></pre>
<h4 id="step-456-do-not-apply-in-this-example"><a class="header" href="#step-456-do-not-apply-in-this-example">Step 4,5,6 do not apply in this example.</a></h4>
<p>This is because the module does not have imports(4) and it has nothing to be instantiated or allocated.</p>
<h4 id="7-execute-wasm_exported-functions"><a class="header" href="#7-execute-wasm_exported-functions">7. Execute wasm_exported Functions</a></h4>
<p>When you pass values to a wasm function, you must turn them to RuntimeValues. A RuntimeValue can be I32,i64,f32 or f64.<br />
Values that are returned from wasm functions are also in Option<RuntimeValue> form. You need to unwrap them as enums.<br />
You can call a wasm function using the </p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut add_args = [RuntimeValue::from(1), RuntimeValue::from(2)];
let wasm_return_value = instance.invoke_export(&quot;add&quot;, &amp;add_args, &amp;mut NopExternals);
<span class="boring">}</span></code></pre></pre>
<p>The name of wasm_exported function is case-sensitive.<br />
When you invoke a function from wasm_module, you can give the wasm module the external resources to use using a type that implements an External trait.<br />
The 'Externals' trait allows you to define which host elements used by the wasm module.</p>
<p>In our case, executing the add function needs no external resources such as host_functions and global variables. </p>
<h4 id="pattern"><a class="header" href="#pattern">Pattern</a></h4>
<ul>
<li>resolve imports</li>
<li>define external resources</li>
<li>wrap the an API around wasm_module_exported function invocations.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="riscv_run"><a class="header" href="#riscv_run">RISCV_RUN</a></h1>
<p>RiscV ISA will be used for a long time.<br />
If you intend to make unique robots with unique circuits, then you may need to learn RISCV.</p>
<p>Why?<br />
It is open source, meaning you can print your own circuits without paying IP fees.<br />
It is open source, meaning you can understand it in and out. No black-holes. Before you can tweak something uniquely, you have to first understand its current implementation through and through.<br />
It is opensource, meaning you can add your own extensions without paying IP fees.</p>
<p>It is an open standard, you can be assured that if individual RiscV companies fall, you will not get stalled<br />
You have the power to create ASICs and make them follow the Riscv ISA for compatibility. If let's say you come up wuth a new circuit design or implementation using molecule circuits or cells.... whatever... you may just extend the RISCV Base ISA, And that extension can be used as a standard to. Or rather, it may be a standard to those who use it. </p>
<p>The Riscv ISA is modular, You have the option of choosing to deal with only a subset of the instructions. Meaning the hardware will support only the instrutions you want. No more general ISAs with baggage hardware. You can create small circuits that are enough.</p>
<p>It has few instructions, It is simple. It is not an Inremental ISA</p>
<p><strong>problem</strong><br />
You am trying to write an OS.<br />
I do not like the way I have been reading snippets of the RiscV manual from blogs, chatgpt, book_chapters...<br />
I am having a hard time truly understanding things. I think the best way to learn it would be to try to read it whole.</p>
<p>Ok ... instead of whole, I will just read on the riviledged ISA. I currently do not care much about Unpriviledged ISA because such code can be written in higher level languages</p>
<p>References :</p>
<ol>
<li>The RISC-V Reader: An Open Architecture Atlas Beta Edition, 0.0.1 By : David Patterson and Andrew Waterman</li>
<li></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reasons_for_riscv"><a class="header" href="#reasons_for_riscv">reasons_for_RISCV</a></h1>
<p>[undone]
Kindly read the first chapter of The RISC-V Reader: An Open Architecture Atlas Beta Edition, 0.0.1 By : David Patterson and Andrew Waterman</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="priviledged_architecture"><a class="header" href="#priviledged_architecture">priviledged_architecture</a></h1>
<p>The assembly language provides sysntax on general instructions. Instructions that any sane program can call.<br />
The instructions are in modules.</p>
<p>There is one module of instructions that is important. This module contains basic instructions that are quite ...well... basic.<br />
You may never prefer to destructure them to simpler commands.<br />
This module is called : The BASE module.</p>
<p>It is agreed upon that the BASE Module is not to be changed, and that every implementation of RiSCV should be able to implement each of the BASE module instructions.</p>
<p>Suppose you want to create a new set of application specific instructions, you will have to create a new module for yourself. And all the instructions you write should build up on the BASE module instructions.</p>
<h3 id="illustration"><a class="header" href="#illustration">Illustration</a></h3>
<p>The BASE Module contains only 47 instructions. Any program can be broken down to assembly code that uses a combination of only these 47 instructions</p>
<p>Here is a representation of these 47 instructions :<br />
<img src="images/RISCV/BASE_instructions.png" alt="All Base Instructions" /></p>
<p>But you can't always write assembly code from scratch, using this basic intructions... so people came up with pseudo_instructions.</p>
<p>Here are the pseudo_instructions and an illustration of their corresponding combinations of Base Instructions:
<img src="images/RISCV/Pseudo_instructions_part_1.png" alt="" />
<img src="images/RISCV/pseudo_instructions_part_2.png" alt="" /></p>
<p>If you add an extension, more extension-specific instructions get added. For example, Adding the Float extension adds the following instructions in addition to the 47 Base Instructions.</p>
<p><img src="images/RISCV/float_instrutions.png" alt="" /></p>
<h3 id="moving-on"><a class="header" href="#moving-on">Moving on...</a></h3>
<p>Now the above instructions can be used to implement any piece of software. But these instructions are available in all priviledge modes {user, supervisor, machine}.<br />
Supervisor and Machine mode contain a few more instructions that are not found in the User mode</p>
<p>More-privileged modes generally have access to all of the features of less-privileged modes, and they add additional functionality not available to less-privileged modes, such as the ability to handle interrupts and perform I/O.</p>
<p>Here are all the added priviledged instructions :
<img src="images/RISCV/Priviledged_instructions.png" alt="" /></p>
<p>Going into either of the priviledged mode gives the HARt the power to access all memory, , I/O, and low-level system features necessary to boot and configure the system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-exceptions-and-interrupts-in-machine-mode"><a class="header" href="#handling-exceptions-and-interrupts-in-machine-mode">Handling Exceptions and Interrupts in Machine Mode</a></h1>
<p>Priviledged modes give you access to some registers that can be used to configure how the CPU deals with Interrupts and exceptions. </p>
<p>First off let us define what exceptions are ... and how different they are from Interrupts.</p>
<p>An exception occurs when the disturbance comes from the code that is currently getting executed. For example, if the current instruction tried to write to a read_only memory location, an exception will occur.</p>
<p>An interrupt occurs when the disturbase does not come from the code executing in the subject HART. This disturbance might come from another HART or the PLIC... something external... something that is not the code running in the Subject HART.</p>
<p>Riscv acknowledges the following Exceptions and Interrupts. You can add your own.<br />
<img src="images/RISCV/supported_exceptions_and_interrupts.png" alt="" /></p>
<p>The Exceptions are a lot. Let's just discuss the Interrupts :</p>
<ol>
<li>Software Interrupt - this is an interrupt that comes from another HART. An interprocessor Interrupt. This interrupt can come from a HART that is in Machine Mode or Supervisor Mode.</li>
<li>Timer Interrupt - this is an interrupt that comes from the time_comparator circuit. ie when the time found in mtimecmp register contains a value that is larger than the mtime register. THe Timer Interrupt might occure when the HART is in Machine Mode or S-Mode</li>
<li>External Interrupt - this is an interrupt that comes from the PLIC. The PLIC is receives interrupts from external devices such as keyboards or sensors</li>
</ol>
<p>To Handle interrupts, you need to manipulate a few control registers :</p>
<ol>
<li><strong>mstatus register</strong></li>
<li><strong>mie register</strong></li>
<li><strong>mtvec register</strong></li>
<li><strong>mcause register</strong></li>
<li><strong>mtval register</strong></li>
<li><strong>mip register</strong></li>
<li><strong>mepc register</strong></li>
<li><strong>mscratch register</strong></li>
</ol>
<h4 id="the-mstatus-register"><a class="header" href="#the-mstatus-register">The Mstatus Register</a></h4>
<p>The mstatus Register is the first stop. You can use this register to set whether the CPU should be able to handle interrupts or not... across all modes.<br />
You can also store the previous interrupt enable status of all modes.<br />
This is like the main switch.</p>
<p>Here is the structure of the mstatus register :
<img src="images/RISCV/mstatus_register_simple.png" alt="" /></p>
<p>MPP stands for Machine Previous protection mode. This segment stores the mode_code of the CPU when the exception happened. The reason we need to store the previous mode is so as to help us return to the previous mode using the mret instruction.<br />
SPP stands for Supervisor Previous protection mode. This segment stores the mode_code of the CPU when the exception happened
Anywhere you see IE ... it stads for Interrupt Enable eg MIE --&gt; Machine Mode Interrup Enable</p>
<p>We have the MPIE field (Machine Previous Interrupt Enable). This stores the previous IE setting. You see when the CPU is handling an exception, it does not need any disturbances, so it sets MIE to 0 by default. Storing the previous setting will help us restore the context after the CPU has finished handling the exception.</p>
<h4 id="the-mie-register"><a class="header" href="#the-mie-register">The MIE register</a></h4>
<p>After allowing the CPU to accept Interrupts, you have to specify which interrupts you are willing to accept using the MIE register.<br />
We do not have to enable Exceptions ... exceptions just happen, we cannot ignore any exceptions.<br />
Each bit in the MIE corresponds to an interrupt (not an exception).<br />
The positions are in accordance to the mcause table :
<img src="images/mcause_asynchronous_interrupts.png" alt="" /></p>
<p>For example, the MIE bit 7 represents the Machine Timer Interrupt<br />
Here is the actual structure of the MIE register :<br />
<img src="images/RISCV/MIE%20register%20and%20MIP%20register.png" alt="Both the MIE and MIP registers" /></p>
<h4 id="the-mtvec-register-mtvec---machine-trap-vector"><a class="header" href="#the-mtvec-register-mtvec---machine-trap-vector">The MTVEC register (MTVEC--&gt; Machine Trap Vector)</a></h4>
<p>A vector is a function's memory address.<br />
This register stores the address of the exception/interrupt handling function. When the CPU receives an exception or an interrupt, it starts executing the code found in this address.</p>
<h4 id="the-mepc-register"><a class="header" href="#the-mepc-register">THe MEPC register</a></h4>
<p>This is where the CPU usually stores the Address of the instruction that caused the exception. So if you want the to know the instruction that caused an exception for the sake of debugging</p>
<h4 id="the-mcause-register"><a class="header" href="#the-mcause-register">The mcause Register</a></h4>
<p>The Mcause register stores the ID of the exception or Interrupt.<br />
Here are the RISCV recognized IDs
<img src="images/mcause_asynchronous_interrupts.png" alt="" />
<img src="images/mcause_synchronous_interrupts.png" alt="" /></p>
<p>You can use this info to identify which interrupt needs to get handled...</p>
<h4 id="the-mtval-register"><a class="header" href="#the-mtval-register">THe mtval register</a></h4>
<p>This register stores the trap value. This is the additional info about the interrupt/exception. </p>
<h4 id="the-mscratch-register"><a class="header" href="#the-mscratch-register">The mscratch register</a></h4>
<p>This is a throw-away register to help you store temporary values. The mscratch register is attached to a buffer in memory. So you can use the mscratch register to store the context of the CPU before handling the exception/interrupt. This is to avoid data loss</p>
<h4 id="mip"><a class="header" href="#mip">mip</a></h4>
<p>Machine Interrupt pending. Shows which interrupt are waiting to be processed</p>
<h2 id="the-whole-process"><a class="header" href="#the-whole-process">The whole process</a></h2>
<p>When an exception occurs...</p>
<ol>
<li>The CPU stores the current PC to the mepc. But for interrupts, the next PC is the one that gets stored in the mepc</li>
<li>the PC is set to mtvec. </li>
<li>msattus and mtval are updated accordingly</li>
<li>the CPU disallows further acceptance of interrupts by setting the MIE segment in the mstatus register to 0 and storing the previous MIE value to the MPIE.</li>
<li>The pre-exception privilege mode is preserved in mstatus’ MPP field, and the privilege mode is changed to M.</li>
</ol>
<p>You need to make all error handlers to have a prologue where they save the context of the CPU pre-exception.</p>
<h2 id="the-mret-function"><a class="header" href="#the-mret-function">The mret function</a></h2>
<p>The mret function does the following actions :</p>
<ol>
<li>restores the previous mstatus' MIE field by reading from the mstatus' MPIE field</li>
<li>restores the CPU's PC to point to the value stored in the mepc register</li>
<li>restores the CPU to the Protection mode specified in the mstatus' MPP field.</li>
</ol>
<p>All Interrupts and Exceptions are handled in Machine mode by default. But you can make the Supervisor Mode handle some exceptions and interrupts by using the medeleg and mideleg registers</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="seperating_user_mode_from_machine_mode"><a class="header" href="#seperating_user_mode_from_machine_mode">seperating_user_mode_from_machine_mode</a></h1>
<p>Being in machine mode gives you power to access all memory addresses.<br />
It also allows you to change the status of Machine Mode CSRegisters. This have serious actions, for example, using the mstatus register, you can disable the CPU from reacting to any interrupts.</p>
<p>So the Machine Mode should only be allowed to run trusted code.<br />
People came up with User Mode.</p>
<p>In User mode...</p>
<ol>
<li>You can only access the memory that the Machine Mode has allowed you to.</li>
<li>You cannot access Machine Mode Registers</li>
<li>If you call any of the priviledged instructions, an &quot;invalid instruction&quot; exception is generated.</li>
</ol>
<h2 id="physical-memory-protection"><a class="header" href="#physical-memory-protection">Physical Memory Protection</a></h2>
<p>As earlier said, the User Mode programs can only access memory that the Machine Mode has allowed it to.<br />
The CPU achieves this by using a mechanism called : PMP - Physical Memory Protection</p>
<p>Physical Memory Protection works this way : </p>
<ul>
<li>You divide up our memory in sections</li>
<li>You declare the access rights of each section. Ie, you declare whether the User Mode program can Read/Write/Execute that section.</li>
<li>You store the start_addresses of those sections to special registers called &quot;PMP Registers&quot;.</li>
<li>You store those Access rights in a special register called &quot;pmpconfig&quot; register.</li>
</ul>
<p><img src="images/RISCV/pmp_configuration.png" alt="" /></p>
<p>The A field of the pmpconfig register can be 1 or 0. 0 means that that configuration has been disabled.<br />
A PMP address is stored as a physical address that has been shifted to the left by 2 bits (It has a granularity of 4 bytes)</p>
<h3 id="pmp-in-action"><a class="header" href="#pmp-in-action"><strong>PMP in action</strong></a></h3>
<ol>
<li>User-mode program tries to access memory address x.</li>
<li>CPU loops through the PMP addresses and checks whether memory x is greater or equal any of the PMP addresses.</li>
<li>If memory x is greater or equal to PMP address p BUT less than PMP address p+1, then we read the pmpconfig specifications of PMP address p+1</li>
<li>If the access rights conflict, an exception is thrown</li>
</ol>
<h3 id="disadvantages-of-the-pmp-mechanism"><a class="header" href="#disadvantages-of-the-pmp-mechanism"><strong>Disadvantages of the PMP mechanism</strong></a></h3>
<p>You can only create 16 sections. You can create 16 pages. With a virtual paging system memory system you can get more granularity. This does not cater for a system that runs multiple complex applications where each application may need its own pages, with different access levels.</p>
<p>All Interrupts and Exceptions are handled in Machine mode by default. But you can make the Supervisor Mode handle some exceptions and interrupts by using the medeleg and mideleg registers</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="supervisor_mode_to_the_rescue"><a class="header" href="#supervisor_mode_to_the_rescue">Supervisor_mode_to_the_rescue</a></h1>
<p>Now the Modes were once divided into User_mode and Machine mode only. The PMP system is a rockstar at isolating memory.<br />
But PMP was just not enough to support complex applications.... even 5!!!</p>
<p>So Supervisor mode was created, a mode where you have access to a Memory Paging system. So people usually implement their kernels on this mode.</p>
<p>Under S-Mode...</p>
<ol>
<li>You cannot call any M-mode instructions ie. mret.</li>
<li>You cannot access any M-Mode CSR. eg the mstatus register. However you get your own S-Mode registers eg sstatus register</li>
<li>You can ONLY access memory in accordance to th PMP configurations set by the Machine Mode configurations</li>
</ol>
<h2 id="interrupt-and-exception-handling-in-s-mode"><a class="header" href="#interrupt-and-exception-handling-in-s-mode">Interrupt and Exception Handling in S-Mode</a></h2>
<p>By default, all exceptions and interrupts are handled in machine Mode. For example a Supervisor timer interrupt will get handled by the fuction found in mtvec. That function might sort the issue... or it might pass that problem back to the S-Mode interrupt handler. This is a lot of work.</p>
<p>An OS usually has ways to sort out u-mode exceptions.<br />
Considering that an OS might be implemeted in S-mode. It is quite inefficient to let the U-mode exceptions cause the CPU to switch from u-mode to M-mode to S-mode and then back to M-mode.<br />
It would have been better if some exceptions and interrupts could just be passed to the s-mode without going through the m-mode.</p>
<p>So the solution is to let the Machine mode delegate some handling responsibilities to S-Mode. You delegate Interrupts using the mideleg register and exceptions using the medeleg register.</p>
<p>The SIE and SIP registers only activate the bits that correspond to the interrupts that have been delegated.<br />
If the exceptions that have been allowed through medeleg occur, those exceptions will get handled in S-mode.<br />
For the exceptioms that haven't been allowed through medeleg, they will be handled in Machine Mode first.</p>
<p>Exceptions and interrupts are handled just like in M-mode. We have the same registers :</p>
<ol>
<li>sstatus</li>
<li>scause</li>
<li>stvec</li>
<li>sie</li>
<li>sip</li>
<li>sscratch</li>
<li>stval</li>
<li>sepc</li>
</ol>
<p>Here is the sstatus register<br />
<img src="images/RISCV/sstatus_register.png" alt="" /></p>
<h2 id="using-the-virtual-paging-system"><a class="header" href="#using-the-virtual-paging-system">Using the Virtual Paging system</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h4 id="web-assembly-1"><a class="header" href="#web-assembly-1">Web assembly</a></h4>
<ol>
<li></li>
<li>Lin Clarks blogs :
<ul>
<li><a href="https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/">Standardizing WASI: A system interface to run WebAssembly outside the web</a></li>
<li><a href="https://hacks.mozilla.org/2019/11/announcing-the-bytecode-alliance/">Announcing the Bytecode Alliance: Building a secure by default, composable future for WebAssembly</a></li>
<li><a href="https://hacks.mozilla.org/2019/08/webassembly-interface-types/">WebAssembly Interface Types: Interoperate with All the Things!</a></li>
</ul>
</li>
<li><a href="https://os.phil-opp.com/">Philip Opperman' blog on building an OS with Rust on x86 cpu</a></li>
<li><a href="https://www.usenix.org/system/files/sec20-lehmann.pdf">Web Assembly paper on security</a></li>
<li><a href="https://www.kubesphere.io/blogs/will-cloud-native-webassembly-replace-docker_/">Will wasm replace docker?</a></li>
</ol>
<h4 id="wasi"><a class="header" href="#wasi">WASI</a></h4>
<ul>
<li><a href="https://github.com/WebAssembly/WASI/blob/main/legacy/preview1/docs.md">Here is the definition of the API structures and functions</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-overview.md">Here is the other github Repo with the Implementation of WASI by wasmtime</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/blob/main/docs/security.md">Web assembly security explained</a></li>
</ul>
<h4 id="wasm-runtimes"><a class="header" href="#wasm-runtimes">Wasm Runtimes</a></h4>
<ul>
<li><a href="https://wasmer.io/wasmer-vs-wasmtime">Wasmer vs Wasmtime</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/blob/main/docs/contributing-architecture.md">Architecture of a runtime explained and configs</a></li>
</ul>
<h4 id="possible-for-rustriscv"><a class="header" href="#possible-for-rustriscv">Possible for Rust+riscv</a></h4>
<ul>
<li><a href="https://pdos.csail.mit.edu/6.828/2020/xv6/book-riscv-rev1.pdf">XV6 book rev 1</a> but find rev3</li>
<li>https://github.com/Ko-oK-OS/xv6-rust</li>
<li><a href="http://rcore-os.cn/rCore-Tutorial-Book-v3/index.html">rcore tutorial v3</a> (Rust + riscv)</li>
<li><a href="https://github.com/Jaic1/xv6-riscv-rust">Implementation of xv6-rust</a></li>
<li><a href="https://github.com/mit-pdos/xv6-riscv">Another Implementation of xv6-rust - MIT </a></li>
<li><a href="https://www.infoq.com/presentations/risc-v-future/">risc-v talk</a></li>
<li><a href="http://osblog.stephenmarz.com/index.html">Stephen Marz Blog</a></li>
<li><a href="http://rcore-os.cn/rCore_tutorial_doc/">rcore v2</a></li>
</ul>
<h5 id="risc-v"><a class="header" href="#risc-v">Risc v</a></h5>
<ul>
<li><a href="https://www.anandtech.com/show/3593">Why adding extensions to x86 is a headache</a></li>
<li><a href="https://riscv.org/technical/specifications/">Risc V official specifications</a></li>
<li><a href="https://danielmangum.com/categories/risc-v-bytes/">Simple down to earth Blog</a></li>
<li><a href="https://smist08.wordpress.com/2019/09/06/introducing-risc-v/">Riscv down to earth Tutorial</a></li>
<li><a href="https://book.rvemu.app/hardware-components/03-csrs.html">Riscv Control and Status Registers</a></li>
<li><a href="https://shakti.org.in/docs/risc-v-asm-manual.pdf">Riscv manual - by SHakti deelopment Team</a></li>
</ul>
<h5 id="general"><a class="header" href="#general">General</a></h5>
<ul>
<li><a href="https://sourceware.org/binutils/docs/ld/Scripts.html">How to write Linker scripts</a></li>
</ul>
<h5 id="redox"><a class="header" href="#redox">Redox</a></h5>
<ul>
<li><a href="https://doc.redox-os.org/book/ch00-00-introduction.html">Redox Official Book</a> </li>
<li><a href="https://www.redox-os.org/">Redox Official Website</a></li>
</ul>
<h5 id="sel4"><a class="header" href="#sel4">Sel4</a></h5>
<ul>
<li><a href="https://twanvl.nl/blog/agda/sorting">proving things</a></li>
<li><a href="https://sel4.systems/">Sel4 Official Website</a></li>
</ul>
<h4 id="tock-os"><a class="header" href="#tock-os">Tock OS</a></h4>
<ul>
<li><a href="https://www.tockos.org/documentation/design">Official website design description</a></li>
<li><a href="http://www.amitlevy.com/papers/tock-plos2015.pdf">why Rust Tock Papers</a></li>
</ul>
<h4 id="theseus"><a class="header" href="#theseus">Theseus</a></h4>
<ul>
<li><a href="https://www.theseus-os.com/Theseus/book/index.html">THeseus Book</a></li>
</ul>
<h4 id="why-rust"><a class="header" href="#why-rust">Why Rust</a></h4>
<ul>
<li><a href="https://youtu.be/mmJiwscpB4o">Video by Kevin Boos</a></li>
<li><a href="https://docs.google.com/presentation/d/e/2PACX-1vQYomAnfTNucuCqYgNkPaxpIdrhPxil9Qzle_6-xd7TYfdEBlgML0B3vztdNC2odwc25dLzW3XsithZ/pub?start=false&amp;loop=false">Slides by Kevin Boos</a></li>
<li><a href="https://scialex.github.io/reenix.pdf">Report on using Rust to write an OS</a></li>
</ul>
<h4 id="memory-safety"><a class="header" href="#memory-safety">Memory safety</a></h4>
<ul>
<li>What is memory safety? - <a href="https://hacks.mozilla.org/2019/01/fearless-security-memory-safety/">Mozilla Hacks</a></li>
<li>What is memory safety? and why is it important? - <a href="https://www.memorysafety.org/docs/memory-safety/#fn:1">Prossimo</a></li>
</ul>
<h4 id="os-reads"><a class="header" href="#os-reads">OS reads</a></h4>
<ul>
<li>[Processes](https://web.eecs.utk.e inspiring in the face of challdu/~smarz1/courses/cosc361/notes/processes/)</li>
<li><a href="http://bravegnu.org/gnu-eprog/lds.html">Linker Scripting</a></li>
</ul>
<h4 id="wasmtime"><a class="header" href="#wasmtime">Wasmtime</a></h4>
<ul>
<li><a href="https://docs.wasmtime.dev/introduction.html">Wasmtime official Docs</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/">Wasmtime crate Docs</a></li>
</ul>
<h4 id="rust-error-handling"><a class="header" href="#rust-error-handling">Rust Error Handling</a></h4>
<ul>
<li>Anyhow</li>
</ul>
<h4 id="elf-files-1"><a class="header" href="#elf-files-1">ELF Files</a></h4>
<ul>
<li><a href="https://osblog.stephenmarz.com/files/elf.pdf">elf file format specifications</a></li>
<li>The ELF file Specifications : (from <a href="https://refspecs.linuxfoundation.org/elf/elf.pdf">Linux Foundation</a>) (From <a href="https://www.cs.cmu.edu/afs/cs/academic/class/15213-f00/docs/elf.pdf">CMU.edu</a>)</li>
<li>https://wiki.osdev.org/ELF</li>
<li><a href="https://linuxhint.com/understanding_elf_file_format/">Tools in inspecting Elf files</a></li>
</ul>
<h4 id="booting"><a class="header" href="#booting">Booting</a></h4>
<ul>
<li><a href="https://os.phil-opp.com/minimal-rust-kernel/#the-boot-process">Booting in x86 CPU</a></li>
<li><a href="https://osblog.stephenmarz.com/ch1.html">Booting In RISCV</a></li>
</ul>
<h4 id="miscelleneous"><a class="header" href="#miscelleneous">Miscelleneous</a></h4>
<ul>
<li><a href="https://os.phil-opp.com/async-await/">multitasking</a></li>
<li><a href="https://refactoring.guru/design-patterns">Patterns..including the singleton pattern</a></li>
<li><a href="https://wiki.osdev.org/Beginner_Mistakes#Is_there_a_tutorial_on....3F">OS beginner mistakes</a>... and debunking them</li>
<li><a href="https://samsclass.info/127/proj/p3-lbuf1.htm">How to conduct an Buffer_overflow attack</a></li>
</ul>
<h4 id="uart-1"><a class="header" href="#uart-1">UART</a></h4>
<ul>
<li><a href="http://caro.su/msx/ocm_de1/16550.pdf">The UART Datasheet</a></li>
<li><a href="https://www.lammertbies.nl/comm/info/serial-uart">The UART specifications</a></li>
<li><a href="https://osblog.stephenmarz.com/ch2.html">Stephen's tutorial</a></li>
</ul>
<h4 id="global-alloc"><a class="header" href="#global-alloc">Global Alloc</a></h4>
<ul>
<li><a href="https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/unstable-book/language-features/global-allocator.html">Readable documentation</a></li>
<li><a href="https://doc.rust-lang.org/std/alloc/trait.GlobalAlloc.html">Official crate documentation</a></li>
</ul>
<p>curiosity over fear<br />
innovation over practicality'<br />
authenticity over professionalism<br />
Learning over Grades</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
